{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49e7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.18.0\n",
      "PyTorch Version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, applications\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e61b6",
   "metadata": {},
   "source": [
    "## 1. Define TensorFlow Architectures\n",
    "These must match exactly what is used in `DFGAN_clip.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d417bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder(Model):\n",
    "    \"\"\"\n",
    "    Bi-Directional LSTM Text Encoder.\n",
    "    Matches DAMSM.py architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, ntoken, ninput=300, nhidden=256, nlayers=1, drop_prob=0.5):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.nhidden = nhidden // 2  # Because bidirectional doubles it\n",
    "        self.ninput = ninput\n",
    "        self.nlayers = nlayers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        # Embedding: vocab_size -> 300\n",
    "        self.embedding = layers.Embedding(ntoken, ninput,\n",
    "                                        embeddings_initializer=tf.initializers.RandomUniform(-0.1, 0.1))\n",
    "        self.drop = layers.Dropout(drop_prob)\n",
    "        \n",
    "        # Bi-LSTM: outputs nhidden*2 = 256\n",
    "        self.rnn = layers.Bidirectional(\n",
    "            layers.LSTM(self.nhidden, return_sequences=True, return_state=True, dropout=drop_prob)\n",
    "        )\n",
    "\n",
    "    def call(self, captions, cap_lens=None, training=False):\n",
    "        # captions: [B, Max_Seq_Len]\n",
    "        emb = self.embedding(captions)\n",
    "        emb = self.drop(emb, training=training)\n",
    "        \n",
    "        # Create mask if cap_lens provided\n",
    "        if cap_lens is not None:\n",
    "            mask = tf.sequence_mask(cap_lens, maxlen=tf.shape(captions)[1])\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        # RNN Forward\n",
    "        # output: [B, Seq, Hidden*2]\n",
    "        # states: forward_h, forward_c, backward_h, backward_c\n",
    "        output, f_h, f_c, b_h, b_c = self.rnn(emb, mask=mask, training=training)\n",
    "        \n",
    "        # Words Embedding: [B, Hidden*2, Seq]\n",
    "        # Transpose to match official PyTorch output [B, Hidden*2, Seq]\n",
    "        words_emb = tf.transpose(output, [0, 2, 1])\n",
    "        \n",
    "        # Sentence Embedding: [B, Hidden*2]\n",
    "        # Concatenate final hidden states of forward and backward\n",
    "        sent_emb = tf.concat([f_h, b_h], axis=1)\n",
    "        \n",
    "        return words_emb, sent_emb\n",
    "\n",
    "class CNN_Encoder(Model):\n",
    "    \"\"\"\n",
    "    InceptionV3 Image Encoder.\n",
    "    Matches DAMSM.py architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, nef=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.nef = 256  # Hardcoded like official!\n",
    "        \n",
    "        # Load InceptionV3 (Pretrained on ImageNet)\n",
    "        base_model = applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(299, 299, 3))\n",
    "        base_model.trainable = False # Freeze base model\n",
    "        \n",
    "        # Define outputs\n",
    "        # 'mixed7' is the last 17x17 block (Matches PyTorch Mixed_6e)\n",
    "        # 'mixed10' is the last 8x8 block (Matches PyTorch Mixed_7c)\n",
    "        layer_names = ['mixed7', 'mixed10']\n",
    "        outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "        \n",
    "        self.inception = Model(inputs=base_model.input, outputs=outputs)\n",
    "        \n",
    "        # Projections\n",
    "        # 1x1 Conv for local features (768 -> nef) - NO BIAS\n",
    "        self.emb_features = layers.Conv2D(self.nef, 1, strides=1, padding='valid', use_bias=False,\n",
    "                                          kernel_initializer=tf.initializers.RandomUniform(-0.1, 0.1))\n",
    "        \n",
    "        # Linear for global features (2048 -> nef) - WITH BIAS\n",
    "        self.emb_cnn_code = layers.Dense(self.nef, use_bias=True,\n",
    "                                         kernel_initializer=tf.initializers.RandomUniform(-0.1, 0.1))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs: [B, H, W, 3] - will be resized to 299x299\n",
    "        x = tf.image.resize(inputs, [299, 299])\n",
    "        \n",
    "        # Normalize to [-1, 1] for InceptionV3\n",
    "        x = (x - 0.5) * 2.0\n",
    "        \n",
    "        # Get Inception Features\n",
    "        feat_local, feat_global = self.inception(x, training=False)\n",
    "        \n",
    "        # --- Local Features ---\n",
    "        # [B, 17, 17, 768] -> [B, 17, 17, nef]\n",
    "        local_emb = self.emb_features(feat_local)\n",
    "        \n",
    "        # Transpose to [B, nef, 17, 17] (NCHW) to match official PyTorch shape\n",
    "        local_emb = tf.transpose(local_emb, [0, 3, 1, 2])\n",
    "        \n",
    "        # --- Global Features ---\n",
    "        # [B, 8, 8, 2048] -> [B, 2048]\n",
    "        global_pool = tf.reduce_mean(feat_global, axis=[1, 2])\n",
    "        \n",
    "        # Project: [B, nef]\n",
    "        global_emb = self.emb_cnn_code(global_pool)\n",
    "        \n",
    "        return local_emb, global_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841d1bb",
   "metadata": {},
   "source": [
    "## 2. Conversion Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "841cdb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rnn_weights(pt_path, tf_model):\n",
    "    print(f\"Loading PyTorch weights from {pt_path}...\")\n",
    "    # Load state dict\n",
    "    state_dict = torch.load(pt_path, map_location='cpu')\n",
    "    \n",
    "    # 1. Embedding\n",
    "    # PT: encoder.weight [Vocab, Dim]\n",
    "    # TF: embedding.embeddings [Vocab, Dim]\n",
    "    pt_emb = state_dict['encoder.weight'].numpy()\n",
    "    tf_model.embedding.set_weights([pt_emb])\n",
    "    print(\" - Embeddings converted.\")\n",
    "    \n",
    "    # 2. LSTM\n",
    "    # PT: rnn.weight_ih_l0, rnn.weight_hh_l0, rnn.bias_ih_l0, rnn.bias_hh_l0 (Forward)\n",
    "    # PT: rnn.weight_ih_l0_reverse, ... (Backward)\n",
    "    \n",
    "    def set_lstm_layer(tf_lstm_layer, suffix=''):\n",
    "        # Weights\n",
    "        w_ih = state_dict[f'rnn.weight_ih_l0{suffix}'].numpy() # [4*H, Input]\n",
    "        w_hh = state_dict[f'rnn.weight_hh_l0{suffix}'].numpy() # [4*H, H]\n",
    "        \n",
    "        # Biases\n",
    "        b_ih = state_dict[f'rnn.bias_ih_l0{suffix}'].numpy()   # [4*H]\n",
    "        b_hh = state_dict[f'rnn.bias_hh_l0{suffix}'].numpy()   # [4*H]\n",
    "        bias = b_ih + b_hh\n",
    "        \n",
    "        # Transpose Weights for TF [Input, 4*H]\n",
    "        w_ih = w_ih.T\n",
    "        w_hh = w_hh.T\n",
    "        \n",
    "        # Set Weights\n",
    "        # TF expects [kernel, recurrent_kernel, bias]\n",
    "        tf_lstm_layer.set_weights([w_ih, w_hh, bias])\n",
    "        \n",
    "    # Forward Layer\n",
    "    print(\" - Converting Forward LSTM...\")\n",
    "    set_lstm_layer(tf_model.rnn.forward_layer, suffix='')\n",
    "    \n",
    "    # Backward Layer\n",
    "    print(\" - Converting Backward LSTM...\")\n",
    "    set_lstm_layer(tf_model.rnn.backward_layer, suffix='_reverse')\n",
    "    \n",
    "    print(\"RNN Conversion Complete.\")\n",
    "\n",
    "def convert_cnn_weights(pt_path, tf_model):\n",
    "    print(f\"Loading PyTorch weights from {pt_path}...\")\n",
    "    state_dict = torch.load(pt_path, map_location='cpu')\n",
    "    \n",
    "    # We only convert the projection layers, assuming InceptionV3 is standard ImageNet\n",
    "    \n",
    "    # 1. emb_features (Conv2d 1x1)\n",
    "    # PT: emb_features.weight [Out, In, kH, kW] -> [256, 768, 1, 1]\n",
    "    # TF: emb_features.kernel [kH, kW, In, Out] -> [1, 1, 768, 256]\n",
    "    w_conv = state_dict['emb_features.weight'].numpy()\n",
    "    w_conv = np.transpose(w_conv, (2, 3, 1, 0))\n",
    "    tf_model.emb_features.set_weights([w_conv])\n",
    "    print(\" - Local features projection converted.\")\n",
    "    \n",
    "    # 2. emb_cnn_code (Linear)\n",
    "    # PT: emb_cnn_code.weight [Out, In] -> [256, 2048]\n",
    "    # PT: emb_cnn_code.bias [Out] -> [256]\n",
    "    # TF: emb_cnn_code.kernel [In, Out] -> [2048, 256]\n",
    "    w_dense = state_dict['emb_cnn_code.weight'].numpy()\n",
    "    b_dense = state_dict['emb_cnn_code.bias'].numpy()\n",
    "    \n",
    "    w_dense = w_dense.T\n",
    "    \n",
    "    tf_model.emb_cnn_code.set_weights([w_dense, b_dense])\n",
    "    print(\" - Global features projection converted.\")\n",
    "    \n",
    "    print(\"CNN Conversion Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac77a9f",
   "metadata": {},
   "source": [
    "## 3. Execute Conversion\n",
    "Set your paths here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142499f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF models...\n",
      "Models built.\n",
      "Loading PyTorch weights from ./DAMSMencoders/text_encoder_best.pth...\n",
      " - Embeddings converted.\n",
      " - Converting Forward LSTM...\n",
      " - Converting Backward LSTM...\n",
      "RNN Conversion Complete.\n",
      "Models built.\n",
      "Loading PyTorch weights from ./DAMSMencoders/text_encoder_best.pth...\n",
      " - Embeddings converted.\n",
      " - Converting Forward LSTM...\n",
      " - Converting Backward LSTM...\n",
      "RNN Conversion Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5h/n64mcyts207dxrlc16rx_5cw0000gn/T/ipykernel_35225/1138426344.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(pt_path, map_location='cpu')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filename must end in `.weights.h5`. Received: filepath=./damsm_checkpoints/text_encoder_weights",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(PT_TEXT_PATH):\n\u001b[1;32m     34\u001b[0m     convert_rnn_weights(PT_TEXT_PATH, text_encoder)\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mtext_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTF_TEXT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved TF Text Encoder to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_TEXT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/damsm/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/damsm/lib/python3.10/site-packages/keras/src/saving/saving_api.py:227\u001b[0m, in \u001b[0;36msave_weights\u001b[0;34m(model, filepath, overwrite, max_shard_size, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m filepath_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(filepath)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filepath_str\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filename must end in `.weights.h5`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m     )\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filepath_str\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[1;32m    232\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filename must end in `.weights.json` when `max_shard_size` is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified. Received: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The filename must end in `.weights.h5`. Received: filepath=./damsm_checkpoints/text_encoder_weights"
     ]
    }
   ],
   "source": [
    "# PATHS TO YOUR PYTORCH MODELS\n",
    "PT_TEXT_PATH = './DAMSMencoders/text_encoder_best.pth'  # <--- CHANGE THIS\n",
    "PT_IMAGE_PATH = './DAMSMencoders/image_encoder_best.pth' # <--- CHANGE THIS\n",
    "\n",
    "# OUTPUT PATHS\n",
    "# Keras 3 (TF 2.18+) requires .weights.h5 extension for save_weights\n",
    "TF_TEXT_PATH = './damsm_checkpoints/text_encoder.weights.h5'\n",
    "TF_IMAGE_PATH = './damsm_checkpoints/image_encoder.weights.h5'\n",
    "\n",
    "if not os.path.exists('./damsm_checkpoints'):\n",
    "    os.makedirs('./damsm_checkpoints')\n",
    "\n",
    "# 1. Initialize TF Models (Need to build them first)\n",
    "# We need the vocab size to initialize RNN\n",
    "vocab_size = 5429 # Updated to match training (Max ID 5428 + 1)\n",
    "# Or load it:\n",
    "# vocab = np.load('./dictionary/vocab.npy')\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "text_encoder = RNN_Encoder(ntoken=vocab_size, nhidden=256)\n",
    "image_encoder = CNN_Encoder(nef=256)\n",
    "\n",
    "# Build models with dummy input\n",
    "print(\"Building TF models...\")\n",
    "dummy_cap = tf.zeros((1, 10), dtype=tf.int32)\n",
    "text_encoder(dummy_cap)\n",
    "\n",
    "dummy_img = tf.zeros((1, 299, 299, 3), dtype=tf.float32)\n",
    "image_encoder(dummy_img)\n",
    "print(\"Models built.\")\n",
    "\n",
    "# 2. Convert\n",
    "if os.path.exists(PT_TEXT_PATH):\n",
    "    convert_rnn_weights(PT_TEXT_PATH, text_encoder)\n",
    "    text_encoder.save_weights(TF_TEXT_PATH)\n",
    "    print(f\"Saved TF Text Encoder to {TF_TEXT_PATH}\")\n",
    "else:\n",
    "    print(f\"Text encoder path not found: {PT_TEXT_PATH}\")\n",
    "\n",
    "if os.path.exists(PT_IMAGE_PATH):\n",
    "    convert_cnn_weights(PT_IMAGE_PATH, image_encoder)\n",
    "    image_encoder.save_weights(TF_IMAGE_PATH)\n",
    "    print(f\"Saved TF Image Encoder to {TF_IMAGE_PATH}\")\n",
    "else:\n",
    "    print(f\"Image encoder path not found: {PT_IMAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293eda9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
