{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, applications\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Define TensorFlow Architectures\n",
    "These must match exactly what is used in `DFGAN_clip.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder(Model):\n",
    "    \"\"\"\n",
    "    Bi-Directional LSTM Text Encoder.\n",
    "    Matches DAMSM.py architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, ntoken, ninput=300, nhidden=256, nlayers=1, drop_prob=0.5):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.nhidden = nhidden // 2  # Because bidirectional doubles it\n",
    "        self.ninput = ninput\n",
    "        self.nlayers = nlayers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        # Embedding: vocab_size -> 300\n",
    "        self.embedding = layers.Embedding(ntoken, ninput,\n",
    "                                        embeddings_initializer=tf.initializers.RandomUniform(-0.1, 0.1))\n",
    "        self.drop = layers.Dropout(drop_prob)\n",
    "        \n",
    "        # Bi-LSTM: outputs nhidden*2 = 256\n",
    "        self.rnn = layers.Bidirectional(\n",
    "            layers.LSTM(self.nhidden, return_sequences=True, return_state=True, dropout=drop_prob)\n",
    "        )\n",
    "\n",
    "    def call(self, captions, cap_lens=None, training=False):\n",
    "        # captions: [B, Max_Seq_Len]\n",
    "        emb = self.embedding(captions)\n",
    "        emb = self.drop(emb, training=training)\n",
    "        \n",
    "        # Create mask if cap_lens provided\n",
    "        if cap_lens is not None:\n",
    "            mask = tf.sequence_mask(cap_lens, maxlen=tf.shape(captions)[1])\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        # RNN Forward\n",
    "        # output: [B, Seq, Hidden*2]\n",
    "        # states: forward_h, forward_c, backward_h, backward_c\n",
    "        output, f_h, f_c, b_h, b_c = self.rnn(emb, mask=mask, training=training)\n",
    "        \n",
    "        # Words Embedding: [B, Hidden*2, Seq]\n",
    "        # Transpose to match official PyTorch output [B, Hidden*2, Seq]\n",
    "        words_emb = tf.transpose(output, [0, 2, 1])\n",
    "        \n",
    "        # Sentence Embedding: [B, Hidden*2]\n",
    "        # Concatenate final hidden states of forward and backward\n",
    "        sent_emb = tf.concat([f_h, b_h], axis=1)\n",
    "        \n",
    "        return words_emb, sent_emb\n",
    "\n",
    "class CNN_Encoder(Model):\n",
    "    \"\"\"\n",
    "    InceptionV3 Image Encoder.\n",
    "    Matches DAMSM.py architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, nef=256):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.nef = 256  # Hardcoded like official!\n",
    "        \n",
    "        # Load InceptionV3 (Pretrained on ImageNet)\n",
    "        base_model = applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(299, 299, 3))\n",
    "        base_model.trainable = False # Freeze base model\n",
    "        \n",
    "        # Define outputs\n",
    "        # 'mixed7' is the last 17x17 block (Matches PyTorch Mixed_6e)\n",
    "        # 'mixed10' is the last 8x8 block (Matches PyTorch Mixed_7c)\n",
    "        layer_names = ['mixed7', 'mixed10']\n",
    "        outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "        \n",
    "        self.inception = Model(inputs=base_model.input, outputs=outputs)\n",
    "        \n",
    "        # Projections\n",
    "        # 1x1 Conv for local features (768 -> nef) - NO BIAS\n",
    "        self.emb_features = layers.Conv2D(self.nef, 1, strides=1, padding='valid', use_bias=False,\n",
    "                                          kernel_initializer=tf.initializers.RandomUniform(-0.1, 0.1))\n",
    "        \n",
    "        # Linear for global features (2048 -> nef) - WITH BIAS\n",
    "        self.emb_cnn_code = layers.Dense(self.nef, use_bias=True,\n",
    "                                         kernel_initializer=tf.initializers.RandomUniform(-0.1, 0.1))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs: [B, H, W, 3] - will be resized to 299x299\n",
    "        x = tf.image.resize(inputs, [299, 299])\n",
    "        \n",
    "        # Normalize to [-1, 1] for InceptionV3\n",
    "        x = (x - 0.5) * 2.0\n",
    "        \n",
    "        # Get Inception Features\n",
    "        feat_local, feat_global = self.inception(x, training=False)\n",
    "        \n",
    "        # --- Local Features ---\n",
    "        # [B, 17, 17, 768] -> [B, 17, 17, nef]\n",
    "        local_emb = self.emb_features(feat_local)\n",
    "        \n",
    "        # Transpose to [B, nef, 17, 17] (NCHW) to match official PyTorch shape\n",
    "        local_emb = tf.transpose(local_emb, [0, 3, 1, 2])\n",
    "        \n",
    "        # --- Global Features ---\n",
    "        # [B, 8, 8, 2048] -> [B, 2048]\n",
    "        global_pool = tf.reduce_mean(feat_global, axis=[1, 2])\n",
    "        \n",
    "        # Project: [B, nef]\n",
    "        global_emb = self.emb_cnn_code(global_pool)\n",
    "        \n",
    "        return local_emb, global_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Conversion Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rnn_weights(pt_path, tf_model):\n",
    "    print(f\"Loading PyTorch weights from {pt_path}...\")\n",
    "    # Load state dict\n",
    "    state_dict = torch.load(pt_path, map_location='cpu')\n",
    "    \n",
    "    # 1. Embedding\n",
    "    # PT: encoder.weight [Vocab, Dim]\n",
    "    # TF: embedding.embeddings [Vocab, Dim]\n",
    "    pt_emb = state_dict['encoder.weight'].numpy()\n",
    "    tf_model.embedding.set_weights([pt_emb])\n",
    "    print(\" - Embeddings converted.\")\n",
    "    \n",
    "    # 2. LSTM\n",
    "    # PT: rnn.weight_ih_l0, rnn.weight_hh_l0, rnn.bias_ih_l0, rnn.bias_hh_l0 (Forward)\n",
    "    # PT: rnn.weight_ih_l0_reverse, ... (Backward)\n",
    "    \n",
    "    def set_lstm_layer(tf_lstm_layer, suffix=''):\n",
    "        # Weights\n",
    "        w_ih = state_dict[f'rnn.weight_ih_l0{suffix}'].numpy() # [4*H, Input]\n",
    "        w_hh = state_dict[f'rnn.weight_hh_l0{suffix}'].numpy() # [4*H, H]\n",
    "        \n",
    "        # Biases\n",
    "        b_ih = state_dict[f'rnn.bias_ih_l0{suffix}'].numpy()   # [4*H]\n",
    "        b_hh = state_dict[f'rnn.bias_hh_l0{suffix}'].numpy()   # [4*H]\n",
    "        bias = b_ih + b_hh\n",
    "        \n",
    "        # Transpose Weights for TF [Input, 4*H]\n",
    "        w_ih = w_ih.T\n",
    "        w_hh = w_hh.T\n",
    "        \n",
    "        # Set Weights\n",
    "        # TF expects [kernel, recurrent_kernel, bias]\n",
    "        tf_lstm_layer.set_weights([w_ih, w_hh, bias])\n",
    "        \n",
    "    # Forward Layer\n",
    "    print(\" - Converting Forward LSTM...\")\n",
    "    set_lstm_layer(tf_model.rnn.forward_layer, suffix='')\n",
    "    \n",
    "    # Backward Layer\n",
    "    print(\" - Converting Backward LSTM...\")\n",
    "    set_lstm_layer(tf_model.rnn.backward_layer, suffix='_reverse')\n",
    "    \n",
    "    print(\"RNN Conversion Complete.\")\n",
    "\n",
    "def convert_cnn_weights(pt_path, tf_model):\n",
    "    print(f\"Loading PyTorch weights from {pt_path}...\")\n",
    "    state_dict = torch.load(pt_path, map_location='cpu')\n",
    "    \n",
    "    # We only convert the projection layers, assuming InceptionV3 is standard ImageNet\n",
    "    \n",
    "    # 1. emb_features (Conv2d 1x1)\n",
    "    # PT: emb_features.weight [Out, In, kH, kW] -> [256, 768, 1, 1]\n",
    "    # TF: emb_features.kernel [kH, kW, In, Out] -> [1, 1, 768, 256]\n",
    "    w_conv = state_dict['emb_features.weight'].numpy()\n",
    "    w_conv = np.transpose(w_conv, (2, 3, 1, 0))\n",
    "    tf_model.emb_features.set_weights([w_conv])\n",
    "    print(\" - Local features projection converted.\")\n",
    "    \n",
    "    # 2. emb_cnn_code (Linear)\n",
    "    # PT: emb_cnn_code.weight [Out, In] -> [256, 2048]\n",
    "    # PT: emb_cnn_code.bias [Out] -> [256]\n",
    "    # TF: emb_cnn_code.kernel [In, Out] -> [2048, 256]\n",
    "    w_dense = state_dict['emb_cnn_code.weight'].numpy()\n",
    "    b_dense = state_dict['emb_cnn_code.bias'].numpy()\n",
    "    \n",
    "    w_dense = w_dense.T\n",
    "    \n",
    "    tf_model.emb_cnn_code.set_weights([w_dense, b_dense])\n",
    "    print(\" - Global features projection converted.\")\n",
    "    \n",
    "    print(\"CNN Conversion Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Execute Conversion\n",
    "Set your paths here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS TO YOUR PYTORCH MODELS\n",
    "PT_TEXT_PATH = './DAMSMencoders/text_encoder_best.pth'  # <--- CHANGE THIS\n",
    "PT_IMAGE_PATH = './DAMSMencoders/image_encoder_best.pth' # <--- CHANGE THIS\n",
    "\n",
    "# OUTPUT PATHS\n",
    "# Keras 3 (TF 2.18+) requires .weights.h5 extension for save_weights\n",
    "TF_TEXT_PATH = './damsm_checkpoints/text_encoder.weights.h5'\n",
    "TF_IMAGE_PATH = './damsm_checkpoints/image_encoder.weights.h5'\n",
    "\n",
    "if not os.path.exists('./damsm_checkpoints'):\n",
    "    os.makedirs('./damsm_checkpoints')\n",
    "\n",
    "# 1. Initialize TF Models (Need to build them first)\n",
    "# We need the vocab size to initialize RNN\n",
    "vocab_size = 5429 # Updated to match training (Max ID 5428 + 1)\n",
    "# Or load it:\n",
    "# vocab = np.load('./dictionary/vocab.npy')\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "text_encoder = RNN_Encoder(ntoken=vocab_size, nhidden=256)\n",
    "image_encoder = CNN_Encoder(nef=256)\n",
    "\n",
    "# Build models with dummy input\n",
    "print(\"Building TF models...\")\n",
    "dummy_cap = tf.zeros((1, 10), dtype=tf.int32)\n",
    "text_encoder(dummy_cap)\n",
    "\n",
    "dummy_img = tf.zeros((1, 299, 299, 3), dtype=tf.float32)\n",
    "image_encoder(dummy_img)\n",
    "print(\"Models built.\")\n",
    "\n",
    "# 2. Convert\n",
    "if os.path.exists(PT_TEXT_PATH):\n",
    "    convert_rnn_weights(PT_TEXT_PATH, text_encoder)\n",
    "    text_encoder.save_weights(TF_TEXT_PATH)\n",
    "    print(f\"Saved TF Text Encoder to {TF_TEXT_PATH}\")\n",
    "else:\n",
    "    print(f\"Text encoder path not found: {PT_TEXT_PATH}\")\n",
    "\n",
    "if os.path.exists(PT_IMAGE_PATH):\n",
    "    convert_cnn_weights(PT_IMAGE_PATH, image_encoder)\n",
    "    image_encoder.save_weights(TF_IMAGE_PATH)\n",
    "    print(f\"Saved TF Image Encoder to {TF_IMAGE_PATH}\")\n",
    "else:\n",
    "    print(f\"Image encoder path not found: {PT_IMAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
