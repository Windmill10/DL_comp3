{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttry:\n",
    "\t\t# Restrict TensorFlow to only use the first GPU\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\t\t# Currently, memory growth needs to be the same across GPUs\n",
    "\t\tfor gpu in gpus:\n",
    "\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\n",
    "\t\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\t\tprint(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\texcept RuntimeError as e:\n",
    "\t\t# Memory growth must be set before GPUs have been initialized\n",
    "\t\tprint(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ Using CLIP tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. DATASET GENERATOR (Adapted for CLIP)\n",
    "# ==============================================================================\n",
    "\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAX_SEQ_LENGTH = 77 # CLIP default\n",
    "\n",
    "# Initialize CLIP Tokenizer\n",
    "try:\n",
    "    from transformers import CLIPTokenizer\n",
    "    # Use the same model name as the vision/text models we will load later\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    print(\"✓ CLIP Tokenizer loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error loading CLIP Tokenizer: {e}\")\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "    \"\"\"\n",
    "    Data generator using CLIP Tokenizer\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string\n",
    "        image_path: Path to image file\n",
    "    \n",
    "    Returns:\n",
    "        img, input_ids, attention_mask\n",
    "    \"\"\"\n",
    "    # ============= IMAGE PROCESSING =============\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    # Normalize to [-1, 1] to match generator's tanh output\n",
    "    img = (img * 2.0) - 1.0\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    # ============= TEXT PROCESSING =============\n",
    "    # Tokenize using CLIP\n",
    "    # We use py_function because tokenizer is Python code\n",
    "    def tokenize(text):\n",
    "        text = text.numpy().decode('utf-8')\n",
    "        # CLIP Tokenizer handles padding and truncation\n",
    "        enc = tokenizer(\n",
    "            text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=MAX_SEQ_LENGTH, \n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return enc['input_ids'][0], enc['attention_mask'][0]\n",
    "        \n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize, \n",
    "        inp=[caption_text], \n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    input_ids.set_shape([MAX_SEQ_LENGTH])\n",
    "    attention_mask.set_shape([MAX_SEQ_LENGTH])\n",
    "    \n",
    "    return img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator, word2Id_dict, id2word_dict, expand_captions=True):\n",
    "    \"\"\"\n",
    "    Dataset generator that decodes IDs to text for CLIP\n",
    "    \"\"\"\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions_ids = df['Captions'].values\n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    print(f\"Loading dataset from {filenames}...\")\n",
    "    \n",
    "    # Helper to decode IDs to text\n",
    "    def decode_ids(id_list):\n",
    "        words = []\n",
    "        for i in id_list:\n",
    "            word = id2word_dict.get(str(i), '')\n",
    "            if word and word != '<PAD>':\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    all_captions_text = []\n",
    "    all_paths = []\n",
    "\n",
    "    if expand_captions:\n",
    "        # Expand: Create a sample for every caption\n",
    "        print(\"Expanding captions (one sample per caption)...\")\n",
    "        for caps, path in zip(captions_ids, image_paths):\n",
    "            for cap_ids in caps:\n",
    "                text = decode_ids(cap_ids)\n",
    "                all_captions_text.append(text)\n",
    "                all_paths.append(path)\n",
    "    else:\n",
    "        # Random Select: Pick one random caption per image (static for this generator call)\n",
    "        # Note: Ideally we'd do random selection at runtime, but decoding text in graph is hard.\n",
    "        # For simplicity/performance, we pick one now. \n",
    "        # To get true randomness per epoch, we'd need to re-create the dataset or use py_function logic.\n",
    "        print(\"Selecting one random caption per image...\")\n",
    "        for caps, path in zip(captions_ids, image_paths):\n",
    "            cap_ids = random.choice(caps)\n",
    "            text = decode_ids(cap_ids)\n",
    "            all_captions_text.append(text)\n",
    "            all_paths.append(path)\n",
    "            \n",
    "    all_captions_text = np.array(all_captions_text)\n",
    "    all_paths = np.array(all_paths)\n",
    "    \n",
    "    print(f\"Dataset size: {len(all_captions_text)} samples\")\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_captions_text, all_paths))\n",
    "    dataset = dataset.shuffle(len(all_captions_text))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "# We use expand_captions=False to keep epoch size manageable (same as number of images)\n",
    "# or True for more training data. Let's use False for faster epochs initially, or True for better quality.\n",
    "# Given the small dataset (7k images), expanding is probably better (70k samples).\n",
    "dataset = dataset_generator(\n",
    "    data_path + '/text2ImgData.pkl', \n",
    "    BATCH_SIZE, \n",
    "    training_data_generator,\n",
    "    word2Id_dict,\n",
    "    id2word_dict,\n",
    "    expand_captions=True \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¶</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "from transformers import TFCLIPVisionModel, TFCLIPTextModel, CLIPProcessor, CLIPConfig\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"Transformers Version:\", transformers.__version__)\n",
    "except ImportError:\n",
    "    print(\"Transformers not installed. Please install it.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PYTORCH-TENSORFLOW COMPATIBILITY CONSTANTS\n",
    "# ==============================================================================\n",
    "# These constants ensure numerical equivalence between PyTorch and TensorFlow\n",
    "# implementations of GALIP.\n",
    "\n",
    "# 1. Optimizer epsilon: PyTorch Adam default is 1e-8, TensorFlow default is 1e-7\n",
    "#    Using 1e-7 can cause subtle numerical divergence over training.\n",
    "ADAM_EPSILON = 1e-8  # Match PyTorch default\n",
    "\n",
    "# 2. LayerNorm epsilon: PyTorch default is 1e-5, TensorFlow default is 1e-3\n",
    "#    This affects CLIP and any custom LayerNorm layers.\n",
    "LAYER_NORM_EPSILON = 1e-5  # Match PyTorch default\n",
    "\n",
    "# 3. Weight initialization: PyTorch Linear/Conv2d use Kaiming Uniform (He)\n",
    "#    TensorFlow defaults to Glorot Uniform (Xavier).\n",
    "#    All our layers now use kernel_initializer='he_uniform' explicitly.\n",
    "\n",
    "print(f\"PyTorch-compatible settings:\")\n",
    "print(f\"  ADAM_EPSILON = {ADAM_EPSILON}\")\n",
    "print(f\"  LAYER_NORM_EPSILON = {LAYER_NORM_EPSILON}\")\n",
    "print(f\"  Weight init: he_uniform (Kaiming Uniform)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# HELPER FUNCTION: Create PyTorch-compatible Adam optimizer\n",
    "# ==============================================================================\n",
    "def create_pytorch_compatible_adam(learning_rate, beta_1=0.0, beta_2=0.9):\n",
    "    \"\"\"\n",
    "    Creates an Adam optimizer with PyTorch-equivalent settings.\n",
    "    \n",
    "    PyTorch defaults:\n",
    "        - lr: required\n",
    "        - betas: (0.9, 0.999) but GALIP uses (0.0, 0.9)\n",
    "        - eps: 1e-8\n",
    "        - weight_decay: 0\n",
    "        - amsgrad: False\n",
    "    \n",
    "    TensorFlow defaults that differ:\n",
    "        - epsilon: 1e-7 (10x larger than PyTorch!)\n",
    "    \n",
    "    Args:\n",
    "        learning_rate: Learning rate\n",
    "        beta_1: First moment decay (default 0.0 for GAN training)\n",
    "        beta_2: Second moment decay (default 0.9 for GAN training)\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.optimizers.Adam with PyTorch-equivalent settings\n",
    "    \"\"\"\n",
    "    return tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=beta_1,\n",
    "        beta_2=beta_2,\n",
    "        epsilon=ADAM_EPSILON  # CRITICAL: Match PyTorch 1e-8\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VERIFY CLIP CONFIG (LayerNorm epsilon, visual_projection bias)\n",
    "# ==============================================================================\n",
    "# Load CLIP and verify critical configuration values\n",
    "\n",
    "def verify_clip_config(clip_model_name=\"openai/clip-vit-base-patch32\"):\n",
    "    \"\"\"\n",
    "    Verify that CLIP model has correct LayerNorm epsilon and check visual_projection bias.\n",
    "    \n",
    "    PyTorch CLIP uses LayerNorm eps=1e-5.\n",
    "    TensorFlow default is eps=1e-3, which can cause numerical divergence.\n",
    "    \n",
    "    Returns:\n",
    "        dict with verification results\n",
    "    \"\"\"\n",
    "    from transformers import TFCLIPModel, CLIPConfig\n",
    "    \n",
    "    config = CLIPConfig.from_pretrained(clip_model_name)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CLIP Configuration Verification\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check vision config\n",
    "    vision_config = config.vision_config\n",
    "    print(f\"\\nVision Config:\")\n",
    "    print(f\"  layer_norm_eps: {vision_config.layer_norm_eps}\")\n",
    "    print(f\"  hidden_size: {vision_config.hidden_size}\")\n",
    "    print(f\"  projection_dim: {config.projection_dim}\")\n",
    "    \n",
    "    # Check text config\n",
    "    text_config = config.text_config\n",
    "    print(f\"\\nText Config:\")\n",
    "    print(f\"  layer_norm_eps: {text_config.layer_norm_eps}\")\n",
    "    print(f\"  hidden_size: {text_config.hidden_size}\")\n",
    "    \n",
    "    # Verify LayerNorm epsilon matches PyTorch\n",
    "    expected_eps = 1e-5\n",
    "    vision_ok = abs(vision_config.layer_norm_eps - expected_eps) < 1e-10\n",
    "    text_ok = abs(text_config.layer_norm_eps - expected_eps) < 1e-10\n",
    "    \n",
    "    print(f\"\\n✓ Vision LayerNorm eps matches PyTorch: {vision_ok}\")\n",
    "    print(f\"✓ Text LayerNorm eps matches PyTorch: {text_ok}\")\n",
    "    \n",
    "    # Load model to check visual_projection bias\n",
    "    print(\"\\nLoading model to verify visual_projection...\")\n",
    "    model = TFCLIPModel.from_pretrained(clip_model_name)\n",
    "    \n",
    "    # Check if visual_projection has bias\n",
    "    has_visual_bias = model.visual_projection.use_bias if hasattr(model.visual_projection, 'use_bias') else model.visual_projection.bias is not None\n",
    "    has_text_bias = model.text_projection.use_bias if hasattr(model.text_projection, 'use_bias') else model.text_projection.bias is not None\n",
    "    \n",
    "    print(f\"\\nProjection Layer Bias:\")\n",
    "    print(f\"  visual_projection.use_bias: {has_visual_bias}\")\n",
    "    print(f\"  text_projection.use_bias: {has_text_bias}\")\n",
    "    \n",
    "    # Note: OpenAI CLIP uses bias=False for projections, HuggingFace may differ\n",
    "    # This is usually fine since pre-trained weights handle it\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'vision_layer_norm_eps': vision_config.layer_norm_eps,\n",
    "        'text_layer_norm_eps': text_config.layer_norm_eps,\n",
    "        'projection_dim': config.projection_dim,\n",
    "        'hidden_size': vision_config.hidden_size,\n",
    "        'vision_eps_ok': vision_ok,\n",
    "        'text_eps_ok': text_ok\n",
    "    }\n",
    "\n",
    "# Run verification\n",
    "clip_verification = verify_clip_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. BASIC BLOCKS (DF-GAN & GALIP Components)\n",
    "# ==============================================================================\n",
    "\n",
    "class Affine(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's Affine layer.\n",
    "    \n",
    "    PyTorch signature: Affine(cond_dim, num_features)\n",
    "    \n",
    "    PyTorch structure:\n",
    "        fc_gamma: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        fc_beta:  Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "    \n",
    "    Initialization:\n",
    "        fc_gamma.linear2: weight=0, bias=1 (so initial gamma=1, identity scaling)\n",
    "        fc_beta.linear2:  weight=0, bias=0 (so initial beta=0, no shift)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, num_features):\n",
    "        super(Affine, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # fc_gamma: 2-layer MLP\n",
    "        # PyTorch: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        # First layer: cond_dim -> num_features, he_uniform init (matches PyTorch Linear default)\n",
    "        # Second layer: num_features -> num_features, zeros weight, ones bias\n",
    "        self.gamma_linear1 = layers.Dense(num_features, kernel_initializer='he_uniform')\n",
    "        self.gamma_linear2 = layers.Dense(\n",
    "            num_features, \n",
    "            kernel_initializer='zeros',\n",
    "            bias_initializer='ones'\n",
    "        )\n",
    "        \n",
    "        # fc_beta: 2-layer MLP\n",
    "        # PyTorch: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        # First layer: cond_dim -> num_features, he_uniform init (matches PyTorch Linear default)\n",
    "        # Second layer: num_features -> num_features, zeros weight, zeros bias\n",
    "        self.beta_linear1 = layers.Dense(num_features, kernel_initializer='he_uniform')\n",
    "        self.beta_linear2 = layers.Dense(\n",
    "            num_features,\n",
    "            kernel_initializer='zeros',\n",
    "            bias_initializer='zeros'\n",
    "        )\n",
    "\n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, H, W, C] feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "        \"\"\"\n",
    "        # Compute gamma (scale)\n",
    "        gamma = self.gamma_linear1(y)\n",
    "        gamma = tf.nn.relu(gamma)\n",
    "        gamma = self.gamma_linear2(gamma)  # [B, num_features]\n",
    "        \n",
    "        # Compute beta (shift)\n",
    "        beta = self.beta_linear1(y)\n",
    "        beta = tf.nn.relu(beta)\n",
    "        beta = self.beta_linear2(beta)  # [B, num_features]\n",
    "        \n",
    "        # Reshape for broadcasting: [B, 1, 1, C]\n",
    "        gamma = tf.reshape(gamma, [-1, 1, 1, self.num_features])\n",
    "        beta = tf.reshape(beta, [-1, 1, 1, self.num_features])\n",
    "        \n",
    "        return gamma * x + beta\n",
    "\n",
    "\n",
    "class DFBLK(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's DFBLK.\n",
    "    \n",
    "    PyTorch signature: DFBLK(cond_dim, in_ch)\n",
    "    \n",
    "    Structure:\n",
    "        affine0 -> LeakyReLU(0.2) -> affine1 -> LeakyReLU(0.2)\n",
    "    \n",
    "    NO convolutions - just two affine transforms with activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch):\n",
    "        super(DFBLK, self).__init__()\n",
    "        # PyTorch: self.affine0 = Affine(cond_dim, in_ch)\n",
    "        # Pass cond_dim to match PyTorch signature exactly\n",
    "        self.affine0 = Affine(cond_dim, in_ch)\n",
    "        self.affine1 = Affine(cond_dim, in_ch)\n",
    "\n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, H, W, C] feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "        Returns:\n",
    "            [B, H, W, C] transformed feature map\n",
    "        \"\"\"\n",
    "        h = self.affine0(x, y)\n",
    "        h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "        h = self.affine1(h, y)\n",
    "        h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "class G_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's G_Block.\n",
    "    \n",
    "    PyTorch signature: G_Block(cond_dim, in_ch, out_ch, imsize)\n",
    "    \n",
    "    Structure:\n",
    "        1. Interpolate to target size\n",
    "        2. Residual path: fuse1(DFBLK) -> c1(conv) -> fuse2(DFBLK) -> c2(conv)\n",
    "        3. Shortcut path: c_sc(1x1 conv) if in_ch != out_ch\n",
    "        4. Output: shortcut + residual\n",
    "    \n",
    "    Note: imsize is handled dynamically via target_size parameter in call().\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch, out_ch):\n",
    "        super(G_Block, self).__init__()\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        \n",
    "        # PyTorch: nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.c1 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        self.c2 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        \n",
    "        # PyTorch: DFBLK(cond_dim, in_ch) and DFBLK(cond_dim, out_ch)\n",
    "        self.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        \n",
    "        # Shortcut: 1x1 conv only if channel dimensions change\n",
    "        # PyTorch: nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, h, y, target_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: [B, H, W, in_ch] input feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "            target_size: int, target spatial size for interpolation\n",
    "        Returns:\n",
    "            [B, target_size, target_size, out_ch] output feature map\n",
    "        \"\"\"\n",
    "        # PyTorch: h = F.interpolate(h, size=(self.imsize, self.imsize))\n",
    "        h = tf.image.resize(h, [target_size, target_size], method='nearest')\n",
    "        \n",
    "        # Residual path: fuse1 -> c1 -> fuse2 -> c2\n",
    "        # PyTorch: h = self.fuse1(h, y); h = self.c1(h); h = self.fuse2(h, y); h = self.c2(h)\n",
    "        res = self.fuse1(h, y)\n",
    "        res = self.c1(res)\n",
    "        res = self.fuse2(res, y)\n",
    "        res = self.c2(res)\n",
    "        \n",
    "        # Shortcut path\n",
    "        if self.learnable_sc:\n",
    "            sc = self.c_sc(h)\n",
    "        else:\n",
    "            sc = h\n",
    "            \n",
    "        return sc + res\n",
    "\n",
    "\n",
    "class D_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's D_Block.\n",
    "    \n",
    "    PyTorch signature: D_Block(fin, fout, k, s, p, res, CLIP_feat)\n",
    "    \n",
    "    PyTorch structure:\n",
    "        conv_r: Conv2D(fin, fout, k, s, p, bias=False) -> LeakyReLU(0.2) -> Conv2D(fout, fout, k, s, p, bias=False) -> LeakyReLU(0.2)\n",
    "        conv_s: Conv2D(fin, fout, 1, stride=1, padding=0) for shortcut\n",
    "        gamma: learnable scalar for residual (init=0)\n",
    "        beta: learnable scalar for CLIP features (init=0)\n",
    "    \n",
    "    Note: All PyTorch D_Block instantiations use k=3, s=1, p=1, so we hardcode these.\n",
    "    \"\"\"\n",
    "    def __init__(self, fin, fout, is_down=False, is_res=True, clip_feat=False):\n",
    "        super(D_Block, self).__init__()\n",
    "        self.is_res = is_res\n",
    "        self.clip_feat = clip_feat\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        \n",
    "        # Main conv path (PyTorch: k=3, s=1, p=1)\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.conv_r1 = layers.Conv2D(fout, 3, padding='same', use_bias=False, kernel_initializer='he_uniform')\n",
    "        self.conv_r2 = layers.Conv2D(fout, 3, padding='same', use_bias=False, kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Shortcut conv (PyTorch: 1x1, stride=1, padding=0)\n",
    "        # CRITICAL: padding='valid' to match PyTorch padding=0\n",
    "        self.conv_s = layers.Conv2D(fout, 1, padding='valid', kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Learnable scalars (initialized to 0, matching PyTorch torch.zeros(1))\n",
    "        if is_res:\n",
    "            self.gamma = tf.Variable(0.0, trainable=True, name='gamma')\n",
    "        if clip_feat:\n",
    "            self.beta = tf.Variable(0.0, trainable=True, name='beta')\n",
    "\n",
    "    def call(self, x, clip_f=None):\n",
    "        # Residual path\n",
    "        res = self.conv_r1(x)\n",
    "        res = tf.nn.leaky_relu(res, alpha=0.2)\n",
    "        res = self.conv_r2(res)\n",
    "        res = tf.nn.leaky_relu(res, alpha=0.2)\n",
    "        \n",
    "        # Shortcut\n",
    "        if self.learned_shortcut:\n",
    "            x = self.conv_s(x)\n",
    "        \n",
    "        # Combine based on flags\n",
    "        out = x\n",
    "        if self.is_res:\n",
    "\n",
    "            out = out + self.gamma * res     \n",
    "\n",
    "        if self.clip_feat and clip_f is not None:            \n",
    "            out = out + self.beta * clip_f\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CLIP ADAPTER (100% Faithful Replication)\n",
    "# ==============================================================================\n",
    "# This cell contains ONLY the CLIP_Adapter and its dependencies:\n",
    "# - DFBLK (also used by G_Block, defined in Basic Blocks cell)\n",
    "# - M_Block (for CLIP_Adapter)\n",
    "# - CLIP_Mapper (for CLIP_Adapter)\n",
    "# - CLIP_Adapter\n",
    "# Matching PyTorch GALIP exactly.\n",
    "\n",
    "class M_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's M_Block.\n",
    "    \n",
    "    PyTorch signature: M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p)\n",
    "    \n",
    "    Structure:\n",
    "        Residual: conv1(k,s,p) -> fuse1(DFBLK) -> conv2(k,s,p) -> fuse2(DFBLK)\n",
    "        Shortcut: 1x1 conv if in_ch != out_ch\n",
    "        Output: shortcut + residual\n",
    "    \n",
    "    Weight Initialization: PyTorch Conv2d uses Kaiming Uniform (He) by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, cond_dim, k, s, p):\n",
    "        super(M_Block, self).__init__()\n",
    "        \n",
    "        # PyTorch: nn.Conv2d(in_ch, mid_ch, k, s, p)\n",
    "        # TensorFlow: padding='same' when p = k//2 and s=1\n",
    "        # For k=3, s=1, p=1: this is standard 'same' padding\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.conv1 = layers.Conv2D(mid_ch, k, strides=s, padding='same', kernel_initializer='he_uniform')\n",
    "        self.fuse1 = DFBLK(cond_dim, mid_ch)\n",
    "        \n",
    "        # PyTorch: nn.Conv2d(mid_ch, out_ch, k, s, p)\n",
    "        self.conv2 = layers.Conv2D(out_ch, k, strides=s, padding='same', kernel_initializer='he_uniform')\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        \n",
    "        # Shortcut: 1x1 conv only if channel dimensions change\n",
    "        # PyTorch: nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, h, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: [B, H, W, in_ch] input feature map\n",
    "            c: [B, cond_dim] conditioning vector\n",
    "        Returns:\n",
    "            [B, H, W, out_ch] output feature map\n",
    "        \"\"\"\n",
    "        # Residual path: conv1 -> fuse1 -> conv2 -> fuse2\n",
    "        res = self.conv1(h)\n",
    "        res = self.fuse1(res, c)\n",
    "        res = self.conv2(res)\n",
    "        res = self.fuse2(res, c)\n",
    "        \n",
    "        # Shortcut path\n",
    "        if self.learnable_sc:\n",
    "            sc = self.c_sc(h)\n",
    "        else:\n",
    "            sc = h\n",
    "            \n",
    "        return sc + res\n",
    "\n",
    "\n",
    "class CLIP_Mapper(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's CLIP_Mapper.\n",
    "    \n",
    "    PyTorch signature: CLIP_Mapper(CLIP)\n",
    "    \n",
    "    Key behaviors:\n",
    "    1. Takes already-processed features [B, H, W, 768] (not raw images)\n",
    "    2. Injects learnable prompts at layers [1,2,3,4,5,6,7,8]\n",
    "    3. Does NOT apply post_layernorm (returns raw transformer output)\n",
    "    4. Returns spatial features [B, H, W, 768]\n",
    "    \n",
    "    PyTorch forward flow:\n",
    "        1. Reshape img [B, C, H, W] -> [B, H*W, C]\n",
    "        2. Add CLS token -> [B, H*W+1, C]\n",
    "        3. Add positional embedding\n",
    "        4. Apply ln_pre (pre-LayerNorm)\n",
    "        5. For each transformer layer (0-11):\n",
    "           - If in selected [1,2,3,4,5,6,7,8]: inject prompt, run layer, remove prompt\n",
    "           - Else: just run layer\n",
    "        6. Remove CLS, reshape back to [B, 768, H, W]\n",
    "    \n",
    "    Note: HuggingFace TF CLIP uses:\n",
    "        - embeddings.class_embedding: [768] raw tensor\n",
    "        - embeddings.position_embedding: Embedding layer (need .embeddings to get weights)\n",
    "        - pre_layrnorm: LayerNorm\n",
    "        - encoder.layers: list of transformer blocks\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_vision_model):\n",
    "        super(CLIP_Mapper, self).__init__()\n",
    "        self.vision_model = clip_vision_model.vision_model\n",
    "        # Freeze all CLIP parameters\n",
    "        self.vision_model.trainable = False\n",
    "        \n",
    "    def call(self, img_feats, prompts):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_feats: [B, H, W, 768] - already 768-channel features from conv_fuse\n",
    "                       (TF channels-last format)\n",
    "            prompts: [B, 8, 768] - learnable prompts for injection\n",
    "            \n",
    "        Returns:\n",
    "            [B, H, W, 768] - CLIP-mapped features (TF channels-last format)\n",
    "        \"\"\"\n",
    "        B = tf.shape(img_feats)[0]\n",
    "        H = tf.shape(img_feats)[1]\n",
    "        W = tf.shape(img_feats)[2]\n",
    "        \n",
    "        # Cast prompts to match dtype (PyTorch: prompts.type(self.dtype))\n",
    "        prompts = tf.cast(prompts, img_feats.dtype)\n",
    "        \n",
    "        # PyTorch: x = img.reshape(B, C, -1).permute(0, 2, 1) -> [B, H*W, C]\n",
    "        # TF: img_feats is [B, H, W, C], reshape to [B, H*W, C]\n",
    "        x = tf.reshape(img_feats, [B, H * W, 768])\n",
    "        \n",
    "        # Add CLS token\n",
    "        # PyTorch: torch.cat([class_embedding + zeros(...), x], dim=1)\n",
    "        cls_token = self.vision_model.embeddings.class_embedding  # [768]\n",
    "        cls_token = tf.cast(cls_token, x.dtype)\n",
    "        cls_token = tf.reshape(cls_token, [1, 1, 768])\n",
    "        cls_token = tf.tile(cls_token, [B, 1, 1])  # [B, 1, 768]\n",
    "        x = tf.concat([cls_token, x], axis=1)  # [B, H*W+1, 768]\n",
    "        \n",
    "        # Add positional embedding\n",
    "        # PyTorch: x = x + self.positional_embedding.to(x.dtype)\n",
    "        # HuggingFace TF: position_embedding is a tf.keras.layers.Embedding\n",
    "        # Access the weight matrix via .weights[0] (NOT .embeddings which doesn't exist in Keras!)\n",
    "        pos_embed = self.vision_model.embeddings.position_embedding.weights[0]  # [num_positions, 768]\n",
    "        pos_embed = tf.cast(pos_embed, x.dtype)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = x + pos_embed[:seq_len, :]\n",
    "        \n",
    "        # Pre-LayerNorm\n",
    "        # PyTorch: x = self.ln_pre(x)\n",
    "        x = self.vision_model.pre_layrnorm(x)\n",
    "        \n",
    "        # Process through transformer layers with prompt injection\n",
    "        # PyTorch: selected = [1,2,3,4,5,6,7,8], begin=0, end=12\n",
    "        selected = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        prompt_idx = 0\n",
    "        \n",
    "        for i, layer in enumerate(self.vision_model.encoder.layers):\n",
    "            if i in selected:\n",
    "                # PyTorch: prompt = prompts[:,prompt_idx,:].unsqueeze(0) -> [1, B, D]\n",
    "                # Then: x = torch.cat((x, prompt), dim=0) in LND format\n",
    "                # In TF (BLD format): concat on axis=1\n",
    "                p = prompts[:, prompt_idx, :]  # [B, 768]\n",
    "                p = tf.expand_dims(p, 1)  # [B, 1, 768]\n",
    "                x = tf.concat([x, p], axis=1)  # [B, L+1, 768]\n",
    "                \n",
    "                # Run transformer layer with training=False to ensure deterministic behavior\n",
    "                # (disables Dropout even though model is frozen)\n",
    "                layer_out = layer(x, output_attentions=False, training=False)\n",
    "                x = layer_out[0]\n",
    "                \n",
    "                # Remove prompt (last token)\n",
    "                # PyTorch: x = x[:-1,:,:]\n",
    "                x = x[:, :-1, :]\n",
    "                \n",
    "                prompt_idx += 1\n",
    "            else:\n",
    "                # Run transformer layer with training=False\n",
    "                layer_out = layer(x, output_attentions=False, training=False)\n",
    "                x = layer_out[0]\n",
    "        \n",
    "        # IMPORTANT: PyTorch CLIP_Mapper does NOT apply post_layernorm!\n",
    "        # It returns raw transformer output directly.\n",
    "        \n",
    "        # Remove CLS token and reshape back to spatial format\n",
    "        # PyTorch: x.permute(1,0,2)[:,1:,:].permute(0,2,1).reshape(-1, 768, grid, grid)\n",
    "        # TF: x is [B, L, 768], remove CLS -> [B, H*W, 768], reshape to [B, H, W, 768]\n",
    "        x = x[:, 1:, :]  # Remove CLS: [B, H*W, 768]\n",
    "        x = tf.reshape(x, [B, H, W, 768])\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP_Adapter(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's CLIP_Adapter.\n",
    "    \n",
    "    PyTorch signature:\n",
    "        CLIP_Adapter(in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num, CLIP)\n",
    "    \n",
    "    NetG instantiates with:\n",
    "        CLIP_Adapter(\n",
    "            in_ch=64,      # code_ch\n",
    "            mid_ch=32,     # mid_ch  \n",
    "            out_ch=64,     # code_ch\n",
    "            G_ch=512,      # ngf*8\n",
    "            CLIP_ch=768,   # CLIP hidden dim\n",
    "            cond_dim=612,  # cond_dim+nz (512+100)\n",
    "            k=3, s=1, p=1, # conv kernel params for M_Block\n",
    "            map_num=4,     # number of M_Blocks\n",
    "            CLIP=clip_model\n",
    "        )\n",
    "    \n",
    "    Structure:\n",
    "        1. fc_prompt: cond_dim -> CLIP_ch*8 (generate 8 prompts)\n",
    "        2. FBlocks: map_num M_Blocks processing features\n",
    "        3. conv_fuse: out_ch -> CLIP_ch (5x5 conv, pad=2)\n",
    "        4. CLIP_ViT: CLIP_Mapper with prompt injection\n",
    "        5. conv_out: 768 -> G_ch (5x5 conv, pad=2)\n",
    "        6. Output: conv_out(fuse_feat + 0.1*map_feat)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num, clip_model):\n",
    "        super(CLIP_Adapter, self).__init__()\n",
    "        self.CLIP_ch = CLIP_ch\n",
    "        \n",
    "        # FBlocks: ModuleList of M_Blocks\n",
    "        # PyTorch: first is M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p)\n",
    "        #          rest are M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p)\n",
    "        self.f_blocks = []\n",
    "        self.f_blocks.append(M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        for _ in range(map_num - 1):\n",
    "            self.f_blocks.append(M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        \n",
    "        # conv_fuse: project features to CLIP dimension\n",
    "        # PyTorch: nn.Conv2d(out_ch, CLIP_ch, 5, 1, 2) -> 5x5 kernel, stride=1, pad=2\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.conv_fuse = layers.Conv2D(CLIP_ch, 5, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        \n",
    "        # CLIP Mapper (ViT with prompt injection)\n",
    "        self.CLIP_ViT = CLIP_Mapper(clip_model)\n",
    "        \n",
    "        # conv_out: project back to generator channels\n",
    "        # PyTorch: nn.Conv2d(768, G_ch, 5, 1, 2) -> 5x5 kernel, stride=1, pad=2\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.conv_out = layers.Conv2D(G_ch, 5, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Prompt generator\n",
    "        # PyTorch: nn.Linear(cond_dim, CLIP_ch*8)\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Linear default (Kaiming Uniform)\n",
    "        self.fc_prompt = layers.Dense(CLIP_ch * 8, kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, out, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            out: [B, H, W, in_ch] - input features (from fc_code reshape in NetG)\n",
    "            c: [B, cond_dim] - conditioning vector (noise + text_embed concatenated)\n",
    "            \n",
    "        Returns:\n",
    "            [B, H, W, G_ch] - output features for generator\n",
    "        \"\"\"\n",
    "        # 1. Generate prompts: [B, CLIP_ch*8] -> [B, 8, CLIP_ch]\n",
    "        # PyTorch: prompts = self.fc_prompt(c).view(c.size(0), -1, self.CLIP_ch)\n",
    "        prompts = self.fc_prompt(c)\n",
    "        prompts = tf.reshape(prompts, [-1, 8, self.CLIP_ch])\n",
    "        \n",
    "        # 2. Process through FBlocks (map_num M_Blocks)\n",
    "        for FBlock in self.f_blocks:\n",
    "            out = FBlock(out, c)\n",
    "        \n",
    "        # 3. Project to CLIP dimension\n",
    "        fuse_feat = self.conv_fuse(out)  # [B, H, W, CLIP_ch]\n",
    "        \n",
    "        # 4. Run through CLIP Mapper with prompt injection\n",
    "        map_feat = self.CLIP_ViT(fuse_feat, prompts)  # [B, H, W, 768]\n",
    "        \n",
    "        # 5. Combine with 0.1 scaling factor and project to output channels\n",
    "        # PyTorch: return self.conv(fuse_feat + 0.1*map_feat)\n",
    "        # The 0.1 factor is crucial - it gates the CLIP-mapped features\n",
    "        return self.conv_out(fuse_feat + 0.1 * map_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. MODELS (Generator, Discriminator, Encoders)\n",
    "# ==============================================================================\n",
    "class CLIP_Text_Encoder(layers.Layer):\n",
    "    def __init__(self, clip_model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip_model: A TFCLIPModel instance (not just text_model)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.text_model = clip_model.text_model\n",
    "        self.text_projection = clip_model.text_projection  # This is a Dense layer\n",
    "        \n",
    "        # Freeze CLIP text model and projection\n",
    "        self.text_model.trainable = False\n",
    "        self.text_projection.trainable = False  # Explicit freeze\n",
    "        \n",
    "    def call(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [B, 77] tokenized text\n",
    "            attention_mask: [B, 77] optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            sent_emb: [B, 512] projected sentence embedding (for conditioning)\n",
    "            word_emb: [B, 77, 768] sequence features (for word-level attention if needed)\n",
    "        \"\"\"\n",
    "        # Get text model outputs\n",
    "        # HuggingFace's TFCLIPTextModel already applies final_layer_norm internally\n",
    "        outputs = self.text_model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # last_hidden_state: [B, 77, 768] (for clip-vit-base-patch32, hidden_size=768)\n",
    "        word_emb = outputs.last_hidden_state\n",
    "        \n",
    "        # Find EOT token position (highest token ID in each sequence)\n",
    "        # EOT token ID is 49407 for CLIP tokenizer\n",
    "        eot_indices = tf.argmax(tf.cast(input_ids, tf.int32), axis=-1)  # [B]\n",
    "        \n",
    "        # Gather EOT embeddings: [B, 768]\n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        batch_indices = tf.range(batch_size, dtype=tf.int64)\n",
    "        eot_indices = tf.cast(eot_indices, tf.int64)\n",
    "        gather_indices = tf.stack([batch_indices, eot_indices], axis=1)  # [B, 2]\n",
    "        \n",
    "        pooled_output = tf.gather_nd(word_emb, gather_indices)  # [B, 768]\n",
    "        \n",
    "        # Project to shared embedding space\n",
    "        # text_projection is a Dense layer [768 -> 512] for ViT-B/32\n",
    "        # IMPORTANT: Call the layer, don't matrix multiply!\n",
    "        sent_emb = self.text_projection(pooled_output)  # [B, 512]\n",
    "        \n",
    "        return sent_emb, word_emb\n",
    "    \n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        # Override to ensure projection is also not trained\n",
    "        # (text_model.trainable=False already handles its weights)\n",
    "        return []\n",
    "    \n",
    "    @property  \n",
    "    def non_trainable_weights(self):\n",
    "        return self.text_model.weights + self.text_projection.weights\n",
    "\n",
    "class CLIP_Image_Encoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Faithful TensorFlow replication of GALIP's CLIP_IMG_ENCODER.\n",
    "    \n",
    "    For clip-vit-base-patch32:\n",
    "    - Hidden size: 768\n",
    "    - Projection output: 512\n",
    "    - Grid size: 7x7 (224/32 = 7)\n",
    "    - 12 transformer layers\n",
    "    - Selected layers for local features: [1, 4, 8]\n",
    "    \n",
    "    Returns:\n",
    "        local_features: [B, 3, 7, 7, 768] - stacked local features from selected layers\n",
    "        global_emb: [B, 512] - projected CLS token embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = clip_model.vision_model\n",
    "        # Get projection layer (visual_projection in HuggingFace is a Dense layer!)\n",
    "        self.visual_projection = clip_model.visual_projection  # Dense [768 -> 512]\n",
    "        \n",
    "        # Freeze CLIP vision model AND projection layer explicitly\n",
    "        self.vision_model.trainable = False\n",
    "        self.visual_projection.trainable = False  # Critical: separate from vision_model\n",
    "        \n",
    "    def transf_to_CLIP_input(self, inputs):\n",
    "        \"\"\"\n",
    "        Transform generator output to CLIP input format.\n",
    "        Matches PyTorch GALIP preprocessing exactly.\n",
    "        \n",
    "        Args:\n",
    "            inputs: [B, H, W, 3] in range [-1, 1] (TF channels-last)\n",
    "            \n",
    "        Returns:\n",
    "            x: [B, 224, 224, 3] normalized for CLIP\n",
    "        \"\"\"\n",
    "        # PyTorch original: inputs*0.5+0.5 then ((inputs+1)*0.5-mean)/var\n",
    "        # This is equivalent to: (inputs + 1) / 2, then normalize\n",
    "        \n",
    "        # 1. Convert from [-1, 1] to [0, 1]\n",
    "        x = (inputs + 1.0) * 0.5\n",
    "        \n",
    "        # 2. Resize to (224, 224) using BICUBIC interpolation\n",
    "        # IMPORTANT: CLIP uses bicubic, TF defaults to bilinear\n",
    "        x = tf.image.resize(x, [224, 224], method='bicubic')\n",
    "        \n",
    "        # 3. Normalize with CLIP stats\n",
    "        mean = tf.constant([0.48145466, 0.4578275, 0.40821073], dtype=x.dtype)\n",
    "        std = tf.constant([0.26862954, 0.26130258, 0.27577711], dtype=x.dtype)\n",
    "        x = (x - mean) / std\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def call(self, img):\n",
    "        \"\"\"\n",
    "        Forward pass matching PyTorch CLIP_IMG_ENCODER.forward()\n",
    "        \n",
    "        Args:\n",
    "            img: [B, H, W, 3] image tensor in [-1, 1] range\n",
    "            \n",
    "        Returns:\n",
    "            local_features: [B, 3, 7, 7, 768] - features from layers [1, 4, 8]\n",
    "            global_emb: [B, 512] - projected global embedding\n",
    "        \"\"\"\n",
    "        x = self.transf_to_CLIP_input(img)\n",
    "        \n",
    "        # Get patch embeddings + CLS token + position embeddings\n",
    "        # HuggingFace TFCLIPVisionModel.embeddings returns [B, 50, 768]\n",
    "        # (1 CLS + 49 patches for 7x7 grid)\n",
    "        x = self.vision_model.embeddings(x)\n",
    "        \n",
    "        # Pre-LayerNorm (ln_pre in PyTorch)\n",
    "        x = self.vision_model.pre_layrnorm(x)\n",
    "        \n",
    "        # Extract local features at selected layers\n",
    "        # PyTorch GALIP uses layers [1, 4, 8] (0-indexed)\n",
    "        local_features = []\n",
    "        selected = [1, 4, 8]\n",
    "        \n",
    "        for i, layer in enumerate(self.vision_model.encoder.layers):\n",
    "            # Run transformer layer\n",
    "            layer_output = layer(x, output_attentions=False)\n",
    "            x = layer_output[0]  # [B, 50, 768]\n",
    "            \n",
    "            if i in selected:\n",
    "                # Extract spatial features (remove CLS token)\n",
    "                # x[:, 1:, :] -> [B, 49, 768]\n",
    "                grid = x[:, 1:, :]  # [B, 49, 768]\n",
    "                B = tf.shape(grid)[0]\n",
    "                # Reshape to spatial: [B, 7, 7, 768]\n",
    "                grid = tf.reshape(grid, [B, 7, 7, 768])\n",
    "                local_features.append(grid)\n",
    "        \n",
    "        # Post-LayerNorm on CLS token only (ln_post in PyTorch)\n",
    "        # x[:, 0, :] selects CLS token -> [B, 768]\n",
    "        cls_token = self.vision_model.post_layernorm(x[:, 0, :])  # [B, 768]\n",
    "        \n",
    "        # Project to shared embedding space\n",
    "        # IMPORTANT: visual_projection is a Dense LAYER, not a matrix!\n",
    "        # Call the layer instead of matrix multiply\n",
    "        global_emb = self.visual_projection(cls_token)  # [B, 512]\n",
    "        \n",
    "        # Stack local features: [B, 3, 7, 7, 768]\n",
    "        # PyTorch returns: torch.stack(local_features, dim=1)\n",
    "        local_features = tf.stack(local_features, axis=1)  # [B, 3, 7, 7, 768]\n",
    "        \n",
    "        return local_features, global_emb\n",
    "\n",
    "\n",
    "class NetG(Model):\n",
    "    \"\"\"\n",
    "    100% Faithful TensorFlow replication of PyTorch GALIP's NetG.\n",
    "    Target output: 64x64x3\n",
    "    \n",
    "    PyTorch get_G_in_out_chs(ngf=64, imsize=64):\n",
    "        in_out_pairs = [(512,512), (512,256), (256,128), (128,64)]\n",
    "        target_sizes = [8, 16, 32, 64]\n",
    "    \"\"\"\n",
    "    def __init__(self, ngf, nz, cond_dim, clip_model):\n",
    "        super(NetG, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.code_sz, self.code_ch, self.mid_ch = 7, 64, 32\n",
    "        self.CLIP_ch = 768\n",
    "        \n",
    "        self.fc_code = layers.Dense(self.code_sz * self.code_sz * self.code_ch, kernel_initializer='he_uniform')\n",
    "        \n",
    "        self.mapping = CLIP_Adapter(\n",
    "            in_ch=self.code_ch,\n",
    "            mid_ch=self.mid_ch,\n",
    "            out_ch=self.code_ch,\n",
    "            G_ch=ngf * 8,\n",
    "            CLIP_ch=self.CLIP_ch,\n",
    "            cond_dim=cond_dim + nz,\n",
    "            k=3, s=1, p=1,\n",
    "            map_num=4,\n",
    "            clip_model=clip_model\n",
    "        )\n",
    "        \n",
    "        # G_Blocks: get_G_in_out_chs(64, 64) -> [(512,512), (512,256), (256,128), (128,64)]\n",
    "        # Target sizes: [8, 16, 32, 64]\n",
    "        self.g_blocks = []\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 8))   # 512->512, 7->8\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 4))   # 512->256, 8->16\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 4, ngf * 2))   # 256->128, 16->32\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 2, ngf * 1))   # 128->64, 32->64\n",
    "        self.target_sizes = [8, 16, 32, 64]\n",
    "        \n",
    "        self.to_rgb = tf.keras.Sequential([\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Conv2D(3, 3, padding='same', kernel_initializer='he_uniform'),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        noise, c = inputs\n",
    "        cond = tf.concat([noise, c], axis=1)\n",
    "        \n",
    "        out = self.fc_code(noise)\n",
    "        out = tf.reshape(out, [-1, self.code_sz, self.code_sz, self.code_ch])\n",
    "        \n",
    "        out = self.mapping(out, cond)\n",
    "        \n",
    "        for block, target_size in zip(self.g_blocks, self.target_sizes):\n",
    "            out = block(out, cond, target_size)\n",
    "        \n",
    "        out = self.to_rgb(out)\n",
    "        out = tf.nn.tanh(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class NetD(Model):\n",
    "    \"\"\"\n",
    "    100% Faithful TensorFlow replication of PyTorch GALIP's NetD.\n",
    "    \n",
    "    Operates on CLIP local features [B, 3, 7, 7, 768].\n",
    "    Returns feature map for NetC to process.\n",
    "    \"\"\"\n",
    "    def __init__(self, ndf):\n",
    "        super(NetD, self).__init__()\n",
    "        self.d_blocks = []\n",
    "        self.d_blocks.append(D_Block(768, 768, is_res=True, clip_feat=True))\n",
    "        self.d_blocks.append(D_Block(768, 768, is_res=True, clip_feat=True))\n",
    "        \n",
    "        self.main = D_Block(768, 512, is_res=True, clip_feat=False)\n",
    "\n",
    "    def call(self, h):\n",
    "        # h: [B, 3, 7, 7, 768] stacked local features\n",
    "        out = h[:, 0]\n",
    "        for idx in range(len(self.d_blocks)):\n",
    "            out = self.d_blocks[idx](out, h[:, idx+1])\n",
    "        out = self.main(out)\n",
    "        return out  # [B, 7, 7, 512]\n",
    "\n",
    "\n",
    "class NetC(Model):\n",
    "\n",
    "    def __init__(self, ndf, cond_dim):\n",
    "        super(NetC, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.joint_conv = tf.keras.Sequential([\n",
    "            layers.Conv2D(128, 4, strides=1, padding='valid', use_bias=False, kernel_initializer='he_uniform'), # 7x7 -> 4x4\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Conv2D(1, 4, strides=1, padding='valid', use_bias=False, kernel_initializer='he_uniform') # 4x4 -> 1x1\n",
    "        ])\n",
    "\n",
    "    def call(self, out, cond):\n",
    "        # out: [B, 7, 7, 512]\n",
    "        # cond: [B, cond_dim]\n",
    "        \n",
    "        B = tf.shape(out)[0]\n",
    "        cond = tf.reshape(cond, [B, 1, 1, self.cond_dim])\n",
    "        cond = tf.tile(cond, [1, 7, 7, 1])\n",
    "        h_c = tf.concat([out, cond], axis=3)\n",
    "        return self.joint_conv(h_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def DiffAugment(x, policy='translation'):\n",
    "    \"\"\"\n",
    "    TensorFlow implementation of DiffAugment.\n",
    "    Supports 'color', 'translation', 'cutout'.\n",
    "    \"\"\"\n",
    "    if policy:\n",
    "        if 'color' in policy:\n",
    "            x = rand_brightness(x)\n",
    "            x = rand_saturation(x)\n",
    "            x = rand_contrast(x)\n",
    "        if 'translation' in policy:\n",
    "            x = rand_translation(x)\n",
    "        if 'cutout' in policy:\n",
    "            x = rand_cutout(x)\n",
    "    return x\n",
    "\n",
    "# --- Augmentation Primitives ---\n",
    "def rand_brightness(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=-0.5, maxval=0.5)\n",
    "    x = x + magnitude\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_saturation(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.0, maxval=2.0)\n",
    "    x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_contrast(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.5, maxval=1.5)\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    shift = int(64 * ratio)\n",
    "    \n",
    "    # Pad the image with reflection\n",
    "    x_padded = tf.pad(x, [[0, 0], [shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "    \n",
    "    # Vectorized Random Crop using crop_and_resize\n",
    "    padded_size = tf.cast(img_size + 2*shift, tf.float32)\n",
    "    max_offset = 2 * shift\n",
    "    \n",
    "    offsets_y = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    offsets_x = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    \n",
    "    offsets_y = tf.cast(offsets_y, tf.float32)\n",
    "    offsets_x = tf.cast(offsets_x, tf.float32)\n",
    "    \n",
    "    # Normalize coordinates to [0, 1] for crop_and_resize\n",
    "    y1 = offsets_y / padded_size\n",
    "    x1 = offsets_x / padded_size\n",
    "    y2 = (offsets_y + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    x2 = (offsets_x + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    \n",
    "    boxes = tf.stack([y1, x1, y2, x2], axis=1) # [B, 4]\n",
    "    box_indices = tf.range(batch_size)\n",
    "    \n",
    "    x_translated = tf.image.crop_and_resize(\n",
    "        x_padded, \n",
    "        boxes, \n",
    "        box_indices, \n",
    "        crop_size=[img_size, img_size]\n",
    "    )\n",
    "    \n",
    "    return x_translated\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    cutout_size = int(64 * ratio // 2) * 2\n",
    "    \n",
    "    iy, ix = tf.meshgrid(tf.range(img_size), tf.range(img_size), indexing='ij')\n",
    "    iy = tf.expand_dims(iy, 0) \n",
    "    ix = tf.expand_dims(ix, 0)\n",
    "    \n",
    "    offset_x = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    offset_y = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    \n",
    "    mask_x = tf.math.logical_and(ix >= offset_x, ix < offset_x + cutout_size)\n",
    "    mask_y = tf.math.logical_and(iy >= offset_y, iy < offset_y + cutout_size)\n",
    "    mask_box = tf.math.logical_and(mask_x, mask_y)\n",
    "    \n",
    "    mask_keep = tf.cast(tf.math.logical_not(mask_box), x.dtype)\n",
    "    mask_keep = tf.expand_dims(mask_keep, -1) \n",
    "    \n",
    "    return x * mask_keep\n",
    "\n",
    "def save_sample_images(generator, text_encoder, fixed_input_ids, fixed_noise, epoch, save_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a grid of images using fixed noise/text for consistency.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Encode text\n",
    "    text_embeds, _ = text_encoder(fixed_input_ids, training=False)\n",
    "    \n",
    "    # Generate\n",
    "    fake_imgs = generator([fixed_noise, text_embeds], training=False)\n",
    "    \n",
    "    # Convert to [0, 1] for plotting\n",
    "    fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "    fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "    \n",
    "    # Plot Grid\n",
    "    n = int(np.sqrt(len(fake_imgs)))\n",
    "    if n * n != len(fake_imgs): n = 8 \n",
    "    \n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(min(8, len(fake_imgs))):\n",
    "        plt.subplot(1, 8, i+1)\n",
    "        plt.imshow(fake_imgs[i])\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:03d}.png'))\n",
    "    plt.close()\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_images, input_ids, attention_mask, \n",
    "               generator, discriminator, net_c, \n",
    "               text_encoder, image_encoder,\n",
    "               g_optimizer, d_optimizer, \n",
    "               batch_size, z_dim, lambda_ma_gp=2.0, diff_augment_fn=None):\n",
    "    \n",
    "    # 1. Encode Text\n",
    "    text_embeds, _ = text_encoder(input_ids, attention_mask=attention_mask) # [B, 512]\n",
    "    \n",
    "    # 2. Generate Fake Images\n",
    "    noise = tf.random.normal([batch_size, z_dim])\n",
    "    fake_images = generator([noise, text_embeds], training=True)\n",
    "    \n",
    "    # 3. Augment (Optional)\n",
    "    if diff_augment_fn:\n",
    "        real_images_aug = diff_augment_fn(real_images)\n",
    "        fake_images_aug = diff_augment_fn(fake_images)\n",
    "    else:\n",
    "        real_images_aug = real_images\n",
    "        fake_images_aug = fake_images\n",
    "        \n",
    "    # 4. Encode Images (CLIP)\n",
    "    real_img_feats = image_encoder(real_images_aug)\n",
    "    fake_img_feats = image_encoder(fake_images_aug)\n",
    "    \n",
    "    # 5. Train Discriminator\n",
    "    with tf.GradientTape() as d_tape:\n",
    "        # D(Real)\n",
    "        d_real_score, d_real_feat = discriminator(real_img_feats, training=True)\n",
    "        # NetC(Real)\n",
    "        d_real_c = net_c(d_real_feat, text_embeds, training=True)\n",
    "        \n",
    "        # D(Fake)\n",
    "        d_fake_score, d_fake_feat = discriminator(fake_img_feats, training=True)\n",
    "        # NetC(Fake)\n",
    "        d_fake_c = net_c(d_fake_feat, text_embeds, training=True)\n",
    "        \n",
    "        # D(Mismatch) - Wrong Text\n",
    "        # Shuffle text embeddings\n",
    "        text_embeds_mis = tf.roll(text_embeds, shift=1, axis=0)\n",
    "        d_mis_c = net_c(d_real_feat, text_embeds_mis, training=True)\n",
    "        \n",
    "        # Losses\n",
    "        # Hinge Loss\n",
    "        errD_real = tf.reduce_mean(tf.nn.relu(1.0 - d_real_score))\n",
    "        errD_real_c = tf.reduce_mean(tf.nn.relu(1.0 - d_real_c))\n",
    "        \n",
    "        errD_fake = tf.reduce_mean(tf.nn.relu(1.0 + d_fake_score))\n",
    "        errD_fake_c = tf.reduce_mean(tf.nn.relu(1.0 + d_fake_c))\n",
    "        \n",
    "        errD_mis = tf.reduce_mean(tf.nn.relu(1.0 + d_mis_c))\n",
    "        \n",
    "        # MA-GP (Matching Aware Gradient Penalty)\n",
    "        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        interpolated_feat = alpha * d_real_feat + (1 - alpha) * d_fake_feat\n",
    "        \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated_feat)\n",
    "            out = net_c(interpolated_feat, text_embeds, training=True)\n",
    "            \n",
    "        grads = gp_tape.gradient(out, [interpolated_feat])[0]\n",
    "        grad_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]) + 1e-8)\n",
    "        ma_gp = tf.reduce_mean(tf.square(grad_norm - 1.0)) * lambda_ma_gp\n",
    "        \n",
    "        d_loss = errD_real + (errD_real_c + errD_fake_c + errD_mis) * 0.5 + errD_fake + ma_gp\n",
    "        \n",
    "    d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables + net_c.trainable_variables)\n",
    "    d_optimizer.apply_gradients(zip(d_grads, discriminator.trainable_variables + net_c.trainable_variables))\n",
    "    \n",
    "    # 6. Train Generator\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        fake_images = generator([noise, text_embeds], training=True)\n",
    "        fake_images_aug = diff_augment_fn(fake_images) if diff_augment_fn else fake_images\n",
    "        fake_img_feats = image_encoder(fake_images_aug)\n",
    "        \n",
    "        d_fake_score, d_fake_feat = discriminator(fake_img_feats, training=True)\n",
    "        d_fake_c = net_c(d_fake_feat, text_embeds, training=True)\n",
    "        \n",
    "        g_loss = -tf.reduce_mean(d_fake_score) - tf.reduce_mean(d_fake_c)\n",
    "        \n",
    "    g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    \n",
    "    return {\n",
    "        'd_loss': d_loss,\n",
    "        'g_loss': g_loss,\n",
    "        'ma_gp': ma_gp,\n",
    "        'errD_real': errD_real,\n",
    "        'errD_fake': errD_fake,\n",
    "        'errD_mis': errD_mis\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(dataset, args):\n",
    "    \"\"\"\n",
    "    Main training loop for GALIP with TensorBoard logging.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. Initialization & Logging Setup\n",
    "    # ==========================================================================\n",
    "    print(f\"--- Initializing Models (Image Size: {args['IMAGE_SIZE']}) ---\")\n",
    "    \n",
    "    # Load CLIP Models\n",
    "    print(\"--- Loading CLIP Models ---\")\n",
    "    try:\n",
    "        clip_vision_model = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        clip_text_model = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        print(\"✓ CLIP Models loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error loading CLIP models: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize Encoders\n",
    "    text_encoder = CLIP_Text_Encoder(clip_text_model)\n",
    "    image_encoder = CLIP_Image_Encoder(clip_vision_model)\n",
    "    \n",
    "    # Initialize GAN Models\n",
    "    generator = NetG(ngf=args['NGF'], nz=args['Z_DIM'], cond_dim=args['EMBED_DIM'], clip_model=clip_vision_model)\n",
    "    discriminator = NetD(ndf=args['NDF'])\n",
    "    net_c = NetC(ndf=args['NDF'], cond_dim=args['EMBED_DIM'])\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_G'], beta_1=0.0, beta_2=0.9)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_D'], beta_1=0.0, beta_2=0.9)\n",
    "\n",
    "    # Checkpoints\n",
    "    checkpoint_dir = os.path.join(args['RUN_DIR'], 'checkpoints')\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        generator=generator, discriminator=discriminator, net_c=net_c,\n",
    "        g_optimizer=g_optimizer, d_optimizer=d_optimizer\n",
    "    )\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    # TensorBoard Setup\n",
    "    log_dir = os.path.join(args['RUN_DIR'], 'logs')\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    try:\n",
    "        tensorboard_process = subprocess.Popen(\n",
    "            [sys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir]\n",
    "        )\n",
    "        print(f\"✓ TensorBoard launched (PID: {tensorboard_process.pid})\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not launch TensorBoard: {e}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. DiffAugment Setup\n",
    "    # ==========================================================================\n",
    "    diff_augment_fn = None\n",
    "    if args.get('USE_DIFFAUG', False):\n",
    "        print(f\"--- DiffAugment Enabled: {args['DIFFAUG_POLICY']} ---\")\n",
    "        def da_fn(imgs): return DiffAugment(imgs, policy=args['DIFFAUG_POLICY'])\n",
    "        diff_augment_fn = da_fn\n",
    "    else:\n",
    "        print(\"--- DiffAugment Disabled ---\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. Training Loop\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # Fixed noise/text for visualization\n",
    "    fixed_noise = tf.random.normal([8, args['Z_DIM']])\n",
    "    # Take first batch for fixed text\n",
    "    for fixed_text, _ in dataset.take(1):\n",
    "        # Tokenize fixed text\n",
    "        # We need to tokenize manually here or use the generator's output if it yields tokens\n",
    "        # Our generator yields (img, input_ids, attention_mask)\n",
    "        pass\n",
    "        \n",
    "    # Wait, dataset yields (img, input_ids, attention_mask)\n",
    "    # So we can just take it.\n",
    "    for _, fixed_input_ids, fixed_mask in dataset.take(1):\n",
    "        fixed_input_ids = fixed_input_ids[:8]\n",
    "        fixed_mask = fixed_mask[:8]\n",
    "        break\n",
    "\n",
    "    start_epoch = 0\n",
    "    if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint)\n",
    "        print(f\"Restored from {manager.latest_checkpoint}\")\n",
    "        \n",
    "    print(f\"Starting training for {args['MAX_EPOCH']} epochs...\")\n",
    "    \n",
    "    for epoch in range(start_epoch, args['MAX_EPOCH']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(dataset, desc=f\"Epoch {epoch+1}/{args['MAX_EPOCH']}\")\n",
    "        \n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        \n",
    "        for step, (real_images, input_ids, attention_mask) in enumerate(pbar):\n",
    "            \n",
    "            losses = train_step(\n",
    "                real_images, \n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                generator, \n",
    "                discriminator, \n",
    "                net_c, \n",
    "                text_encoder, \n",
    "                image_encoder,\n",
    "                g_optimizer, \n",
    "                d_optimizer, \n",
    "                args['BATCH_SIZE'], \n",
    "                args['Z_DIM'],\n",
    "                lambda_ma_gp=args['LAMBDA_MA_GP'],\n",
    "                diff_augment_fn=diff_augment_fn\n",
    "            )\n",
    "            \n",
    "            d_losses.append(losses['d_loss'])\n",
    "            g_losses.append(losses['g_loss'])\n",
    "            \n",
    "            # Update pbar\n",
    "            pbar.set_postfix({\n",
    "                'D': f\"{losses['d_loss']:.4f}\", \n",
    "                'G': f\"{losses['g_loss']:.4f}\",\n",
    "                'MA': f\"{losses['ma_gp']:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            with summary_writer.as_default():\n",
    "                step_global = epoch * len(dataset) + step\n",
    "                tf.summary.scalar('Loss/D', losses['d_loss'], step=step_global)\n",
    "                tf.summary.scalar('Loss/G', losses['g_loss'], step=step_global)\n",
    "                tf.summary.scalar('Loss/MA_GP', losses['ma_gp'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_Real', losses['errD_real'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_Fake', losses['errD_fake'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_Mis', losses['errD_mis'], step=step_global)\n",
    "\n",
    "        # End of Epoch\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"Epoch {epoch+1} done. D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}, Time: {time.time()-start_time:.1f}s\")\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        if (epoch + 1) % args['SAVE_FREQ'] == 0:\n",
    "            save_path = manager.save()\n",
    "            print(f\"Saved checkpoint for epoch {epoch+1}: {save_path}\")\n",
    "            \n",
    "        # Save Sample Images\n",
    "        if (epoch + 1) % args['SAMPLE_FREQ'] == 0:\n",
    "            save_sample_images(generator, text_encoder, fixed_input_ids, fixed_noise, epoch+1, os.path.join(args['RUN_DIR'], 'samples'))\n",
    "\n",
    "    print(\"Training Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define configuration for training\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create a unique run directory\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"./runs/{run_id}\"\n",
    "if not os.path.exists(run_dir):\n",
    "    os.makedirs(run_dir)\n",
    "\n",
    "# User provided config\n",
    "config = {\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    'NGF': 64,\n",
    "    'NDF': 64,\n",
    "    'Z_DIM': 100,\n",
    "    'EMBED_DIM': 512,        # CLIP embedding dimension\n",
    "    'LR_G': 0.0001,\n",
    "    'LR_D': 0.0004,          # MATCHED to LR_G to prevent D from overpowering G\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'MAX_EPOCH': 600,          # Updated to 600 for good results\n",
    "    'LAMBDA_MA_GP': 2.0,\n",
    "    'RUN_DIR': run_dir,\n",
    "    'SAVE_FREQ': 25,         # Save less frequently to save space\n",
    "    'SAMPLE_FREQ': 1,        # Sample every epoch\n",
    "    'USE_DIFFAUG': False,    # DISABLED: To strictly match official DF-GAN and avoid MA-GP conflicts\n",
    "    'DIFFAUG_POLICY': 'translation',\n",
    "    'N_SAMPLE': num_training_sample if 'num_training_sample' in locals() else 7370\n",
    "}\n",
    "\n",
    "# Save config for reproducibility\n",
    "with open(os.path.join(run_dir, 'config.json'), 'w') as f:\n",
    "    # Filter for JSON serializable values\n",
    "    json_config = {k: v for k, v in config.items() if isinstance(v, (int, float, str, list, bool))}\n",
    "    json.dump(json_config, f, indent=4)\n",
    "\n",
    "print(f\"Training Run Directory: {run_dir}\")\n",
    "print(f\"Config: {json.dumps(json_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dataset' is the tf.data.Dataset object you created in the notebook\n",
    "train(dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string\n",
    "\t\t\t\tindex: Test sample ID\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tinput_ids, attention_mask, index\n",
    "\t\t\"\"\"\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing dataset generator - decodes IDs to raw text\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_pickle('./dataset/testData.pkl')\n",
    "\t\tcaptions_ids = data['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\tchosen_caption_ids = captions_ids[i]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\tindex = data['ID'].values\n",
    "\t\tindex = np.asarray(index)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.repeat().batch(batch_size)\n",
    "\t\t\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference directory inside the run directory\n",
    "inference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "if not os.path.exists(inference_dir):\n",
    "    os.makedirs(inference_dir)\n",
    "print(f\"Inference Directory: {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, config):\n",
    "    print(\"--- Starting Inference ---\")\n",
    "    \n",
    "    # 1. Re-initialize Models\n",
    "    # We need to re-create the models to load weights into them\n",
    "    print(\"Loading models...\")\n",
    "    generator = NetG(ngf=config['NGF'], nz=config['Z_DIM'], cond_dim=config['EMBED_DIM'])\n",
    "    \n",
    "    # Use RNN_Encoder instead of ClipTextEncoder\n",
    "    # nhidden=256 to match training\n",
    "    # Force vocab_size=5429\n",
    "    vocab_size = 5429\n",
    "    text_encoder = RNN_Encoder(ntoken=vocab_size, ninput=300, nhidden=256, nlayers=1)\n",
    "    \n",
    "    # Load Pretrained Weights for Text Encoder\n",
    "    # Keras 3 requires .weights.h5 extension\n",
    "    damsm_weights_path = './damsm_checkpoints/text_encoder.weights.h5'\n",
    "    \n",
    "    if os.path.exists(damsm_weights_path):\n",
    "        print(f\"✓ Loading pretrained DAMSM weights from {damsm_weights_path}\")\n",
    "        dummy_input = tf.zeros((1, 20), dtype=tf.int32)\n",
    "        text_encoder(dummy_input)\n",
    "        try:\n",
    "            text_encoder.load_weights(damsm_weights_path)\n",
    "            print(\"✓ Weights loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error loading weights: {e}\")\n",
    "    else:\n",
    "        print(\"⚠ WARNING: No pretrained DAMSM weights found! Encoder is random.\")\n",
    "\n",
    "    # Dummy call to build the model (optional but good practice)\n",
    "    # generator.build((None, config['Z_DIM'])) \n",
    "    \n",
    "    # 2. Load Checkpoint\n",
    "    checkpoint_dir = os.path.join(config['RUN_DIR'], 'checkpoints')\n",
    "    \n",
    "    # We need to restore the generator. \n",
    "    # Note: We must define the checkpoint object exactly as it was saved to restore correctly,\n",
    "    # or use expect_partial() if we only care about specific parts (like generator).\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    \n",
    "    latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_ckpt:\n",
    "        print(f\"Loading weights from: {latest_ckpt}\")\n",
    "        status = checkpoint.restore(latest_ckpt).expect_partial()\n",
    "        status.assert_existing_objects_matched()\n",
    "        print(\"✓ Weights loaded successfully\")\n",
    "    else:\n",
    "        print(\"⚠ NO CHECKPOINT FOUND! Generating with random weights (Garbage output).\")\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    total_images = 0\n",
    "    pbar = tqdm(total=NUM_TEST, desc='Generating images', unit='img')\n",
    "    \n",
    "    for step, (caption_texts, image_ids) in enumerate(dataset):\n",
    "        # caption_texts: [B, 10] (list of strings? No, dataset generator returns strings?)\n",
    "        # Wait, testing_dataset_generator returns (caption_texts, index)\n",
    "        # caption_texts is a list of strings.\n",
    "        # We need to tokenize them.\n",
    "        \n",
    "        # Actually, let's check testing_dataset_generator.\n",
    "        # It returns caption_texts which are strings.\n",
    "        # We need to convert to IDs.\n",
    "        \n",
    "        # Tokenize\n",
    "        # We need to map words to IDs using word2Id_dict\n",
    "        # This is slow in loop, but fine for inference.\n",
    "        \n",
    "        batch_size_curr = len(caption_texts)\n",
    "        input_ids_list = []\n",
    "        \n",
    "        for cap in caption_texts:\n",
    "            # cap is a tensor string, need to decode\n",
    "            cap_str = cap.numpy().decode('utf-8')\n",
    "            \n",
    "            # Preprocess (simple split and map)\n",
    "            # Remove punctuation\n",
    "            cap_str = cap_str.translate(str.maketrans('', '', string.punctuation))\n",
    "            words = cap_str.lower().split()\n",
    "            \n",
    "            ids = []\n",
    "            for w in words:\n",
    "                if w in word2Id_dict:\n",
    "                    ids.append(word2Id_dict[w])\n",
    "                else:\n",
    "                    ids.append(word2Id_dict['<RARE>'])\n",
    "            \n",
    "            # Pad/Truncate\n",
    "            if len(ids) > MAX_SEQ_LENGTH:\n",
    "                ids = ids[:MAX_SEQ_LENGTH]\n",
    "            else:\n",
    "                ids = ids + [word2Id_dict['<PAD>']] * (MAX_SEQ_LENGTH - len(ids))\n",
    "            \n",
    "            input_ids_list.append(ids)\n",
    "            \n",
    "        input_ids = tf.convert_to_tensor(input_ids_list, dtype=tf.int32)\n",
    "        \n",
    "        # Encode Text\n",
    "        # Compute lengths\n",
    "        cap_lens = tf.reduce_sum(tf.cast(tf.not_equal(input_ids, 0), tf.int32), axis=1)\n",
    "        _, text_embeddings = text_encoder(input_ids, cap_lens=cap_lens, training=False)\n",
    "        \n",
    "        # Generate Noise\n",
    "        noise = tf.random.normal([batch_size_curr, config['Z_DIM']])\n",
    "        \n",
    "        # Generate Images\n",
    "        fake_imgs = generator([noise, text_embeddings], training=False)\n",
    "        \n",
    "        # Post-process\n",
    "        fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "        fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "        \n",
    "        # Save Images\n",
    "        for i in range(batch_size_curr):\n",
    "            img_id = image_ids[i].numpy().decode('utf-8')\n",
    "            save_path = os.path.join(inference_dir, f'inference_{img_id}.jpg')\n",
    "            plt.imsave(save_path, fake_imgs[i])\n",
    "            total_images += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "    pbar.close()\n",
    "    print(f\"Inference Complete. Saved {total_images} images to {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "\t\tprint(f'⚠ No images found in {inference_dir}')\n",
    "\t\tprint('Please run the inference cell first!')\n",
    "else:\n",
    "\t\t# Randomly sample 20 images\n",
    "\t\tnp.random.seed(42)  # For reproducibility\n",
    "\t\tnum_samples = min(20, len(image_files))\n",
    "\t\tsample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "\t\tsample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "\t\t# Create 4x5 grid\n",
    "\t\tfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\t\taxes = axes.flatten()\n",
    "\n",
    "\t\tfor idx, img_path in enumerate(sample_files):\n",
    "\t\t\t\t# Extract image ID from filename\n",
    "\t\t\t\timg_id = int(Path(img_path).stem.split('_')[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Find caption\n",
    "\t\t\t\tcaption_idx = np.where(test_ids == img_id)[0][0]\n",
    "\t\t\t\tcaption_ids = test_captions[caption_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode caption\n",
    "\t\t\t\tcaption_text = ''\n",
    "\t\t\t\tfor word_id in caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':\n",
    "\t\t\t\t\t\t\t\tcaption_text += word + ' '\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Load and display image\n",
    "\t\t\t\timg = plt.imread(img_path)\n",
    "\t\t\t\taxes[idx].imshow(img)\n",
    "\t\t\t\taxes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\t# Hide unused subplots if less than 20 images\n",
    "\t\tfor idx in range(num_samples, 20):\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\tprint(f'\\nTotal generated images: {len(image_files)}')\n",
    "\t\tprint(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
