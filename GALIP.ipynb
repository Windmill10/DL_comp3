{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttry:\n",
    "\t\t# Restrict TensorFlow to only use the first GPU\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\t\t# Currently, memory growth needs to be the same across GPUs\n",
    "\t\tfor gpu in gpus:\n",
    "\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\n",
    "\t\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\t\tprint(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\texcept RuntimeError as e:\n",
    "\t\t# Memory growth must be set before GPUs have been initialized\n",
    "\t\tprint(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¬∂</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using CLIP tokenizer (sent2IdList removed)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úì Using CLIP tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¬∂</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CLIP Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. DATASET GENERATOR (Adapted for CLIP)\n",
    "# ==============================================================================\n",
    "\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAX_SEQ_LENGTH = 77 # CLIP default\n",
    "\n",
    "# Initialize CLIP Tokenizer\n",
    "try:\n",
    "    from transformers import CLIPTokenizer\n",
    "    # Use the same model name as the vision/text models we will load later\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    print(\"‚úì CLIP Tokenizer loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading CLIP Tokenizer: {e}\")\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "    \"\"\"\n",
    "    Data generator using CLIP Tokenizer\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string\n",
    "        image_path: Path to image file\n",
    "    \n",
    "    Returns:\n",
    "        img, input_ids, attention_mask\n",
    "    \"\"\"\n",
    "    # ============= IMAGE PROCESSING =============\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    # Normalize to [-1, 1] to match generator's tanh output\n",
    "    img = (img * 2.0) - 1.0\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    # ============= TEXT PROCESSING =============\n",
    "    # Tokenize using CLIP\n",
    "    # We use py_function because tokenizer is Python code\n",
    "    def tokenize(text):\n",
    "        text = text.numpy().decode('utf-8')\n",
    "        # CLIP Tokenizer handles padding and truncation\n",
    "        enc = tokenizer(\n",
    "            text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=MAX_SEQ_LENGTH, \n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return enc['input_ids'][0], enc['attention_mask'][0]\n",
    "        \n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize, \n",
    "        inp=[caption_text], \n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    input_ids.set_shape([MAX_SEQ_LENGTH])\n",
    "    attention_mask.set_shape([MAX_SEQ_LENGTH])\n",
    "    \n",
    "    return img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator, word2Id_dict, id2word_dict, expand_captions=True):\n",
    "    \"\"\"\n",
    "    Dataset generator that decodes IDs to text for CLIP\n",
    "    \"\"\"\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions_ids = df['Captions'].values\n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    print(f\"Loading dataset from {filenames}...\")\n",
    "    \n",
    "    # Helper to decode IDs to text\n",
    "    def decode_ids(id_list):\n",
    "        words = []\n",
    "        for i in id_list:\n",
    "            word = id2word_dict.get(str(i), '')\n",
    "            if word and word != '<PAD>':\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    all_captions_text = []\n",
    "    all_paths = []\n",
    "\n",
    "    if expand_captions:\n",
    "        # Expand: Create a sample for every caption\n",
    "        print(\"Expanding captions (one sample per caption)...\")\n",
    "        for caps, path in zip(captions_ids, image_paths):\n",
    "            for cap_ids in caps:\n",
    "                text = decode_ids(cap_ids)\n",
    "                all_captions_text.append(text)\n",
    "                all_paths.append(path)\n",
    "    else:\n",
    "        # Random Select: Pick one random caption per image (static for this generator call)\n",
    "        # Note: Ideally we'd do random selection at runtime, but decoding text in graph is hard.\n",
    "        # For simplicity/performance, we pick one now. \n",
    "        # To get true randomness per epoch, we'd need to re-create the dataset or use py_function logic.\n",
    "        print(\"Selecting one random caption per image...\")\n",
    "        for caps, path in zip(captions_ids, image_paths):\n",
    "            cap_ids = random.choice(caps)\n",
    "            text = decode_ids(cap_ids)\n",
    "            all_captions_text.append(text)\n",
    "            all_paths.append(path)\n",
    "            \n",
    "    all_captions_text = np.array(all_captions_text)\n",
    "    all_paths = np.array(all_paths)\n",
    "    \n",
    "    print(f\"Dataset size: {len(all_captions_text)} samples\")\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_captions_text, all_paths))\n",
    "    dataset = dataset.shuffle(len(all_captions_text))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./dataset/text2ImgData.pkl...\n",
      "Expanding captions (one sample per caption)...\n",
      "Dataset size: 70504 samples\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "# We use expand_captions=False to keep epoch size manageable (same as number of images)\n",
    "# or True for more training data. Let's use False for faster epochs initially, or True for better quality.\n",
    "# Given the small dataset (7k images), expanding is probably better (70k samples).\n",
    "dataset = dataset_generator(\n",
    "    data_path + '/text2ImgData.pkl', \n",
    "    BATCH_SIZE, \n",
    "    training_data_generator,\n",
    "    word2Id_dict,\n",
    "    id2word_dict,\n",
    "    expand_captions=True \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¬∂</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¬∂</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Transformers Version: 4.57.3\n",
      "PyTorch-compatible settings:\n",
      "  ADAM_EPSILON = 1e-08\n",
      "  LAYER_NORM_EPSILON = 1e-05\n",
      "  Weight init: he_uniform (Kaiming Uniform)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "from transformers import TFCLIPVisionModel, TFCLIPTextModel, CLIPProcessor, CLIPConfig\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"Transformers Version:\", transformers.__version__)\n",
    "except ImportError:\n",
    "    print(\"Transformers not installed. Please install it.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PYTORCH-TENSORFLOW COMPATIBILITY CONSTANTS\n",
    "# ==============================================================================\n",
    "# These constants ensure numerical equivalence between PyTorch and TensorFlow\n",
    "# implementations of GALIP.\n",
    "\n",
    "# 1. Optimizer epsilon: PyTorch Adam default is 1e-8, TensorFlow default is 1e-7\n",
    "#    Using 1e-7 can cause subtle numerical divergence over training.\n",
    "ADAM_EPSILON = 1e-8  # Match PyTorch default\n",
    "\n",
    "# 2. LayerNorm epsilon: PyTorch default is 1e-5, TensorFlow default is 1e-3\n",
    "#    This affects CLIP and any custom LayerNorm layers.\n",
    "LAYER_NORM_EPSILON = 1e-5  # Match PyTorch default\n",
    "\n",
    "# 3. Weight initialization: PyTorch Linear/Conv2d use Kaiming Uniform (He)\n",
    "#    TensorFlow defaults to Glorot Uniform (Xavier).\n",
    "#    All our layers now use kernel_initializer='he_uniform' explicitly.\n",
    "\n",
    "print(f\"PyTorch-compatible settings:\")\n",
    "print(f\"  ADAM_EPSILON = {ADAM_EPSILON}\")\n",
    "print(f\"  LAYER_NORM_EPSILON = {LAYER_NORM_EPSILON}\")\n",
    "print(f\"  Weight init: he_uniform (Kaiming Uniform)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# HELPER FUNCTION: Create PyTorch-compatible Adam optimizer\n",
    "# ==============================================================================\n",
    "def create_pytorch_compatible_adam(learning_rate, beta_1=0.0, beta_2=0.9):\n",
    "    \"\"\"\n",
    "    Creates an Adam optimizer with PyTorch-equivalent settings.\n",
    "    \n",
    "    PyTorch defaults:\n",
    "        - lr: required\n",
    "        - betas: (0.9, 0.999) but GALIP uses (0.0, 0.9)\n",
    "        - eps: 1e-8\n",
    "        - weight_decay: 0\n",
    "        - amsgrad: False\n",
    "    \n",
    "    TensorFlow defaults that differ:\n",
    "        - epsilon: 1e-7 (10x larger than PyTorch!)\n",
    "    \n",
    "    Args:\n",
    "        learning_rate: Learning rate\n",
    "        beta_1: First moment decay (default 0.0 for GAN training)\n",
    "        beta_2: Second moment decay (default 0.9 for GAN training)\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.optimizers.Adam with PyTorch-equivalent settings\n",
    "    \"\"\"\n",
    "    return tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=beta_1,\n",
    "        beta_2=beta_2,\n",
    "        epsilon=ADAM_EPSILON  # CRITICAL: Match PyTorch 1e-8\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç Deep Inspection of openai/clip-vit-base-patch32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded successfully.\n",
      "\n",
      "[Checking visual_projection]\n",
      "  ‚úì Found via: Inside 'clip' Wrapper\n",
      "  ‚úì Layer Type: Dense\n",
      "  ‚Ä¢ Weight tensors found: 1\n",
      "  ‚úì PASS: Only kernel found. Bias=False. (100% PyTorch Faithful)\n",
      "\n",
      "[Checking text_projection]\n",
      "  ‚úì Found via: Inside 'clip' Wrapper\n",
      "  ‚úì Layer Type: Dense\n",
      "  ‚Ä¢ Weight tensors found: 1\n",
      "  ‚úì PASS: Only kernel found. Bias=False. (100% PyTorch Faithful)\n",
      "\n",
      "[LayerNorm Epsilon Analysis]\n",
      "  ‚ö† Could not locate vision_model.pre_layrnorm\n",
      "  Text LN epsilon:   1e-05\n",
      "  ‚úì Text Epsilon matches PyTorch (1e-5).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFCLIPModel, CLIPConfig\n",
    "\n",
    "def robust_verify_clip(model_name=\"openai/clip-vit-base-patch32\"):\n",
    "    print(f\"--- üîç Deep Inspection of {model_name} ---\")\n",
    "    \n",
    "    # 1. Load Model\n",
    "    try:\n",
    "        model = TFCLIPModel.from_pretrained(model_name)\n",
    "        print(\"‚úì Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CRITICAL ERROR loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Helper to find layers recursively (Handles the 'clip' wrapper)\n",
    "    def find_layer_recursive(model, layer_name):\n",
    "        # 1. Check top-level attributes\n",
    "        if hasattr(model, layer_name):\n",
    "            return getattr(model, layer_name), \"Top-level Attribute\"\n",
    "            \n",
    "        # 2. Check direct children layers\n",
    "        for layer in model.layers:\n",
    "            if layer.name == layer_name:\n",
    "                return layer, \"Direct Layer List\"\n",
    "                \n",
    "        # 3. CRITICAL: Check inside 'clip' wrapper if it exists\n",
    "        # This fixes the specific error you are seeing\n",
    "        if hasattr(model, 'clip'):\n",
    "            clip_layer = model.clip\n",
    "            if hasattr(clip_layer, layer_name):\n",
    "                return getattr(clip_layer, layer_name), \"Inside 'clip' Wrapper\"\n",
    "        \n",
    "        # 4. Check inside any layer named 'clip' in the layers list\n",
    "        for layer in model.layers:\n",
    "            if layer.name == 'clip':\n",
    "                if hasattr(layer, layer_name):\n",
    "                    return getattr(layer, layer_name), \"Inside 'clip' Layer (List)\"\n",
    "                    \n",
    "        return None, \"NOT FOUND\"\n",
    "\n",
    "    # 2. Inspect Projections (Visual & Text)\n",
    "    # ---------------------------------------------------------\n",
    "    for proj_name in [\"visual_projection\", \"text_projection\"]:\n",
    "        print(f\"\\n[Checking {proj_name}]\")\n",
    "        layer, source = find_layer_recursive(model, proj_name)\n",
    "        \n",
    "        if layer is None:\n",
    "            print(f\"  ‚ùå FAIL: Layer '{proj_name}' DOES NOT EXIST.\")\n",
    "            print(f\"     Top-level layers: {[l.name for l in model.layers]}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  ‚úì Found via: {source}\")\n",
    "        print(f\"  ‚úì Layer Type: {type(layer).__name__}\")\n",
    "        \n",
    "        # Check Bias\n",
    "        weights = layer.weights\n",
    "        print(f\"  ‚Ä¢ Weight tensors found: {len(weights)}\")\n",
    "        \n",
    "        if len(weights) == 1:\n",
    "            print(\"  ‚úì PASS: Only kernel found. Bias=False. (100% PyTorch Faithful)\")\n",
    "        elif len(weights) == 2:\n",
    "            bias_tensor = weights[1]\n",
    "            bias_sum = tf.reduce_sum(tf.abs(bias_tensor)).numpy()\n",
    "            print(f\"  ‚ö† WARNING: Bias vector exists!\")\n",
    "            print(f\"    Sum of absolute values: {bias_sum}\")\n",
    "            \n",
    "            if bias_sum < 1e-9:\n",
    "                print(\"    ‚úì PASS (Soft): Bias exists but is effectively ZERO.\")\n",
    "            else:\n",
    "                print(\"    ‚ùå FAIL: Non-zero bias found! Divergence risk.\")\n",
    "\n",
    "    # 3. LayerNorm Epsilon Check\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[LayerNorm Epsilon Analysis]\")\n",
    "    \n",
    "    # We use the same finder for the sub-models\n",
    "    vision_model, _ = find_layer_recursive(model, \"vision_model\")\n",
    "    text_model, _ = find_layer_recursive(model, \"text_model\")\n",
    "    \n",
    "    # Check Vision Pre-LayerNorm\n",
    "    if vision_model and hasattr(vision_model, \"pre_layrnorm\"):\n",
    "        eps = vision_model.pre_layrnorm.epsilon\n",
    "        print(f\"  Vision LN epsilon: {eps}\")\n",
    "        if abs(eps - 1e-5) < 1e-9:\n",
    "            print(\"  ‚úì Vision Epsilon matches PyTorch (1e-5).\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå FAIL: Vision Epsilon mismatch! Found {eps}.\")\n",
    "    else:\n",
    "        print(\"  ‚ö† Could not locate vision_model.pre_layrnorm\")\n",
    "\n",
    "    # Check Text Final-LayerNorm\n",
    "    if text_model and hasattr(text_model, \"final_layer_norm\"):\n",
    "        eps = text_model.final_layer_norm.epsilon\n",
    "        print(f\"  Text LN epsilon:   {eps}\")\n",
    "        if abs(eps - 1e-5) < 1e-9:\n",
    "            print(\"  ‚úì Text Epsilon matches PyTorch (1e-5).\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå FAIL: Text Epsilon mismatch! Found {eps}.\")\n",
    "    else:\n",
    "         print(\"  ‚ö† Could not locate text_model.final_layer_norm\")\n",
    "\n",
    "# Run the verification\n",
    "robust_verify_clip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. BASIC BLOCKS (DF-GAN & GALIP Components)\n",
    "# ==============================================================================\n",
    "\n",
    "class Affine(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's Affine layer.\n",
    "    \n",
    "    PyTorch signature: Affine(cond_dim, num_features)\n",
    "    \n",
    "    PyTorch structure:\n",
    "        fc_gamma: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        fc_beta:  Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "    \n",
    "    Initialization:\n",
    "        fc_gamma.linear2: weight=0, bias=1 (so initial gamma=1, identity scaling)\n",
    "        fc_beta.linear2:  weight=0, bias=0 (so initial beta=0, no shift)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, num_features):\n",
    "        super(Affine, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # fc_gamma: 2-layer MLP\n",
    "        # PyTorch: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        # First layer: cond_dim -> num_features, he_uniform init (matches PyTorch Linear default)\n",
    "        # Second layer: num_features -> num_features, zeros weight, ones bias\n",
    "        self.gamma_linear1 = layers.Dense(num_features, kernel_initializer='he_uniform')\n",
    "        self.gamma_linear2 = layers.Dense(\n",
    "            num_features, \n",
    "            kernel_initializer='zeros',\n",
    "            bias_initializer='ones'\n",
    "        )\n",
    "        \n",
    "        # fc_beta: 2-layer MLP\n",
    "        # PyTorch: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        # First layer: cond_dim -> num_features, he_uniform init (matches PyTorch Linear default)\n",
    "        # Second layer: num_features -> num_features, zeros weight, zeros bias\n",
    "        self.beta_linear1 = layers.Dense(num_features, kernel_initializer='he_uniform')\n",
    "        self.beta_linear2 = layers.Dense(\n",
    "            num_features,\n",
    "            kernel_initializer='zeros',\n",
    "            bias_initializer='zeros'\n",
    "        )\n",
    "\n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, H, W, C] feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "        \"\"\"\n",
    "        # Compute gamma (scale)\n",
    "        gamma = self.gamma_linear1(y)\n",
    "        gamma = tf.nn.relu(gamma)\n",
    "        gamma = self.gamma_linear2(gamma)  # [B, num_features]\n",
    "        \n",
    "        # Compute beta (shift)\n",
    "        beta = self.beta_linear1(y)\n",
    "        beta = tf.nn.relu(beta)\n",
    "        beta = self.beta_linear2(beta)  # [B, num_features]\n",
    "        \n",
    "        # Reshape for broadcasting: [B, 1, 1, C]\n",
    "        gamma = tf.reshape(gamma, [-1, 1, 1, self.num_features])\n",
    "        beta = tf.reshape(beta, [-1, 1, 1, self.num_features])\n",
    "        \n",
    "        return gamma * x + beta\n",
    "\n",
    "\n",
    "class DFBLK(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's DFBLK.\n",
    "    \n",
    "    PyTorch signature: DFBLK(cond_dim, in_ch)\n",
    "    \n",
    "    Structure:\n",
    "        affine0 -> LeakyReLU(0.2) -> affine1 -> LeakyReLU(0.2)\n",
    "    \n",
    "    NO convolutions - just two affine transforms with activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch):\n",
    "        super(DFBLK, self).__init__()\n",
    "        # PyTorch: self.affine0 = Affine(cond_dim, in_ch)\n",
    "        # Pass cond_dim to match PyTorch signature exactly\n",
    "        self.affine0 = Affine(cond_dim, in_ch)\n",
    "        self.affine1 = Affine(cond_dim, in_ch)\n",
    "\n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, H, W, C] feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "        Returns:\n",
    "            [B, H, W, C] transformed feature map\n",
    "        \"\"\"\n",
    "        h = self.affine0(x, y)\n",
    "        h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "        h = self.affine1(h, y)\n",
    "        h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "class G_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's G_Block.\n",
    "    \n",
    "    PyTorch signature: G_Block(cond_dim, in_ch, out_ch, imsize)\n",
    "    \n",
    "    Structure:\n",
    "        1. Interpolate to target size\n",
    "        2. Residual path: fuse1(DFBLK) -> c1(conv) -> fuse2(DFBLK) -> c2(conv)\n",
    "        3. Shortcut path: c_sc(1x1 conv) if in_ch != out_ch\n",
    "        4. Output: shortcut + residual\n",
    "    \n",
    "    Note: imsize is handled dynamically via target_size parameter in call().\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch, out_ch):\n",
    "        super(G_Block, self).__init__()\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        \n",
    "        # PyTorch: nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.c1 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        self.c2 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        \n",
    "        # PyTorch: DFBLK(cond_dim, in_ch) and DFBLK(cond_dim, out_ch)\n",
    "        self.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        \n",
    "        # Shortcut: 1x1 conv only if channel dimensions change\n",
    "        # PyTorch: nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, h, y, target_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: [B, H, W, in_ch] input feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "            target_size: int, target spatial size for interpolation\n",
    "        Returns:\n",
    "            [B, target_size, target_size, out_ch] output feature map\n",
    "        \"\"\"\n",
    "        # PyTorch: h = F.interpolate(h, size=(self.imsize, self.imsize))\n",
    "        h = tf.image.resize(h, [target_size, target_size], method='nearest')\n",
    "        \n",
    "        # Residual path: fuse1 -> c1 -> fuse2 -> c2\n",
    "        # PyTorch: h = self.fuse1(h, y); h = self.c1(h); h = self.fuse2(h, y); h = self.c2(h)\n",
    "        res = self.fuse1(h, y)\n",
    "        res = self.c1(res)\n",
    "        res = self.fuse2(res, y)\n",
    "        res = self.c2(res)\n",
    "        \n",
    "        # Shortcut path\n",
    "        if self.learnable_sc:\n",
    "            sc = self.c_sc(h)\n",
    "        else:\n",
    "            sc = h\n",
    "            \n",
    "        return sc + res\n",
    "\n",
    "\n",
    "class D_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's D_Block.\n",
    "    \n",
    "    PyTorch signature: D_Block(fin, fout, k, s, p, res, CLIP_feat)\n",
    "    \n",
    "    PyTorch structure:\n",
    "        conv_r: Conv2D(fin, fout, k, s, p, bias=False) -> LeakyReLU(0.2) -> Conv2D(fout, fout, k, s, p, bias=False) -> LeakyReLU(0.2)\n",
    "        conv_s: Conv2D(fin, fout, 1, stride=1, padding=0) for shortcut\n",
    "        gamma: learnable scalar for residual (init=0)\n",
    "        beta: learnable scalar for CLIP features (init=0)\n",
    "    \n",
    "    Note: All PyTorch D_Block instantiations use k=3, s=1, p=1, so we hardcode these.\n",
    "    \"\"\"\n",
    "    def __init__(self, fin, fout, is_down=False, is_res=True, clip_feat=False):\n",
    "        super(D_Block, self).__init__()\n",
    "        self.is_res = is_res\n",
    "        self.clip_feat = clip_feat\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        \n",
    "        # Main conv path (PyTorch: k=3, s=1, p=1)\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.conv_r1 = layers.Conv2D(fout, 3, padding='same', use_bias=False, kernel_initializer='he_uniform')\n",
    "        self.conv_r2 = layers.Conv2D(fout, 3, padding='same', use_bias=False, kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Shortcut conv (PyTorch: 1x1, stride=1, padding=0)\n",
    "        # CRITICAL: padding='valid' to match PyTorch padding=0\n",
    "        self.conv_s = layers.Conv2D(fout, 1, padding='valid', kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Learnable scalars (initialized to 0, matching PyTorch torch.zeros(1))\n",
    "        if is_res:\n",
    "            self.gamma = tf.Variable(0.0, trainable=True, name='gamma')\n",
    "        if clip_feat:\n",
    "            self.beta = tf.Variable(0.0, trainable=True, name='beta')\n",
    "\n",
    "    def call(self, x, clip_f=None):\n",
    "        # Residual path\n",
    "        res = self.conv_r1(x)\n",
    "        res = tf.nn.leaky_relu(res, alpha=0.2)\n",
    "        res = self.conv_r2(res)\n",
    "        res = tf.nn.leaky_relu(res, alpha=0.2)\n",
    "        \n",
    "        # Shortcut\n",
    "        if self.learned_shortcut:\n",
    "            x = self.conv_s(x)\n",
    "        \n",
    "        # Combine based on flags\n",
    "        out = x\n",
    "        if self.is_res:\n",
    "\n",
    "            out = out + self.gamma * res     \n",
    "\n",
    "        if self.clip_feat and clip_f is not None:            \n",
    "            out = out + self.beta * clip_f\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CLIP ADAPTER (Robust Fix)\n",
    "# ==============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_layer_safe(model, layer_name):\n",
    "    \"\"\"\n",
    "    Robustly find a layer in a Keras model, handling the 'clip' wrapper case.\n",
    "    \"\"\"\n",
    "    # 1. Try direct access\n",
    "    if hasattr(model, layer_name):\n",
    "        return getattr(model, layer_name)\n",
    "    \n",
    "    # 2. Try inside 'clip' wrapper\n",
    "    if hasattr(model, 'clip') and hasattr(model.clip, layer_name):\n",
    "        return getattr(model.clip, layer_name)\n",
    "        \n",
    "    # 3. Search layer list (Fallback)\n",
    "    for layer in model.layers:\n",
    "        if layer.name == layer_name:\n",
    "            return layer\n",
    "        if layer.name == 'clip':\n",
    "             if hasattr(layer, layer_name):\n",
    "                 return getattr(layer, layer_name)\n",
    "                 \n",
    "    raise AttributeError(f\"Could not find '{layer_name}' in TFCLIPModel. Available: {[l.name for l in model.layers]}\")\n",
    "\n",
    "# Helper to find sub-attributes like pre_layrnorm inside vision_model\n",
    "def get_sublayer_safe(model, possible_names):\n",
    "    for name in possible_names:\n",
    "        if hasattr(model, name):\n",
    "            return getattr(model, name)\n",
    "    # If not found as attribute, check layers list\n",
    "    for layer in model.layers:\n",
    "        if layer.name in possible_names:\n",
    "            return layer\n",
    "    raise AttributeError(f\"Could not find any of {possible_names} in model. Available: {[l.name for l in model.layers]}\")\n",
    "\n",
    "\n",
    "class M_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's M_Block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, cond_dim, k, s, p):\n",
    "        super(M_Block, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(mid_ch, k, strides=s, padding='same', kernel_initializer='he_uniform')\n",
    "        self.fuse1 = DFBLK(cond_dim, mid_ch)\n",
    "        self.conv2 = layers.Conv2D(out_ch, k, strides=s, padding='same', kernel_initializer='he_uniform')\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, h, c):\n",
    "        res = self.conv1(h)\n",
    "        res = self.fuse1(res, c)\n",
    "        res = self.conv2(res)\n",
    "        res = self.fuse2(res, c)\n",
    "        if self.learnable_sc:\n",
    "            sc = self.c_sc(h)\n",
    "        else:\n",
    "            sc = h\n",
    "        return sc + res\n",
    "\n",
    "\n",
    "class CLIP_Mapper(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's CLIP_Mapper.\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model):\n",
    "        super(CLIP_Mapper, self).__init__()\n",
    "        \n",
    "        self.vision_model = get_layer_safe(clip_model, 'vision_model')\n",
    "        \n",
    "        # FIX: Find sub-layers robustly\n",
    "        self.embeddings = get_sublayer_safe(self.vision_model, ['embeddings'])\n",
    "        self.pre_layrnorm = get_sublayer_safe(self.vision_model, ['pre_layrnorm', 'pre_layernorm', 'layernorm_pre'])\n",
    "        # encoder is standard, usually .encoder\n",
    "        self.encoder = get_sublayer_safe(self.vision_model, ['encoder'])\n",
    "        \n",
    "        # Freeze\n",
    "        self.vision_model.trainable = False\n",
    "        \n",
    "    def call(self, img_feats, prompts):\n",
    "        B = tf.shape(img_feats)[0]\n",
    "        H = tf.shape(img_feats)[1]\n",
    "        W = tf.shape(img_feats)[2]\n",
    "        \n",
    "        prompts = tf.cast(prompts, img_feats.dtype)\n",
    "        x = tf.reshape(img_feats, [B, H * W, 768])\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_token = self.embeddings.class_embedding\n",
    "        cls_token = tf.cast(cls_token, x.dtype)\n",
    "        cls_token = tf.reshape(cls_token, [1, 1, 768])\n",
    "        cls_token = tf.tile(cls_token, [B, 1, 1])\n",
    "        x = tf.concat([cls_token, x], axis=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        pos_embed_obj = self.embeddings.position_embedding\n",
    "        if hasattr(pos_embed_obj, 'weights'):\n",
    "             pos_embed = pos_embed_obj.weights[0]\n",
    "        else:\n",
    "             pos_embed = pos_embed_obj\n",
    "             \n",
    "        pos_embed = tf.cast(pos_embed, x.dtype)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = x + pos_embed[:seq_len, :]\n",
    "        \n",
    "        # Pre-LayerNorm (Using found layer)\n",
    "        x = self.pre_layrnorm(x)\n",
    "        \n",
    "        selected = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        prompt_idx = 0\n",
    "        \n",
    "        for i, layer in enumerate(self.encoder.layers):\n",
    "            if i in selected:\n",
    "                p = prompts[:, prompt_idx, :]\n",
    "                p = tf.expand_dims(p, 1)\n",
    "                x = tf.concat([x, p], axis=1)\n",
    "                # Explicit None args\n",
    "                layer_out = layer(x, attention_mask=None, output_attentions=False, training=False, causal_attention_mask=None)\n",
    "                x = layer_out[0]\n",
    "                x = x[:, :-1, :]\n",
    "                prompt_idx += 1\n",
    "            else:\n",
    "                layer_out = layer(x, attention_mask=None, output_attentions=False, training=False, causal_attention_mask=None)\n",
    "                x = layer_out[0]\n",
    "        \n",
    "        x = x[:, 1:, :]\n",
    "        x = tf.reshape(x, [B, H, W, 768])\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP_Adapter(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's CLIP_Adapter.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num, clip_model):\n",
    "        super(CLIP_Adapter, self).__init__()\n",
    "        self.CLIP_ch = CLIP_ch\n",
    "        self.f_blocks = []\n",
    "        self.f_blocks.append(M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        for _ in range(map_num - 1):\n",
    "            self.f_blocks.append(M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        self.conv_fuse = layers.Conv2D(CLIP_ch, 5, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        self.CLIP_ViT = CLIP_Mapper(clip_model)\n",
    "        self.conv_out = layers.Conv2D(G_ch, 5, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        self.fc_prompt = layers.Dense(CLIP_ch * 8, kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, out, c):\n",
    "        prompts = self.fc_prompt(c)\n",
    "        prompts = tf.reshape(prompts, [-1, 8, self.CLIP_ch])\n",
    "        for FBlock in self.f_blocks:\n",
    "            out = FBlock(out, c)\n",
    "        fuse_feat = self.conv_fuse(out)\n",
    "        map_feat = self.CLIP_ViT(fuse_feat, prompts)\n",
    "        return self.conv_out(fuse_feat + 0.1 * map_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. MODELS (Generator, Discriminator, Encoders)\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# --- Helpers for Robust Layer Lookup ---\n",
    "def get_layer_safe(model, layer_name):\n",
    "    \"\"\"Finds a layer in a Keras model, handling the 'clip' wrapper case.\"\"\"\n",
    "    if hasattr(model, layer_name): return getattr(model, layer_name)\n",
    "    if hasattr(model, 'clip') and hasattr(model.clip, layer_name): return getattr(model.clip, layer_name)\n",
    "    for layer in model.layers:\n",
    "        if layer.name == layer_name: return layer\n",
    "        if layer.name == 'clip' and hasattr(layer, layer_name): return getattr(layer, layer_name)\n",
    "    raise AttributeError(f\"Could not find '{layer_name}' in TFCLIPModel.\")\n",
    "\n",
    "def get_sublayer_safe(model, possible_names):\n",
    "    \"\"\"Finds a sub-layer (like pre_layrnorm) by checking multiple name variants.\"\"\"\n",
    "    for name in possible_names:\n",
    "        if hasattr(model, name): return getattr(model, name)\n",
    "    for layer in model.layers:\n",
    "        for name in possible_names:\n",
    "            if name in layer.name: return layer\n",
    "    raise AttributeError(f\"Could not find any of {possible_names} in model.\")\n",
    "\n",
    "# --- Encoders ---\n",
    "\n",
    "class CLIP_Text_Encoder(layers.Layer):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.text_model = get_layer_safe(clip_model, 'text_model')\n",
    "        self.text_projection = get_layer_safe(clip_model, 'text_projection')\n",
    "        self.text_model.trainable = False\n",
    "        self.text_projection.trainable = False\n",
    "        \n",
    "    def call(self, input_ids, attention_mask=None):\n",
    "        # Fix 1: Explicit None args for Autograph strictness\n",
    "        outputs = self.text_model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=None,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        word_emb = outputs.last_hidden_state\n",
    "        \n",
    "        # Standard GALIP/CLIP pooling logic\n",
    "        eot_indices = tf.argmax(tf.cast(input_ids, tf.int32), axis=-1)\n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        batch_indices = tf.range(batch_size, dtype=tf.int64)\n",
    "        gather_indices = tf.stack([batch_indices, tf.cast(eot_indices, tf.int64)], axis=1)\n",
    "        pooled_output = tf.gather_nd(word_emb, gather_indices)\n",
    "        \n",
    "        sent_emb = self.text_projection(pooled_output)\n",
    "        return sent_emb, word_emb\n",
    "    \n",
    "    @property\n",
    "    def trainable_weights(self): return []\n",
    "    @property  \n",
    "    def non_trainable_weights(self): return self.text_model.weights + self.text_projection.weights\n",
    "\n",
    "class CLIP_Image_Encoder(layers.Layer):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = get_layer_safe(clip_model, 'vision_model')\n",
    "        self.visual_projection = get_layer_safe(clip_model, 'visual_projection')\n",
    "        \n",
    "        # Fix 2: Robust lookup for internal layers (avoids AttributeError)\n",
    "        self.embeddings = get_sublayer_safe(self.vision_model, ['embeddings'])\n",
    "        self.pre_layrnorm = get_sublayer_safe(self.vision_model, ['pre_layrnorm', 'pre_layernorm', 'layernorm_pre'])\n",
    "        self.post_layernorm = get_sublayer_safe(self.vision_model, ['post_layernorm', 'post_layernorm', 'layernorm_post'])\n",
    "        self.encoder = get_sublayer_safe(self.vision_model, ['encoder'])\n",
    "        \n",
    "        self.vision_model.trainable = False\n",
    "        self.visual_projection.trainable = False\n",
    "        \n",
    "    def transf_to_CLIP_input(self, inputs):\n",
    "        x = (inputs + 1.0) * 0.5\n",
    "        x = tf.image.resize(x, [224, 224], method='bicubic')\n",
    "        mean = tf.constant([0.48145466, 0.4578275, 0.40821073], dtype=x.dtype)\n",
    "        std = tf.constant([0.26862954, 0.26130258, 0.27577711], dtype=x.dtype)\n",
    "        x = (x - mean) / std\n",
    "        return x\n",
    "\n",
    "    def call(self, img):\n",
    "        # 1. Get standard NHWC input: (B, 224, 224, 3)\n",
    "        x = self.transf_to_CLIP_input(img)\n",
    "        \n",
    "        # 2. Fix 3: Transpose to NCHW: (B, 3, 224, 224)\n",
    "        # HuggingFace TFCLIPVisionEmbeddings strictly expects NCHW input.\n",
    "        x = tf.transpose(x, [0, 3, 1, 2])\n",
    "        \n",
    "        x = self.embeddings(x)\n",
    "        x = self.pre_layrnorm(x)\n",
    "        \n",
    "        local_features = []\n",
    "        selected = [1, 4, 8]\n",
    "        \n",
    "        for i, layer in enumerate(self.encoder.layers):\n",
    "            # Fix 4: Explicit None args for Autograph strictness\n",
    "            layer_out = layer(\n",
    "                x, \n",
    "                attention_mask=None, \n",
    "                causal_attention_mask=None,\n",
    "                output_attentions=False, \n",
    "                training=False\n",
    "            )\n",
    "            x = layer_out[0]\n",
    "            \n",
    "            if i in selected:\n",
    "                # Remove CLS (index 0) and reshape\n",
    "                grid = x[:, 1:, :] \n",
    "                B = tf.shape(grid)[0]\n",
    "                grid = tf.reshape(grid, [B, 7, 7, 768])\n",
    "                local_features.append(grid)\n",
    "        \n",
    "        cls_token = self.post_layernorm(x[:, 0, :])\n",
    "        global_emb = self.visual_projection(cls_token)\n",
    "        local_features = tf.stack(local_features, axis=1)\n",
    "        \n",
    "        return local_features, global_emb\n",
    "\n",
    "# --- GAN Models ---\n",
    "\n",
    "class NetG(Model):\n",
    "    def __init__(self, ngf, nz, cond_dim, clip_model):\n",
    "        super(NetG, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.code_sz, self.code_ch, self.mid_ch = 7, 64, 32\n",
    "        self.CLIP_ch = 768\n",
    "        \n",
    "        self.fc_code = layers.Dense(self.code_sz * self.code_sz * self.code_ch, kernel_initializer='he_uniform')\n",
    "        \n",
    "        self.mapping = CLIP_Adapter(\n",
    "            in_ch=self.code_ch,\n",
    "            mid_ch=self.mid_ch,\n",
    "            out_ch=self.code_ch,\n",
    "            G_ch=ngf * 8,\n",
    "            CLIP_ch=self.CLIP_ch,\n",
    "            cond_dim=cond_dim + nz,\n",
    "            k=3, s=1, p=1,\n",
    "            map_num=4,\n",
    "            clip_model=clip_model\n",
    "        )\n",
    "        \n",
    "        self.g_blocks = []\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 8))\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 4))\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 4, ngf * 2))\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 2, ngf * 1))\n",
    "        self.target_sizes = [8, 16, 32, 64]\n",
    "        \n",
    "        self.to_rgb = tf.keras.Sequential([\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Conv2D(3, 3, padding='same', kernel_initializer='he_uniform'),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        noise, c = inputs\n",
    "        cond = tf.concat([noise, c], axis=1)\n",
    "        \n",
    "        out = self.fc_code(noise)\n",
    "        out = tf.reshape(out, [-1, self.code_sz, self.code_sz, self.code_ch])\n",
    "        \n",
    "        out = self.mapping(out, cond)\n",
    "        \n",
    "        for block, target_size in zip(self.g_blocks, self.target_sizes):\n",
    "            # Fix 5: Pass 'target_size' as a KEYWORD argument (Keras requirement)\n",
    "            out = block(out, cond, target_size=target_size)\n",
    "        \n",
    "        out = self.to_rgb(out)\n",
    "        out = tf.nn.tanh(out)\n",
    "        return out\n",
    "\n",
    "class NetD(Model):\n",
    "    def __init__(self, ndf):\n",
    "        super(NetD, self).__init__()\n",
    "        self.d_blocks = []\n",
    "        self.d_blocks.append(D_Block(768, 768, is_res=True, clip_feat=True))\n",
    "        self.d_blocks.append(D_Block(768, 768, is_res=True, clip_feat=True))\n",
    "        self.main = D_Block(768, 512, is_res=True, clip_feat=False)\n",
    "\n",
    "    def call(self, h):\n",
    "        out = h[:, 0]\n",
    "        for idx in range(len(self.d_blocks)):\n",
    "            out = self.d_blocks[idx](out, h[:, idx+1])\n",
    "        out = self.main(out)\n",
    "        return out\n",
    "\n",
    "class NetC(Model):\n",
    "    def __init__(self, ndf, cond_dim):\n",
    "        super(NetC, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.joint_conv = tf.keras.Sequential([\n",
    "            layers.Conv2D(ndf * 2, 4, strides=1, padding='valid', use_bias=False, kernel_initializer='he_uniform'),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Conv2D(1, 4, strides=1, padding='valid', use_bias=False, kernel_initializer='he_uniform')\n",
    "        ])\n",
    "\n",
    "    def call(self, out, cond):\n",
    "        B = tf.shape(out)[0]\n",
    "        cond = tf.reshape(cond, [B, 1, 1, self.cond_dim])\n",
    "        cond = tf.tile(cond, [1, 7, 7, 1])\n",
    "        h_c = tf.concat([out, cond], axis=3)\n",
    "        return self.joint_conv(h_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # Hinge loss for Discriminator\n",
    "    # Real: min(0, -1 + real) -> ReLU(1 - real)\n",
    "    # Fake: min(0, -1 - fake) -> ReLU(1 + fake)\n",
    "    real_loss = tf.reduce_mean(tf.nn.relu(1.0 - real_logits))\n",
    "    fake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake_logits))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_adversarial_loss(fake_logits):\n",
    "    # Hinge loss for Generator (maximize D's output)\n",
    "    return -tf.reduce_mean(fake_logits)\n",
    "  \n",
    "  \n",
    "def clip_matching_loss(image_emb, text_emb):\n",
    "    # image_emb: [B, 512] (from CLIP_Image_Encoder global_emb)\n",
    "    # text_emb: [B, 512] (from CLIP_Text_Encoder sent_emb)\n",
    "    \n",
    "    # 1. Normalize embeddings (Crucial for Cosine Similarity)\n",
    "    image_emb = tf.nn.l2_normalize(image_emb, axis=1)\n",
    "    text_emb = tf.nn.l2_normalize(text_emb, axis=1)\n",
    "    \n",
    "    # 2. Compute Cosine Distance (1 - Cosine Similarity)\n",
    "    # Reducing across the batch dimension\n",
    "    sim = tf.reduce_sum(image_emb * text_emb, axis=1)\n",
    "    loss = 1.0 - sim\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "  \n",
    "  \n",
    "def MA_GP(discriminator, net_c, CLIP_real, sent_emb, pred_real):\n",
    "    \"\"\"\n",
    "    Matching-Aware Gradient Penalty (MA-GP).\n",
    "    Calculates gradient of D(real, text) w.r.t. input features.\n",
    "    Target: Penalize gradients to enforce Lipschitz continuity.\n",
    "    \n",
    "    Args:\n",
    "        discriminator: NetD model\n",
    "        net_c: NetC model\n",
    "        CLIP_real: [B, 3, 7, 7, 768] Real image CLIP features\n",
    "        sent_emb: [B, 512] Text embeddings\n",
    "        pred_real: [B, 1, 1, 1] The validity score (output of NetC)\n",
    "        \n",
    "    Returns:\n",
    "        gradient_penalty: Scalar tensor\n",
    "    \"\"\"\n",
    "    # In TensorFlow, we need the tape to verify the gradients.\n",
    "    # We assume this function is called INSIDE the GradientTape where \n",
    "    # CLIP_real and sent_emb were watched.\n",
    "    \n",
    "    # 1. Get gradients of the prediction w.r.t inputs\n",
    "    # Note: We need gradients w.r.t BOTH visual features and text embeddings\n",
    "    grads = tf.gradients(pred_real, [CLIP_real, sent_emb])\n",
    "    \n",
    "    # 2. Flatten and Concatenate\n",
    "    grad_img = tf.reshape(grads[0], [tf.shape(grads[0])[0], -1]) # [B, N_img]\n",
    "    grad_txt = tf.reshape(grads[1], [tf.shape(grads[1])[0], -1]) # [B, N_txt]\n",
    "    grad = tf.concat([grad_img, grad_txt], axis=1) # [B, N_total]\n",
    "    \n",
    "    # 3. Calculate Norm\n",
    "    grad_l2norm = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "    \n",
    "    # 4. Apply Penalty (Power of 6 is specific to DF-GAN/GALIP)\n",
    "    # Weight is typically 2.0 in their repo\n",
    "    d_loss_gp = 2.0 * tf.reduce_mean(tf.pow(grad_l2norm, 6))\n",
    "    \n",
    "    return d_loss_gp\n",
    "\n",
    "def predict_loss(net_c, d_feats, sent_emb, negative=False):\n",
    "    \"\"\"\n",
    "    Computes Hinge Loss component.\n",
    "    Args:\n",
    "        negative: True for Fake/Mismatch (minimize -1 - D), False for Real (minimize -1 + D)\n",
    "    \"\"\"\n",
    "    # NetC output: [B, 1, 1, 1] -> Flatten to [B]\n",
    "    logits = net_c(d_feats, sent_emb, training=True)\n",
    "    logits = tf.reshape(logits, [-1])\n",
    "    \n",
    "    if negative:\n",
    "        # Fake or Mismatch: ReLU(1 + D(x))\n",
    "        loss = tf.reduce_mean(tf.nn.relu(1.0 + logits))\n",
    "    else:\n",
    "        # Real: ReLU(1 - D(x))\n",
    "        loss = tf.reduce_mean(tf.nn.relu(1.0 - logits))\n",
    "        \n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def DiffAugment(x, policy='translation'):\n",
    "    \"\"\"\n",
    "    TensorFlow implementation of DiffAugment.\n",
    "    Supports 'color', 'translation', 'cutout'.\n",
    "    \"\"\"\n",
    "    if policy:\n",
    "        if 'color' in policy:\n",
    "            x = rand_brightness(x)\n",
    "            x = rand_saturation(x)\n",
    "            x = rand_contrast(x)\n",
    "        if 'translation' in policy:\n",
    "            x = rand_translation(x)\n",
    "        if 'cutout' in policy:\n",
    "            x = rand_cutout(x)\n",
    "    return x\n",
    "\n",
    "# --- Augmentation Primitives ---\n",
    "def rand_brightness(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=-0.5, maxval=0.5)\n",
    "    x = x + magnitude\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_saturation(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.0, maxval=2.0)\n",
    "    x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_contrast(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.5, maxval=1.5)\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    shift = int(64 * ratio)\n",
    "    \n",
    "    # Pad the image with reflection\n",
    "    x_padded = tf.pad(x, [[0, 0], [shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "    \n",
    "    # Vectorized Random Crop using crop_and_resize\n",
    "    padded_size = tf.cast(img_size + 2*shift, tf.float32)\n",
    "    max_offset = 2 * shift\n",
    "    \n",
    "    offsets_y = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    offsets_x = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    \n",
    "    offsets_y = tf.cast(offsets_y, tf.float32)\n",
    "    offsets_x = tf.cast(offsets_x, tf.float32)\n",
    "    \n",
    "    # Normalize coordinates to [0, 1] for crop_and_resize\n",
    "    y1 = offsets_y / padded_size\n",
    "    x1 = offsets_x / padded_size\n",
    "    y2 = (offsets_y + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    x2 = (offsets_x + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    \n",
    "    boxes = tf.stack([y1, x1, y2, x2], axis=1) # [B, 4]\n",
    "    box_indices = tf.range(batch_size)\n",
    "    \n",
    "    x_translated = tf.image.crop_and_resize(\n",
    "        x_padded, \n",
    "        boxes, \n",
    "        box_indices, \n",
    "        crop_size=[img_size, img_size]\n",
    "    )\n",
    "    \n",
    "    return x_translated\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    cutout_size = int(64 * ratio // 2) * 2\n",
    "    \n",
    "    iy, ix = tf.meshgrid(tf.range(img_size), tf.range(img_size), indexing='ij')\n",
    "    iy = tf.expand_dims(iy, 0) \n",
    "    ix = tf.expand_dims(ix, 0)\n",
    "    \n",
    "    offset_x = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    offset_y = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    \n",
    "    mask_x = tf.math.logical_and(ix >= offset_x, ix < offset_x + cutout_size)\n",
    "    mask_y = tf.math.logical_and(iy >= offset_y, iy < offset_y + cutout_size)\n",
    "    mask_box = tf.math.logical_and(mask_x, mask_y)\n",
    "    \n",
    "    mask_keep = tf.cast(tf.math.logical_not(mask_box), x.dtype)\n",
    "    mask_keep = tf.expand_dims(mask_keep, -1) \n",
    "    \n",
    "    return x * mask_keep\n",
    "\n",
    "def save_sample_images(generator, text_encoder, fixed_input_ids, fixed_noise, epoch, save_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a grid of images using fixed noise/text for consistency.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Encode text\n",
    "    text_embeds, _ = text_encoder(fixed_input_ids, training=False)\n",
    "    \n",
    "    # Generate\n",
    "    fake_imgs = generator([fixed_noise, text_embeds], training=False)\n",
    "    \n",
    "    # Convert to [0, 1] for plotting\n",
    "    fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "    fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "    \n",
    "    # Plot Grid\n",
    "    n = int(np.sqrt(len(fake_imgs)))\n",
    "    if n * n != len(fake_imgs): n = 8 \n",
    "    \n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(min(8, len(fake_imgs))):\n",
    "        plt.subplot(1, 8, i+1)\n",
    "        plt.imshow(fake_imgs[i])\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:03d}.png'))\n",
    "    plt.close()\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_images, input_ids, attention_mask, \n",
    "               generator, discriminator, net_c, \n",
    "               text_encoder, image_encoder,\n",
    "               g_optimizer, d_optimizer, \n",
    "               batch_size, z_dim, sim_w=1.0):\n",
    "    \"\"\"\n",
    "    100% Faithful TensorFlow replication of PyTorch GALIP train step.\n",
    "    \n",
    "    Key differences from typical GAN:\n",
    "    1. NetD only returns feature maps, no unconditional score\n",
    "    2. NetC is the conditional discriminator\n",
    "    3. MA-GP is computed on (CLIP_real, sent_emb) with 6th power\n",
    "    4. G loss includes CLIP similarity loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Encode Text (frozen CLIP)\n",
    "    sent_emb, _ = text_encoder(input_ids, attention_mask=attention_mask)  # [B, 512]\n",
    "    \n",
    "    # 2. Generate Fake Images\n",
    "    noise = tf.random.normal([batch_size, z_dim])\n",
    "    fake_images = generator([noise, sent_emb], training=True)\n",
    "    \n",
    "    # ====================\n",
    "    # Train Discriminator\n",
    "    # ====================\n",
    "    with tf.GradientTape(persistent=True) as d_tape:\n",
    "        # Make inputs watchable for MA-GP\n",
    "        d_tape.watch(sent_emb)\n",
    "        \n",
    "        # Encode Real Images (CLIP)\n",
    "        CLIP_real, real_emb = image_encoder(real_images)  # [B, 3, 7, 7, 768], [B, 512]\n",
    "        d_tape.watch(CLIP_real)\n",
    "        \n",
    "        # D(Real) -> NetC(Real)\n",
    "        real_feats = discriminator(CLIP_real, training=True)  # [B, 7, 7, 512]\n",
    "        pred_real = net_c(real_feats, sent_emb, training=True)  # [B, 1, 1, 1]\n",
    "        errD_real = tf.reduce_mean(tf.nn.relu(1.0 - pred_real))\n",
    "        \n",
    "        # D(Mismatch) - Wrong Text (roll by 1)\n",
    "        mis_sent_emb = tf.roll(sent_emb, shift=1, axis=0)\n",
    "        pred_mis = net_c(real_feats, mis_sent_emb, training=True)\n",
    "        errD_mis = tf.reduce_mean(tf.nn.relu(1.0 + pred_mis))\n",
    "        \n",
    "        # Encode Fake Images (CLIP) - detached from G\n",
    "        fake_images_stopped = tf.stop_gradient(fake_images)\n",
    "        CLIP_fake, fake_emb = image_encoder(fake_images_stopped)\n",
    "        \n",
    "        # D(Fake) -> NetC(Fake)\n",
    "        fake_feats = discriminator(CLIP_fake, training=True)\n",
    "        pred_fake = net_c(fake_feats, sent_emb, training=True)\n",
    "        errD_fake = tf.reduce_mean(tf.nn.relu(1.0 + pred_fake))\n",
    "    \n",
    "    # MA-GP: Matching-Aware Gradient Penalty\n",
    "    # Computed on (CLIP_real, sent_emb) with 6th power - EXACTLY as PyTorch\n",
    "    grads = d_tape.gradient(pred_real, [CLIP_real, sent_emb])\n",
    "    grad0 = tf.reshape(grads[0], [batch_size, -1])  # Flatten CLIP features\n",
    "    grad1 = tf.reshape(grads[1], [batch_size, -1])  # Flatten text embedding\n",
    "    grad = tf.concat([grad0, grad1], axis=1)\n",
    "    grad_l2norm = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1) + 1e-8)\n",
    "    errD_MAGP = 2.0 * tf.reduce_mean(tf.pow(grad_l2norm, 6))\n",
    "    \n",
    "    # Total D Loss (PyTorch: errD_real + (errD_fake + errD_mis)/2.0 + errD_MAGP)\n",
    "    d_loss = errD_real + (errD_fake + errD_mis) / 2.0 + errD_MAGP\n",
    "    \n",
    "    del d_tape\n",
    "    \n",
    "    # Update D\n",
    "    d_vars = discriminator.trainable_variables + net_c.trainable_variables\n",
    "    d_grads = tf.gradients(d_loss, d_vars)\n",
    "    d_optimizer.apply_gradients(zip(d_grads, d_vars))\n",
    "    \n",
    "    # ====================\n",
    "    # Train Generator\n",
    "    # ====================\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        # Generate fresh fake images\n",
    "        fake_images = generator([noise, sent_emb], training=True)\n",
    "        \n",
    "        # Encode Fake Images (CLIP)\n",
    "        CLIP_fake, fake_emb = image_encoder(fake_images)\n",
    "        \n",
    "        # D(Fake) -> NetC(Fake)\n",
    "        fake_feats = discriminator(CLIP_fake, training=True)\n",
    "        output = net_c(fake_feats, sent_emb, training=True)\n",
    "        \n",
    "        # CLIP Similarity Loss: cosine similarity between fake_emb and sent_emb\n",
    "        fake_emb_norm = tf.nn.l2_normalize(fake_emb, axis=1)\n",
    "        sent_emb_norm = tf.nn.l2_normalize(sent_emb, axis=1)\n",
    "        text_img_sim = tf.reduce_mean(tf.reduce_sum(fake_emb_norm * sent_emb_norm, axis=1))\n",
    "        \n",
    "        # G Loss (PyTorch: -output.mean() - sim_w*text_img_sim)\n",
    "        g_loss = -tf.reduce_mean(output) - sim_w * text_img_sim\n",
    "    \n",
    "    # Update G\n",
    "    g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    \n",
    "    return {\n",
    "        'd_loss': d_loss,\n",
    "        'g_loss': g_loss,\n",
    "        'errD_real': errD_real,\n",
    "        'errD_fake': errD_fake,\n",
    "        'errD_mis': errD_mis,\n",
    "        'errD_MAGP': errD_MAGP,\n",
    "        'text_img_sim': text_img_sim,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from transformers import TFCLIPModel\n",
    "def train(dataset, args):\n",
    "    \"\"\"\n",
    "    100% Faithful TensorFlow replication of PyTorch GALIP training loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. Initialization & Logging Setup\n",
    "    # ==========================================================================\n",
    "    print(f\"--- Initializing Models (Image Size: {args['IMAGE_SIZE']}) ---\")\n",
    "    \n",
    "    # Load CLIP Models (full model needed for both encoders)\n",
    "    print(\"--- Loading CLIP Models ---\")\n",
    "    try:\n",
    "        clip_model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        print(\"‚úì CLIP Model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error loading CLIP model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize Encoders (pass full CLIP model)\n",
    "    text_encoder = CLIP_Text_Encoder(clip_model)\n",
    "    image_encoder = CLIP_Image_Encoder(clip_model)\n",
    "    \n",
    "    # Initialize GAN Models\n",
    "    generator = NetG(ngf=args['NGF'], nz=args['Z_DIM'], cond_dim=args['EMBED_DIM'], clip_model=clip_model)\n",
    "    discriminator = NetD(ndf=args['NDF'])\n",
    "    net_c = NetC(ndf=args['NDF'], cond_dim=args['EMBED_DIM'])\n",
    "    \n",
    "    # Optimizers (PyTorch: Adam with betas=(0.0, 0.9))\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_G'], beta_1=0.0, beta_2=0.9, epsilon=ADAM_EPSILON)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_D'], beta_1=0.0, beta_2=0.9, epsilon=ADAM_EPSILON)\n",
    "\n",
    "    # Checkpoints\n",
    "    checkpoint_dir = os.path.join(args['RUN_DIR'], 'checkpoints')\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        generator=generator, discriminator=discriminator, net_c=net_c,\n",
    "        g_optimizer=g_optimizer, d_optimizer=d_optimizer\n",
    "    )\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    # TensorBoard Setup\n",
    "    log_dir = os.path.join(args['RUN_DIR'], 'logs')\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    try:\n",
    "        tensorboard_process = subprocess.Popen(\n",
    "            [sys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir]\n",
    "        )\n",
    "        print(f\"‚úì TensorBoard launched (PID: {tensorboard_process.pid})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not launch TensorBoard: {e}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. Training Loop Setup\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # Fixed noise/text for visualization\n",
    "    fixed_noise = tf.random.normal([8, args['Z_DIM']])\n",
    "    \n",
    "    # Get fixed text from first batch\n",
    "    for _, fixed_input_ids, fixed_mask in dataset.take(1):\n",
    "        fixed_input_ids = fixed_input_ids[:8]\n",
    "        fixed_mask = fixed_mask[:8]\n",
    "        break\n",
    "\n",
    "    start_epoch = 0\n",
    "    if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint)\n",
    "        print(f\"Restored from {manager.latest_checkpoint}\")\n",
    "        \n",
    "    print(f\"Starting training for {args['MAX_EPOCH']} epochs...\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 3. Training Loop\n",
    "    # ==========================================================================\n",
    "    for epoch in range(start_epoch, args['MAX_EPOCH']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(dataset, desc=f\"Epoch {epoch+1}/{args['MAX_EPOCH']}\")\n",
    "        \n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        \n",
    "        for step, (real_images, input_ids, attention_mask) in enumerate(pbar):\n",
    "            \n",
    "            losses = train_step(\n",
    "                real_images, \n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                generator, \n",
    "                discriminator, \n",
    "                net_c, \n",
    "                text_encoder, \n",
    "                image_encoder,\n",
    "                g_optimizer, \n",
    "                d_optimizer, \n",
    "                args['BATCH_SIZE'], \n",
    "                args['Z_DIM'],\n",
    "                sim_w=args.get('SIM_W', 1.0)\n",
    "            )\n",
    "            \n",
    "            d_losses.append(float(losses['d_loss']))\n",
    "            g_losses.append(float(losses['g_loss']))\n",
    "            \n",
    "            # Update pbar\n",
    "            pbar.set_postfix({\n",
    "                'D': f\"{losses['d_loss']:.4f}\", \n",
    "                'G': f\"{losses['g_loss']:.4f}\",\n",
    "                'MAGP': f\"{losses['errD_MAGP']:.4f}\",\n",
    "                'CLIP': f\"{losses['text_img_sim']:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            with summary_writer.as_default():\n",
    "                step_global = epoch * len(dataset) + step\n",
    "                tf.summary.scalar('Loss/D_total', losses['d_loss'], step=step_global)\n",
    "                tf.summary.scalar('Loss/G_total', losses['g_loss'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_real', losses['errD_real'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_fake', losses['errD_fake'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_mis', losses['errD_mis'], step=step_global)\n",
    "                tf.summary.scalar('Loss/MA_GP', losses['errD_MAGP'], step=step_global)\n",
    "                tf.summary.scalar('Loss/CLIP_sim', losses['text_img_sim'], step=step_global)\n",
    "\n",
    "        # End of Epoch\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"Epoch {epoch+1} done. D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}, Time: {time.time()-start_time:.1f}s\")\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        if (epoch + 1) % args['SAVE_FREQ'] == 0:\n",
    "            save_path = manager.save()\n",
    "            print(f\"Saved checkpoint for epoch {epoch+1}: {save_path}\")\n",
    "            \n",
    "        # Save Sample Images\n",
    "        if (epoch + 1) % args['SAMPLE_FREQ'] == 0:\n",
    "            save_sample_images(generator, text_encoder, fixed_input_ids, fixed_noise, epoch+1, os.path.join(args['RUN_DIR'], 'samples'))\n",
    "\n",
    "    print(\"Training Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Run Directory: ./runs/20251129-154401\n",
      "Config: {\n",
      "  \"IMAGE_SIZE\": [\n",
      "    64,\n",
      "    64,\n",
      "    3\n",
      "  ],\n",
      "  \"NGF\": 64,\n",
      "  \"NDF\": 64,\n",
      "  \"Z_DIM\": 100,\n",
      "  \"EMBED_DIM\": 512,\n",
      "  \"LR_G\": 0.0001,\n",
      "  \"LR_D\": 0.0004,\n",
      "  \"SIM_W\": 4.0,\n",
      "  \"BATCH_SIZE\": 128,\n",
      "  \"MAX_EPOCH\": 600,\n",
      "  \"RUN_DIR\": \"./runs/20251129-154401\",\n",
      "  \"SAVE_FREQ\": 25,\n",
      "  \"SAMPLE_FREQ\": 1,\n",
      "  \"N_SAMPLE\": 7370\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Define configuration for training\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create a unique run directory\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"./runs/{run_id}\"\n",
    "if not os.path.exists(run_dir):\n",
    "    os.makedirs(run_dir)\n",
    "\n",
    "# User provided config - 100% faithful to PyTorch GALIP\n",
    "config = {\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    'NGF': 64,                # nf in PyTorch\n",
    "    'NDF': 64,                # nf in PyTorch  \n",
    "    'Z_DIM': 100,             # z_dim in PyTorch\n",
    "    'EMBED_DIM': 512,         # cond_dim in PyTorch (CLIP embedding dimension)\n",
    "    'LR_G': 0.0001,           # lr_g in PyTorch\n",
    "    'LR_D': 0.0004,           # lr_d in PyTorch\n",
    "    'SIM_W': 4.0,             # sim_w in PyTorch (CLIP similarity loss weight) - GALIP default is 4.0\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'MAX_EPOCH': 600,\n",
    "    'RUN_DIR': run_dir,\n",
    "    'SAVE_FREQ': 25,\n",
    "    'SAMPLE_FREQ': 1,\n",
    "    'N_SAMPLE': num_training_sample if 'num_training_sample' in locals() else 7370\n",
    "}\n",
    "\n",
    "# Save config for reproducibility\n",
    "with open(os.path.join(run_dir, 'config.json'), 'w') as f:\n",
    "    # Filter for JSON serializable values\n",
    "    json_config = {k: v for k, v in config.items() if isinstance(v, (int, float, str, list, bool))}\n",
    "    json.dump(json_config, f, indent=4)\n",
    "\n",
    "print(f\"Training Run Directory: {run_dir}\")\n",
    "print(f\"Config: {json.dumps(json_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Models (Image Size: [64, 64, 3]) ---\n",
      "--- Loading CLIP Models ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CLIP Model loaded\n",
      "‚úì TensorBoard launched (PID: 94328)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lee_eason/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 600 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   0%|          | 0/550 [00:00<?, ?it/s]Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.20.0 at http://localhost:6016/ (Press CTRL+C to quit)\n",
      "/Users/lee_eason/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'd__block_24', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/lee_eason/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'd__block_25', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 'dataset' is the tf.data.Dataset object you created in the notebook\n",
    "train(dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¬∂</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¬∂</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string\n",
    "\t\t\t\tindex: Test sample ID\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tinput_ids, attention_mask, index\n",
    "\t\t\"\"\"\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing dataset generator - decodes IDs to raw text\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_pickle('./dataset/testData.pkl')\n",
    "\t\tcaptions_ids = data['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\tchosen_caption_ids = captions_ids[i]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\tindex = data['ID'].values\n",
    "\t\tindex = np.asarray(index)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.repeat().batch(batch_size)\n",
    "\t\t\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference directory inside the run directory\n",
    "inference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "if not os.path.exists(inference_dir):\n",
    "    os.makedirs(inference_dir)\n",
    "print(f\"Inference Directory: {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, config):\n",
    "    print(\"--- Starting Inference (Corrected for GALIP) ---\")\n",
    "    \n",
    "    # 1. Re-initialize CLIP (Required for Tokenizer and Encoder)\n",
    "    print(\"Loading CLIP components...\")\n",
    "    try:\n",
    "        from transformers import TFCLIPModel, CLIPTokenizer\n",
    "        clip_model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Re-create the specific Encoder wrapper used in training\n",
    "        text_encoder = CLIP_Text_Encoder(clip_model)\n",
    "        print(\"‚úì CLIP Model & Encoder loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error loading CLIP: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Load Generator\n",
    "    print(\"Loading Generator...\")\n",
    "    # Initialize with same args as training\n",
    "    generator = NetG(ngf=config['NGF'], nz=config['Z_DIM'], cond_dim=config['EMBED_DIM'], clip_model=clip_model)\n",
    "    \n",
    "    # Run a dummy forward pass to initialize variables before loading weights\n",
    "    dummy_noise = tf.random.normal([1, config['Z_DIM']])\n",
    "    dummy_text = tf.random.normal([1, config['EMBED_DIM']])\n",
    "    _ = generator([dummy_noise, dummy_text], training=False)\n",
    "    \n",
    "    # Restore Checkpoint\n",
    "    checkpoint_dir = os.path.join(config['RUN_DIR'], 'checkpoints')\n",
    "    # Use expect_partial() because we are strictly loading the generator, \n",
    "    # ignoring optimizer states or discriminator if they exist in the ckpt\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    if latest_ckpt:\n",
    "        print(f\"Loading weights from: {latest_ckpt}\")\n",
    "        status = checkpoint.restore(latest_ckpt).expect_partial()\n",
    "        status.assert_existing_objects_matched()\n",
    "        print(\"‚úì Generator weights loaded successfully\")\n",
    "    else:\n",
    "        print(\"‚ö† NO CHECKPOINT FOUND! Generating with random weights.\")\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    inference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "    if not os.path.exists(inference_dir): os.makedirs(inference_dir)\n",
    "        \n",
    "    total_images = 0\n",
    "    \n",
    "    # Iterate over the testing dataset\n",
    "    # Note: testing_dataset yields (caption_texts, ids)\n",
    "    for step, (caption_texts, image_ids) in enumerate(tqdm(dataset, desc='Generating')):\n",
    "        \n",
    "        batch_size_curr = len(caption_texts)\n",
    "        \n",
    "        # --- A. Tokenize using CLIP (Not dictionary!) ---\n",
    "        # Convert tensors to string list\n",
    "        text_list = [t.numpy().decode('utf-8') for t in caption_texts]\n",
    "        \n",
    "        enc = tokenizer(\n",
    "            text_list,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=77, # CLIP default\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids = enc['input_ids']\n",
    "        attention_mask = enc['attention_mask']\n",
    "        \n",
    "        # --- B. Encode Text using CLIP (Not RNN!) ---\n",
    "        sent_emb, _ = text_encoder(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # --- C. Generate ---\n",
    "        noise = tf.random.normal([batch_size_curr, config['Z_DIM']])\n",
    "        fake_imgs = generator([noise, sent_emb], training=False)\n",
    "        \n",
    "        # Post-process\n",
    "        fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "        fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "        \n",
    "        # Save\n",
    "        for i in range(batch_size_curr):\n",
    "            try:\n",
    "                # Handle ID decoding safely\n",
    "                img_id_val = image_ids[i].numpy()\n",
    "                if isinstance(img_id_val, bytes):\n",
    "                    img_id = img_id_val.decode('utf-8')\n",
    "                else:\n",
    "                    img_id = str(img_id_val)\n",
    "                    \n",
    "                save_path = os.path.join(inference_dir, f'inference_{img_id}.jpg')\n",
    "                plt.imsave(save_path, fake_imgs[i])\n",
    "                total_images += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image {i}: {e}\")\n",
    "                \n",
    "    print(f\"Inference Complete. Saved {total_images} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "\t\tprint(f'‚ö† No images found in {inference_dir}')\n",
    "\t\tprint('Please run the inference cell first!')\n",
    "else:\n",
    "\t\t# Randomly sample 20 images\n",
    "\t\tnp.random.seed(42)  # For reproducibility\n",
    "\t\tnum_samples = min(20, len(image_files))\n",
    "\t\tsample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "\t\tsample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "\t\t# Create 4x5 grid\n",
    "\t\tfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\t\taxes = axes.flatten()\n",
    "\n",
    "\t\tfor idx, img_path in enumerate(sample_files):\n",
    "\t\t\t\t# Extract image ID from filename\n",
    "\t\t\t\timg_id = int(Path(img_path).stem.split('_')[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Find caption\n",
    "\t\t\t\tcaption_idx = np.where(test_ids == img_id)[0][0]\n",
    "\t\t\t\tcaption_ids = test_captions[caption_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode caption\n",
    "\t\t\t\tcaption_text = ''\n",
    "\t\t\t\tfor word_id in caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':\n",
    "\t\t\t\t\t\t\t\tcaption_text += word + ' '\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Load and display image\n",
    "\t\t\t\timg = plt.imread(img_path)\n",
    "\t\t\t\taxes[idx].imshow(img)\n",
    "\t\t\t\taxes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\t# Hide unused subplots if less than 20 images\n",
    "\t\tfor idx in range(num_samples, 20):\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\tprint(f'\\nTotal generated images: {len(image_files)}')\n",
    "\t\tprint(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_comp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
