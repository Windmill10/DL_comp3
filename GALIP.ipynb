{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttry:\n",
    "\t\t# Restrict TensorFlow to only use the first GPU\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\t\t# Currently, memory growth needs to be the same across GPUs\n",
    "\t\tfor gpu in gpus:\n",
    "\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\n",
    "\t\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\t\tprint(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\texcept RuntimeError as e:\n",
    "\t\t# Memory growth must be set before GPUs have been initialized\n",
    "\t\tprint(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def pytorch_kaiming_uniform(seed=None):\n",
    "    \"\"\"Matches PyTorch's default Conv2d/Linear initialization\"\"\"\n",
    "    return tf.keras.initializers.VarianceScaling(\n",
    "        scale=1.0 / 3.0,  # This gives sqrt(1/fan_in) bound\n",
    "        mode='fan_in',\n",
    "        distribution='uniform',\n",
    "        seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¬∂</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úì Using CLIP tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¬∂</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. DATASET GENERATOR (Adapted for CLIP)\n",
    "# ==============================================================================\n",
    "\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAX_SEQ_LENGTH = 77 # CLIP default\n",
    "\n",
    "# Initialize CLIP Tokenizer\n",
    "try:\n",
    "\tfrom transformers import CLIPTokenizer\n",
    "\t# Use the same model name as the vision/text models we will load later\n",
    "\ttokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\tprint(\"‚úì CLIP Tokenizer loaded\")\n",
    "except Exception as e:\n",
    "\tprint(f\"‚ö† Error loading CLIP Tokenizer: {e}\")\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "\t\"\"\"\n",
    "\tData generator using CLIP Tokenizer\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tcaption_text: Raw text string\n",
    "\t\timage_path: Path to image file\n",
    "\t\n",
    "\tReturns:\n",
    "\t\timg, input_ids, attention_mask\n",
    "\t\"\"\"\n",
    "\t# ============= IMAGE PROCESSING =============\n",
    "\timg = tf.io.read_file(image_path)\n",
    "\timg = tf.image.decode_image(img, channels=3)\n",
    "\timg = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "\timg.set_shape([None, None, 3])\n",
    "\timg = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "\t\n",
    "\t# Normalize to [-1, 1] to match generator's tanh output\n",
    "\timg = (img * 2.0) - 1.0\n",
    "\timg.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "\t\n",
    "\t# ============= TEXT PROCESSING =============\n",
    "\t# Tokenize using CLIP\n",
    "\t# We use py_function because tokenizer is Python code\n",
    "\tdef tokenize(text):\n",
    "\t\ttext_str = text.numpy().decode('utf-8')\n",
    "\t\t# CLIP Tokenizer handles padding and truncation\n",
    "\t\tenc = tokenizer(\n",
    "\t\t\ttext_str, \n",
    "\t\t\tpadding='max_length', \n",
    "\t\t\ttruncation=True, \n",
    "\t\t\tmax_length=MAX_SEQ_LENGTH, \n",
    "\t\t\treturn_tensors='np'\n",
    "\t\t)\n",
    "\t\treturn enc['input_ids'][0], enc['attention_mask'][0]\n",
    "\t\t\n",
    "\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\tfunc=tokenize, \n",
    "\t\tinp=[caption_text], \n",
    "\t\tTout=[tf.int32, tf.int32]\n",
    "\t)\n",
    "\t\n",
    "\tinput_ids.set_shape([MAX_SEQ_LENGTH])\n",
    "\tattention_mask.set_shape([MAX_SEQ_LENGTH])\n",
    "\t\n",
    "\treturn img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator, word2Id_dict, id2word_dict, expand_captions=True):\n",
    "\t\"\"\"\n",
    "\tDataset generator that decodes IDs to text for CLIP\n",
    "\t\"\"\"\n",
    "\tdf = pd.read_pickle(filenames)\n",
    "\tcaptions_ids = df['Captions'].values\n",
    "\timage_paths = df['ImagePath'].values\n",
    "\t\n",
    "\tprint(f\"Loading dataset from {filenames}...\")\n",
    "\t\n",
    "\t# Helper to decode IDs to text\n",
    "\tdef decode_ids(id_list):\n",
    "\t\twords = []\n",
    "\t\tfor i in id_list:\n",
    "\t\t\tword = id2word_dict.get(str(i), '')\n",
    "\t\t\tif word and word != '<PAD>':\n",
    "\t\t\t\twords.append(word)\n",
    "\t\treturn ' '.join(words)\n",
    "\n",
    "\tall_captions_text = []\n",
    "\tall_paths = []\n",
    "\n",
    "\tif expand_captions:\n",
    "\t\t# Expand: Create a sample for every caption\n",
    "\t\tprint(\"Expanding captions (one sample per caption)...\")\n",
    "\t\tfor caps, path in zip(captions_ids, image_paths):\n",
    "\t\t\tfor cap_ids in caps:\n",
    "\t\t\t\ttext = decode_ids(cap_ids)\n",
    "\t\t\t\tall_captions_text.append(text)\n",
    "\t\t\t\tall_paths.append(path)\n",
    "\telse:\n",
    "\t\t# Random Select: Pick one random caption per image (static for this generator call)\n",
    "\t\t# Note: Ideally we'd do random selection at runtime, but decoding text in graph is hard.\n",
    "\t\t# For simplicity/performance, we pick one now. \n",
    "\t\t# To get true randomness per epoch, we'd need to re-create the dataset or use py_function logic.\n",
    "\t\tprint(\"Selecting one random caption per image...\")\n",
    "\t\tfor caps, path in zip(captions_ids, image_paths):\n",
    "\t\t\tcap_ids = random.choice(caps)\n",
    "\t\t\ttext = decode_ids(cap_ids)\n",
    "\t\t\tall_captions_text.append(text)\n",
    "\t\t\tall_paths.append(path)\n",
    "\t\t\t\n",
    "\tall_captions_text = np.array(all_captions_text)\n",
    "\tall_paths = np.array(all_paths)\n",
    "\t\n",
    "\tprint(f\"Dataset size: {len(all_captions_text)} samples\")\n",
    "\t\n",
    "\tdataset = tf.data.Dataset.from_tensor_slices((all_captions_text, all_paths))\n",
    "\tdataset = dataset.shuffle(len(all_captions_text))\n",
    "\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\tdataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\tdataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\treturn dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "# We use expand_captions=False to keep epoch size manageable (same as number of images)\n",
    "# or True for more training data. Let's use False for faster epochs initially, or True for better quality.\n",
    "# Given the small dataset (7k images), expanding is probably better (70k samples).\n",
    "dataset = dataset_generator(\n",
    "\tdata_path + '/text2ImgData.pkl', \n",
    "\tBATCH_SIZE, \n",
    "\ttraining_data_generator,\n",
    "\tword2Id_dict,\n",
    "\tid2word_dict,\n",
    "\texpand_captions=True \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¬∂</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¬∂</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "from transformers import TFCLIPVisionModel, TFCLIPTextModel, CLIPProcessor, CLIPConfig\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "try:\n",
    "\timport transformers\n",
    "\tprint(\"Transformers Version:\", transformers.__version__)\n",
    "except ImportError:\n",
    "\tprint(\"Transformers not installed. Please install it.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PYTORCH-TENSORFLOW COMPATIBILITY CONSTANTS\n",
    "# ==============================================================================\n",
    "# These constants ensure numerical equivalence between PyTorch and TensorFlow\n",
    "# implementations of GALIP.\n",
    "\n",
    "# 1. Optimizer epsilon: PyTorch Adam default is 1e-8, TensorFlow default is 1e-7\n",
    "#    Using 1e-7 can cause subtle numerical divergence over training.\n",
    "ADAM_EPSILON = 1e-8  # Match PyTorch default\n",
    "\n",
    "# 2. LayerNorm epsilon: PyTorch default is 1e-5, TensorFlow default is 1e-3\n",
    "#    This affects CLIP and any custom LayerNorm layers.\n",
    "LAYER_NORM_EPSILON = 1e-5  # Match PyTorch default\n",
    "\n",
    "# 3. Weight initialization: PyTorch Linear/Conv2d use Kaiming Uniform (He)\n",
    "#    TensorFlow defaults to Glorot Uniform (Xavier).\n",
    "#    All our layers now use kernel_initializer=pytorch_kaiming_uniform() explicitly.\n",
    "\n",
    "print(f\"PyTorch-compatible settings:\")\n",
    "print(f\"  ADAM_EPSILON = {ADAM_EPSILON}\")\n",
    "print(f\"  LAYER_NORM_EPSILON = {LAYER_NORM_EPSILON}\")\n",
    "print(f\"  Weight init: he_uniform (Kaiming Uniform)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# HELPER FUNCTION: Create PyTorch-compatible Adam optimizer\n",
    "# ==============================================================================\n",
    "def create_pytorch_compatible_adam(learning_rate, beta_1=0.0, beta_2=0.9):\n",
    "\t\"\"\"\n",
    "\tCreates an Adam optimizer with PyTorch-equivalent settings.\n",
    "\t\n",
    "\tPyTorch defaults:\n",
    "\t\t- lr: required\n",
    "\t\t- betas: (0.9, 0.999) but GALIP uses (0.0, 0.9)\n",
    "\t\t- eps: 1e-8\n",
    "\t\t- weight_decay: 0\n",
    "\t\t- amsgrad: False\n",
    "\t\n",
    "\tTensorFlow defaults that differ:\n",
    "\t\t- epsilon: 1e-7 (10x larger than PyTorch!)\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tlearning_rate: Learning rate\n",
    "\t\tbeta_1: First moment decay (default 0.0 for GAN training)\n",
    "\t\tbeta_2: Second moment decay (default 0.9 for GAN training)\n",
    "\t\n",
    "\tReturns:\n",
    "\t\ttf.keras.optimizers.Adam with PyTorch-equivalent settings\n",
    "\t\"\"\"\n",
    "\treturn tf.keras.optimizers.Adam(\n",
    "\t\tlearning_rate=learning_rate,\n",
    "\t\tbeta_1=beta_1,\n",
    "\t\tbeta_2=beta_2,\n",
    "\t\tepsilon=ADAM_EPSILON  # CRITICAL: Match PyTorch 1e-8\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFCLIPModel, CLIPConfig\n",
    "\n",
    "def robust_verify_clip(model_name=\"openai/clip-vit-base-patch32\"):\n",
    "\tprint(f\"--- üîç Deep Inspection of {model_name} ---\")\n",
    "\t\n",
    "\t# 1. Load Model\n",
    "\ttry:\n",
    "\t\tmodel = TFCLIPModel.from_pretrained(model_name)\n",
    "\t\tprint(\"‚úì Model loaded successfully.\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"‚ùå CRITICAL ERROR loading model: {e}\")\n",
    "\t\treturn\n",
    "\n",
    "\t# Helper to find layers recursively (Handles the 'clip' wrapper)\n",
    "\tdef find_layer_recursive(model, layer_name):\n",
    "\t\t# 1. Check top-level attributes\n",
    "\t\tif hasattr(model, layer_name):\n",
    "\t\t\treturn getattr(model, layer_name), \"Top-level Attribute\"\n",
    "\t\t\t\n",
    "\t\t# 2. Check direct children layers\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\tif layer.name == layer_name:\n",
    "\t\t\t\treturn layer, \"Direct Layer List\"\n",
    "\t\t\t\t\n",
    "\t\t# 3. CRITICAL: Check inside 'clip' wrapper if it exists\n",
    "\t\t# This fixes the specific error you are seeing\n",
    "\t\tif hasattr(model, 'clip'):\n",
    "\t\t\tclip_layer = model.clip\n",
    "\t\t\tif hasattr(clip_layer, layer_name):\n",
    "\t\t\t\treturn getattr(clip_layer, layer_name), \"Inside 'clip' Wrapper\"\n",
    "\t\t\n",
    "\t\t# 4. Check inside any layer named 'clip' in the layers list\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\tif layer.name == 'clip':\n",
    "\t\t\t\tif hasattr(layer, layer_name):\n",
    "\t\t\t\t\treturn getattr(layer, layer_name), \"Inside 'clip' Layer (List)\"\n",
    "\t\t\t\t\t\n",
    "\t\treturn None, \"NOT FOUND\"\n",
    "\n",
    "\t# 2. Inspect Projections (Visual & Text)\n",
    "\t# ---------------------------------------------------------\n",
    "\tfor proj_name in [\"visual_projection\", \"text_projection\"]:\n",
    "\t\tprint(f\"\\n[Checking {proj_name}]\")\n",
    "\t\tlayer, source = find_layer_recursive(model, proj_name)\n",
    "\t\t\n",
    "\t\tif layer is None:\n",
    "\t\t\tprint(f\"  ‚ùå FAIL: Layer '{proj_name}' DOES NOT EXIST.\")\n",
    "\t\t\tprint(f\"     Top-level layers: {[l.name for l in model.layers]}\")\n",
    "\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\tprint(f\"  ‚úì Found via: {source}\")\n",
    "\t\tprint(f\"  ‚úì Layer Type: {type(layer).__name__}\")\n",
    "\t\t\n",
    "\t\t# Check Bias\n",
    "\t\tweights = layer.weights\n",
    "\t\tprint(f\"  ‚Ä¢ Weight tensors found: {len(weights)}\")\n",
    "\t\t\n",
    "\t\tif len(weights) == 1:\n",
    "\t\t\tprint(\"  ‚úì PASS: Only kernel found. Bias=False. (100% PyTorch Faithful)\")\n",
    "\t\telif len(weights) == 2:\n",
    "\t\t\tbias_tensor = weights[1]\n",
    "\t\t\tbias_sum = tf.reduce_sum(tf.abs(bias_tensor)).numpy()\n",
    "\t\t\tprint(f\"  ‚ö† WARNING: Bias vector exists!\")\n",
    "\t\t\tprint(f\"    Sum of absolute values: {bias_sum}\")\n",
    "\t\t\t\n",
    "\t\t\tif bias_sum < 1e-9:\n",
    "\t\t\t\tprint(\"    ‚úì PASS (Soft): Bias exists but is effectively ZERO.\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"    ‚ùå FAIL: Non-zero bias found! Divergence risk.\")\n",
    "\n",
    "\t# 3. LayerNorm Epsilon Check\n",
    "\t# ---------------------------------------------------------\n",
    "\tprint(\"\\n[LayerNorm Epsilon Analysis]\")\n",
    "\t\n",
    "\t# We use the same finder for the sub-models\n",
    "\tvision_model, _ = find_layer_recursive(model, \"vision_model\")\n",
    "\ttext_model, _ = find_layer_recursive(model, \"text_model\")\n",
    "\t\n",
    "\t# Check Vision Pre-LayerNorm\n",
    "\tif vision_model and hasattr(vision_model, \"pre_layrnorm\"):\n",
    "\t\teps = vision_model.pre_layrnorm.epsilon\n",
    "\t\tprint(f\"  Vision LN epsilon: {eps}\")\n",
    "\t\tif abs(eps - 1e-5) < 1e-9:\n",
    "\t\t\tprint(\"  ‚úì Vision Epsilon matches PyTorch (1e-5).\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"  ‚ùå FAIL: Vision Epsilon mismatch! Found {eps}.\")\n",
    "\telse:\n",
    "\t\tprint(\"  ‚ö† Could not locate vision_model.pre_layrnorm\")\n",
    "\n",
    "\t# Check Text Final-LayerNorm\n",
    "\tif text_model and hasattr(text_model, \"final_layer_norm\"):\n",
    "\t\teps = text_model.final_layer_norm.epsilon\n",
    "\t\tprint(f\"  Text LN epsilon:   {eps}\")\n",
    "\t\tif abs(eps - 1e-5) < 1e-9:\n",
    "\t\t\tprint(\"  ‚úì Text Epsilon matches PyTorch (1e-5).\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"  ‚ùå FAIL: Text Epsilon mismatch! Found {eps}.\")\n",
    "\telse:\n",
    "\t\t print(\"  ‚ö† Could not locate text_model.final_layer_norm\")\n",
    "\n",
    "# Run the verification\n",
    "robust_verify_clip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. BASIC BLOCKS (DF-GAN & GALIP Components)\n",
    "# ==============================================================================\n",
    "\n",
    "class Affine(layers.Layer):\n",
    "\t\"\"\"\n",
    "\t100% Faithful replication of PyTorch GALIP's Affine layer.\n",
    "\t\n",
    "\tPyTorch signature: Affine(cond_dim, num_features)\n",
    "\t\n",
    "\tPyTorch structure:\n",
    "\t\tfc_gamma: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "\t\tfc_beta:  Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "\t\n",
    "\tInitialization:\n",
    "\t\tfc_gamma.linear2: weight=0, bias=1 (so initial gamma=1, identity scaling)\n",
    "\t\tfc_beta.linear2:  weight=0, bias=0 (so initial beta=0, no shift)\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, cond_dim, num_features):\n",
    "\t\tsuper(Affine, self).__init__()\n",
    "\t\tself.cond_dim = cond_dim\n",
    "\t\tself.num_features = num_features\n",
    "\t\t\n",
    "\t\t# fc_gamma: 2-layer MLP\n",
    "\t\t# PyTorch: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "\t\t# First layer: cond_dim -> num_features, he_uniform init (matches PyTorch Linear default)\n",
    "\t\t# Second layer: num_features -> num_features, zeros weight, ones bias\n",
    "\t\tself.gamma_linear1 = layers.Dense(num_features, kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\tself.gamma_linear2 = layers.Dense(\n",
    "\t\t\tnum_features, \n",
    "\t\t\tkernel_initializer='zeros',\n",
    "\t\t\tbias_initializer='ones'\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# fc_beta: 2-layer MLP\n",
    "\t\t# PyTorch: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "\t\t# First layer: cond_dim -> num_features, he_uniform init (matches PyTorch Linear default)\n",
    "\t\t# Second layer: num_features -> num_features, zeros weight, zeros bias\n",
    "\t\tself.beta_linear1 = layers.Dense(num_features, kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\tself.beta_linear2 = layers.Dense(\n",
    "\t\t\tnum_features,\n",
    "\t\t\tkernel_initializer='zeros',\n",
    "\t\t\tbias_initializer='zeros'\n",
    "\t\t)\n",
    "\n",
    "\tdef call(self, x, y):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tx: [B, H, W, C] feature map\n",
    "\t\t\ty: [B, cond_dim] conditioning vector\n",
    "\t\t\"\"\"\n",
    "\t\t# Compute gamma (scale)\n",
    "\t\tgamma = self.gamma_linear1(y)\n",
    "\t\tgamma = tf.nn.relu(gamma)\n",
    "\t\tgamma = self.gamma_linear2(gamma)  # [B, num_features]\n",
    "\t\t\n",
    "\t\t# Compute beta (shift)\n",
    "\t\tbeta = self.beta_linear1(y)\n",
    "\t\tbeta = tf.nn.relu(beta)\n",
    "\t\tbeta = self.beta_linear2(beta)  # [B, num_features]\n",
    "\t\t\n",
    "\t\t# Reshape for broadcasting: [B, 1, 1, C]\n",
    "\t\tgamma = tf.reshape(gamma, [-1, 1, 1, self.num_features])\n",
    "\t\tbeta = tf.reshape(beta, [-1, 1, 1, self.num_features])\n",
    "\t\t\n",
    "\t\treturn gamma * x + beta\n",
    "\n",
    "\n",
    "class DFBLK(layers.Layer):\n",
    "\t\"\"\"\n",
    "\t100% Faithful replication of PyTorch GALIP's DFBLK.\n",
    "\t\n",
    "\tPyTorch signature: DFBLK(cond_dim, in_ch)\n",
    "\t\n",
    "\tStructure:\n",
    "\t\taffine0 -> LeakyReLU(0.2) -> affine1 -> LeakyReLU(0.2)\n",
    "\t\n",
    "\tNO convolutions - just two affine transforms with activations.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, cond_dim, in_ch):\n",
    "\t\tsuper(DFBLK, self).__init__()\n",
    "\t\t# PyTorch: self.affine0 = Affine(cond_dim, in_ch)\n",
    "\t\t# Pass cond_dim to match PyTorch signature exactly\n",
    "\t\tself.affine0 = Affine(cond_dim, in_ch)\n",
    "\t\tself.affine1 = Affine(cond_dim, in_ch)\n",
    "\n",
    "\tdef call(self, x, y):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tx: [B, H, W, C] feature map\n",
    "\t\t\ty: [B, cond_dim] conditioning vector\n",
    "\t\tReturns:\n",
    "\t\t\t[B, H, W, C] transformed feature map\n",
    "\t\t\"\"\"\n",
    "\t\th = self.affine0(x, y)\n",
    "\t\th = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "\t\th = self.affine1(h, y)\n",
    "\t\th = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "\t\treturn h\n",
    "\n",
    "\n",
    "\n",
    "class G_Block(layers.Layer):\n",
    "\t\"\"\"\n",
    "\t100% Faithful replication of PyTorch GALIP's G_Block.\n",
    "\t\n",
    "\tPyTorch signature: G_Block(cond_dim, in_ch, out_ch, imsize)\n",
    "\t\n",
    "\tStructure:\n",
    "\t\t1. Interpolate to target size\n",
    "\t\t2. Residual path: fuse1(DFBLK) -> c1(conv) -> fuse2(DFBLK) -> c2(conv)\n",
    "\t\t3. Shortcut path: c_sc(1x1 conv) if in_ch != out_ch\n",
    "\t\t4. Output: shortcut + residual\n",
    "\t\n",
    "\tNote: imsize is handled dynamically via target_size parameter in call().\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, cond_dim, in_ch, out_ch):\n",
    "\t\tsuper(G_Block, self).__init__()\n",
    "\t\tself.learnable_sc = in_ch != out_ch\n",
    "\t\t\n",
    "\t\t# PyTorch: nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "\t\t# CRITICAL: kernel_initializer=pytorch_kaiming_uniform() to match PyTorch Conv2d default\n",
    "\t\tself.c1 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\tself.c2 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\t\n",
    "\t\t# PyTorch: DFBLK(cond_dim, in_ch) and DFBLK(cond_dim, out_ch)\n",
    "\t\tself.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "\t\tself.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "\t\t\n",
    "\t\t# Shortcut: 1x1 conv only if channel dimensions change\n",
    "\t\t# PyTorch: nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "\t\tif self.learnable_sc:\n",
    "\t\t\tself.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\n",
    "\tdef call(self, h, y, target_size):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\th: [B, H, W, in_ch] input feature map\n",
    "\t\t\ty: [B, cond_dim] conditioning vector\n",
    "\t\t\ttarget_size: int, target spatial size for interpolation\n",
    "\t\tReturns:\n",
    "\t\t\t[B, target_size, target_size, out_ch] output feature map\n",
    "\t\t\"\"\"\n",
    "\t\t# PyTorch: h = F.interpolate(h, size=(self.imsize, self.imsize))\n",
    "\t\th = tf.image.resize(h, [target_size, target_size], method='nearest')\n",
    "\t\t\n",
    "\t\t# Residual path: fuse1 -> c1 -> fuse2 -> c2\n",
    "\t\t# PyTorch: h = self.fuse1(h, y); h = self.c1(h); h = self.fuse2(h, y); h = self.c2(h)\n",
    "\t\tres = self.fuse1(h, y)\n",
    "\t\tres = self.c1(res)\n",
    "\t\tres = self.fuse2(res, y)\n",
    "\t\tres = self.c2(res)\n",
    "\t\t\n",
    "\t\t# Shortcut path\n",
    "\t\tif self.learnable_sc:\n",
    "\t\t\tsc = self.c_sc(h)\n",
    "\t\telse:\n",
    "\t\t\tsc = h\n",
    "\t\t\t\n",
    "\t\treturn sc + res\n",
    "\n",
    "\n",
    "class D_Block(layers.Layer):\n",
    "\t\"\"\"\n",
    "\t100% Faithful replication of PyTorch GALIP's D_Block.\n",
    "\t\n",
    "\tPyTorch signature: D_Block(fin, fout, k, s, p, res, CLIP_feat)\n",
    "\t\n",
    "\tPyTorch structure:\n",
    "\t\tconv_r: Conv2D(fin, fout, k, s, p, bias=False) -> LeakyReLU(0.2) -> Conv2D(fout, fout, k, s, p, bias=False) -> LeakyReLU(0.2)\n",
    "\t\tconv_s: Conv2D(fin, fout, 1, stride=1, padding=0) for shortcut\n",
    "\t\tgamma: learnable scalar for residual (init=0)\n",
    "\t\tbeta: learnable scalar for CLIP features (init=0)\n",
    "\t\n",
    "\tNote: All PyTorch D_Block instantiations use k=3, s=1, p=1, so we hardcode these.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, fin, fout, is_down=False, is_res=True, clip_feat=False):\n",
    "\t\tsuper(D_Block, self).__init__()\n",
    "\t\tself.is_res = is_res\n",
    "\t\tself.clip_feat = clip_feat\n",
    "\t\tself.learned_shortcut = (fin != fout)\n",
    "\t\t\n",
    "\t\t# Main conv path (PyTorch: k=3, s=1, p=1)\n",
    "\t\t# CRITICAL: kernel_initializer=pytorch_kaiming_uniform() to match PyTorch Conv2d default\n",
    "\t\tself.conv_r1 = layers.Conv2D(fout, 3, padding='same', use_bias=False, kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\tself.conv_r2 = layers.Conv2D(fout, 3, padding='same', use_bias=False, kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\t\n",
    "\t\t# Shortcut conv (PyTorch: 1x1, stride=1, padding=0)\n",
    "\t\t# CRITICAL: padding='valid' to match PyTorch padding=0\n",
    "\t\tself.conv_s = layers.Conv2D(fout, 1, padding='valid', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\t\n",
    "\t\t# Learnable scalars (initialized to 0, matching PyTorch torch.zeros(1))\n",
    "\t\tif is_res:\n",
    "\t\t\tself.gamma = tf.Variable(0.0, trainable=True, name='gamma')\n",
    "\t\tif clip_feat:\n",
    "\t\t\tself.beta = tf.Variable(0.0, trainable=True, name='beta')\n",
    "\n",
    "\tdef call(self, x, clip_f=None):\n",
    "\t\t# Residual path\n",
    "\t\tres = self.conv_r1(x)\n",
    "\t\tres = tf.nn.leaky_relu(res, alpha=0.2)\n",
    "\t\tres = self.conv_r2(res)\n",
    "\t\tres = tf.nn.leaky_relu(res, alpha=0.2)\n",
    "\t\t\n",
    "\t\t# Shortcut\n",
    "\t\tif self.learned_shortcut:\n",
    "\t\t\tx = self.conv_s(x)\n",
    "\t\t\n",
    "\t\t# Combine based on flags\n",
    "\t\tout = x\n",
    "\t\tif self.is_res:\n",
    "\n",
    "\t\t\tout = out + self.gamma * res     \n",
    "\n",
    "\t\tif self.clip_feat and clip_f is not None:            \n",
    "\t\t\tout = out + self.beta * clip_f\n",
    "\t\t\t\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CLIP ADAPTER (Robust Fix)\n",
    "# ==============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_layer_safe(model, layer_name):\n",
    "\t\"\"\"\n",
    "\tRobustly find a layer in a Keras model, handling the 'clip' wrapper case.\n",
    "\t\"\"\"\n",
    "\t# 1. Try direct access\n",
    "\tif hasattr(model, layer_name):\n",
    "\t\treturn getattr(model, layer_name)\n",
    "\t\n",
    "\t# 2. Try inside 'clip' wrapper\n",
    "\tif hasattr(model, 'clip') and hasattr(model.clip, layer_name):\n",
    "\t\treturn getattr(model.clip, layer_name)\n",
    "\t\t\n",
    "\t# 3. Search layer list (Fallback)\n",
    "\tfor layer in model.layers:\n",
    "\t\tif layer.name == layer_name:\n",
    "\t\t\treturn layer\n",
    "\t\tif layer.name == 'clip':\n",
    "\t\t\t if hasattr(layer, layer_name):\n",
    "\t\t\t\t return getattr(layer, layer_name)\n",
    "\t\t\t\t \n",
    "\traise AttributeError(f\"Could not find '{layer_name}' in TFCLIPModel. Available: {[l.name for l in model.layers]}\")\n",
    "\n",
    "# Helper to find sub-attributes like pre_layrnorm inside vision_model\n",
    "def get_sublayer_safe(model, possible_names):\n",
    "\tfor name in possible_names:\n",
    "\t\tif hasattr(model, name):\n",
    "\t\t\treturn getattr(model, name)\n",
    "\t# If not found as attribute, check layers list\n",
    "\tfor layer in model.layers:\n",
    "\t\tif layer.name in possible_names:\n",
    "\t\t\treturn layer\n",
    "\traise AttributeError(f\"Could not find any of {possible_names} in model. Available: {[l.name for l in model.layers]}\")\n",
    "\n",
    "\n",
    "class M_Block(layers.Layer):\n",
    "\t\"\"\"\n",
    "\t100% Faithful replication of PyTorch GALIP's M_Block.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, in_ch, mid_ch, out_ch, cond_dim, k, s, p):\n",
    "\t\tsuper(M_Block, self).__init__()\n",
    "\t\tself.conv1 = layers.Conv2D(mid_ch, k, strides=s, padding='same', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\tself.fuse1 = DFBLK(cond_dim, mid_ch)\n",
    "\t\tself.conv2 = layers.Conv2D(out_ch, k, strides=s, padding='same', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\tself.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "\t\tself.learnable_sc = in_ch != out_ch\n",
    "\t\tif self.learnable_sc:\n",
    "\t\t\tself.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\n",
    "\tdef call(self, h, c):\n",
    "\t\tres = self.conv1(h)\n",
    "\t\tres = self.fuse1(res, c)\n",
    "\t\tres = self.conv2(res)\n",
    "\t\tres = self.fuse2(res, c)\n",
    "\t\tif self.learnable_sc:\n",
    "\t\t\tsc = self.c_sc(h)\n",
    "\t\telse:\n",
    "\t\t\tsc = h\n",
    "\t\treturn sc + res\n",
    "\n",
    "\n",
    "class CLIP_Mapper(layers.Layer):\n",
    "\t\"\"\"\n",
    "\t100% Faithful replication of PyTorch GALIP's CLIP_Mapper.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, clip_model):\n",
    "\t\tsuper(CLIP_Mapper, self).__init__()\n",
    "\t\t\n",
    "\t\tself.vision_model = get_layer_safe(clip_model, 'vision_model')\n",
    "\t\t\n",
    "\t\t# FIX: Find sub-layers robustly\n",
    "\t\tself.embeddings = get_sublayer_safe(self.vision_model, ['embeddings'])\n",
    "\t\tself.pre_layrnorm = get_sublayer_safe(self.vision_model, ['pre_layrnorm', 'pre_layernorm', 'layernorm_pre'])\n",
    "\t\t# encoder is standard, usually .encoder\n",
    "\t\tself.encoder = get_sublayer_safe(self.vision_model, ['encoder'])\n",
    "\t\t\n",
    "\t\t# Freeze\n",
    "\t\tself.vision_model.trainable = False\n",
    "\t\t\n",
    "\tdef call(self, img_feats, prompts):\n",
    "\t\tB = tf.shape(img_feats)[0]\n",
    "\t\tH = tf.shape(img_feats)[1]\n",
    "\t\tW = tf.shape(img_feats)[2]\n",
    "\t\t\n",
    "\t\tprompts = tf.cast(prompts, img_feats.dtype)\n",
    "\t\tx = tf.reshape(img_feats, [B, H * W, 768])\n",
    "\t\t\n",
    "\t\t# Add CLS token\n",
    "\t\tcls_token = self.embeddings.class_embedding\n",
    "\t\tcls_token = tf.cast(cls_token, x.dtype)\n",
    "\t\tcls_token = tf.reshape(cls_token, [1, 1, 768])\n",
    "\t\tcls_token = tf.tile(cls_token, [B, 1, 1])\n",
    "\t\tx = tf.concat([cls_token, x], axis=1)\n",
    "\t\t\n",
    "\t\t# Add positional embedding\n",
    "\t\tpos_embed_obj = self.embeddings.position_embedding\n",
    "\t\tif hasattr(pos_embed_obj, 'weights'):\n",
    "\t\t\t pos_embed = pos_embed_obj.weights[0]\n",
    "\t\telse:\n",
    "\t\t\t pos_embed = pos_embed_obj\n",
    "\t\t\t \n",
    "\t\tpos_embed = tf.cast(pos_embed, x.dtype)\n",
    "\t\tseq_len = tf.shape(x)[1]\n",
    "\t\tx = x + pos_embed[:seq_len, :]\n",
    "\t\t\n",
    "\t\t# Pre-LayerNorm (Using found layer)\n",
    "\t\tx = self.pre_layrnorm(x)\n",
    "\t\t\n",
    "\t\tselected = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\t\tprompt_idx = 0\n",
    "\t\t\n",
    "\t\tfor i, layer in enumerate(self.encoder.layers):\n",
    "\t\t\tif i in selected:\n",
    "\t\t\t\tp = prompts[:, prompt_idx, :]\n",
    "\t\t\t\tp = tf.expand_dims(p, 1)\n",
    "\t\t\t\tx = tf.concat([x, p], axis=1)\n",
    "\t\t\t\t# Explicit None args\n",
    "\t\t\t\tlayer_out = layer(x, attention_mask=None, output_attentions=False, training=False, causal_attention_mask=None)\n",
    "\t\t\t\tx = layer_out[0]\n",
    "\t\t\t\tx = x[:, :-1, :]\n",
    "\t\t\t\tprompt_idx += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tlayer_out = layer(x, attention_mask=None, output_attentions=False, training=False, causal_attention_mask=None)\n",
    "\t\t\t\tx = layer_out[0]\n",
    "\t\t\n",
    "\t\tx = x[:, 1:, :]\n",
    "\t\tx = tf.reshape(x, [B, H, W, 768])\n",
    "\t\t\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class CLIP_Adapter(layers.Layer):\n",
    "\t\"\"\"\n",
    "\t100% Faithful replication of PyTorch GALIP's CLIP_Adapter.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num, clip_model):\n",
    "\t\tsuper(CLIP_Adapter, self).__init__()\n",
    "\t\tself.CLIP_ch = CLIP_ch\n",
    "\t\tself.f_blocks = []\n",
    "\t\tself.f_blocks.append(M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "\t\tfor _ in range(map_num - 1):\n",
    "\t\t\tself.f_blocks.append(M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "\t\tself.conv_fuse = layers.Conv2D(CLIP_ch, 5, strides=1, padding='same', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\tself.CLIP_ViT = CLIP_Mapper(clip_model)\n",
    "\t\tself.conv_out = layers.Conv2D(G_ch, 5, strides=1, padding='same', kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\tself.fc_prompt = layers.Dense(CLIP_ch * 8, kernel_initializer=pytorch_kaiming_uniform())\n",
    "\n",
    "\tdef call(self, out, c):\n",
    "\t\tprompts = self.fc_prompt(c)\n",
    "\t\tprompts = tf.reshape(prompts, [-1, 8, self.CLIP_ch])\n",
    "\t\tfor FBlock in self.f_blocks:\n",
    "\t\t\tout = FBlock(out, c)\n",
    "\t\tfuse_feat = self.conv_fuse(out)\n",
    "\t\tmap_feat = self.CLIP_ViT(fuse_feat, prompts)\n",
    "\t\treturn self.conv_out(fuse_feat + 0.1 * map_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. MODELS (Fixed & Robust)\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# --- Robust Layer Lookup Helpers ---\n",
    "def get_layer_safe(model, layer_name):\n",
    "\tif hasattr(model, layer_name): return getattr(model, layer_name)\n",
    "\tif hasattr(model, 'clip') and hasattr(model.clip, layer_name): return getattr(model.clip, layer_name)\n",
    "\tfor layer in model.layers:\n",
    "\t\tif layer.name == layer_name: return layer\n",
    "\t\tif layer.name == 'clip' and hasattr(layer, layer_name): return getattr(layer, layer_name)\n",
    "\traise AttributeError(f\"Could not find '{layer_name}' in TFCLIPModel.\")\n",
    "\n",
    "def get_sublayer_safe(model, possible_names):\n",
    "\tfor name in possible_names:\n",
    "\t\tif hasattr(model, name): return getattr(model, name)\n",
    "\tfor layer in model.layers:\n",
    "\t\tfor name in possible_names:\n",
    "\t\t\tif name in layer.name: return layer\n",
    "\traise AttributeError(f\"Could not find any of {possible_names} in model.\")\n",
    "\n",
    "# --- Encoders ---\n",
    "class CLIP_Text_Encoder(layers.Layer):\n",
    "\tdef __init__(self, clip_model):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.text_model = get_layer_safe(clip_model, 'text_model')\n",
    "\t\tself.text_projection = get_layer_safe(clip_model, 'text_projection')\n",
    "\t\tself.text_model.trainable = False\n",
    "\t\tself.text_projection.trainable = False\n",
    "\t\t\n",
    "\tdef call(self, input_ids, attention_mask=None):\n",
    "\t\toutputs = self.text_model(\n",
    "\t\t\tinput_ids=input_ids, \n",
    "\t\t\tattention_mask=attention_mask,\n",
    "\t\t\tposition_ids=None,\n",
    "\t\t\toutput_attentions=False,\n",
    "\t\t\toutput_hidden_states=False,\n",
    "\t\t\treturn_dict=True\n",
    "\t\t)\n",
    "\t\tword_emb = outputs.last_hidden_state\n",
    "\t\teot_indices = tf.argmax(tf.cast(input_ids, tf.int32), axis=-1)\n",
    "\t\tbatch_size = tf.shape(input_ids)[0]\n",
    "\t\tbatch_indices = tf.range(batch_size, dtype=tf.int64)\n",
    "\t\tgather_indices = tf.stack([batch_indices, tf.cast(eot_indices, tf.int64)], axis=1)\n",
    "\t\tpooled_output = tf.gather_nd(word_emb, gather_indices)\n",
    "\t\tsent_emb = self.text_projection(pooled_output)\n",
    "\t\treturn sent_emb, word_emb\n",
    "\t\n",
    "\t@property\n",
    "\tdef trainable_weights(self): return []\n",
    "\t@property  \n",
    "\tdef non_trainable_weights(self): return self.text_model.weights + self.text_projection.weights\n",
    "\n",
    "class CLIP_Image_Encoder(layers.Layer):\n",
    "\tdef __init__(self, clip_model):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.vision_model = get_layer_safe(clip_model, 'vision_model')\n",
    "\t\tself.visual_projection = get_layer_safe(clip_model, 'visual_projection')\n",
    "\t\tself.embeddings = get_sublayer_safe(self.vision_model, ['embeddings'])\n",
    "\t\tself.pre_layrnorm = get_sublayer_safe(self.vision_model, ['pre_layrnorm', 'pre_layernorm', 'layernorm_pre'])\n",
    "\t\tself.post_layernorm = get_sublayer_safe(self.vision_model, ['post_layernorm', 'post_layernorm', 'layernorm_post'])\n",
    "\t\tself.encoder = get_sublayer_safe(self.vision_model, ['encoder'])\n",
    "\t\tself.vision_model.trainable = False\n",
    "\t\tself.visual_projection.trainable = False\n",
    "\t\t\n",
    "\tdef transf_to_CLIP_input(self, inputs):\n",
    "\t\tx = (inputs + 1.0) * 0.5\n",
    "\t\tx = tf.image.resize(x, [224, 224], method='bicubic')\n",
    "\t\tmean = tf.constant([0.48145466, 0.4578275, 0.40821073], dtype=x.dtype)\n",
    "\t\tstd = tf.constant([0.26862954, 0.26130258, 0.27577711], dtype=x.dtype)\n",
    "\t\tx = (x - mean) / std\n",
    "\t\treturn x\n",
    "\n",
    "\tdef call(self, img):\n",
    "\t\tx = self.transf_to_CLIP_input(img)\n",
    "\t\tx = tf.transpose(x, [0, 3, 1, 2]) # Fix: Transpose to NCHW\n",
    "\t\tx = self.embeddings(x)\n",
    "\t\tx = self.pre_layrnorm(x)\n",
    "\t\tlocal_features = []\n",
    "\t\tselected = [1, 4, 8]\n",
    "\t\tfor i, layer in enumerate(self.encoder.layers):\n",
    "\t\t\tlayer_out = layer(x, attention_mask=None, causal_attention_mask=None, output_attentions=False, training=False)\n",
    "\t\t\tx = layer_out[0]\n",
    "\t\t\tif i in selected:\n",
    "\t\t\t\tgrid = x[:, 1:, :] \n",
    "\t\t\t\tB = tf.shape(grid)[0]\n",
    "\t\t\t\tgrid = tf.reshape(grid, [B, 7, 7, 768])\n",
    "\t\t\t\tlocal_features.append(grid)\n",
    "\t\tcls_token = self.post_layernorm(x[:, 0, :])\n",
    "\t\tglobal_emb = self.visual_projection(cls_token)\n",
    "\t\tlocal_features = tf.stack(local_features, axis=1)\n",
    "\t\treturn local_features, global_emb\n",
    "\n",
    "# --- GAN Models ---\n",
    "class NetG(Model):\n",
    "    def __init__(self, ngf, nz, cond_dim, clip_model, output_size=64):\n",
    "        super(NetG, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.output_size = output_size\n",
    "        self.code_sz, self.code_ch, self.mid_ch = 7, 64, 32\n",
    "        self.CLIP_ch = 768\n",
    "        self.fc_code = layers.Dense(self.code_sz * self.code_sz * self.code_ch, kernel_initializer=pytorch_kaiming_uniform())\n",
    "        self.mapping = CLIP_Adapter(self.code_ch, self.mid_ch, self.code_ch, ngf * 8, self.CLIP_ch, cond_dim + nz, 3, 1, 1, 4, clip_model)\n",
    "        # 6 G_Blocks for 224x224 output\n",
    "        self.g_blocks = []\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 8))   # 7  -> 8\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 8))   # 8  -> 16\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 8))   # 16 -> 32\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 4))   # 32 -> 64\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 4, ngf * 2))   # 64 -> 128\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 2, ngf * 1))   # 128-> 224\n",
    "        self.target_sizes = [8, 16, 32, 64, 128, 224]\n",
    "        self.to_rgb = tf.keras.Sequential([layers.LeakyReLU(0.2), layers.Conv2D(3, 3, padding='same', kernel_initializer=pytorch_kaiming_uniform())])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        noise, c = inputs\n",
    "        cond = tf.concat([noise, c], axis=1)\n",
    "        out = self.fc_code(noise)\n",
    "        out = tf.reshape(out, [-1, self.code_sz, self.code_sz, self.code_ch])\n",
    "        out = self.mapping(out, cond)\n",
    "        for block, target_size in zip(self.g_blocks, self.target_sizes):\n",
    "            out = block(out, cond, target_size=target_size)\n",
    "        out = self.to_rgb(out)\n",
    "        out = tf.nn.tanh(out)\n",
    "        # Downscale 224 -> 64\n",
    "        if self.output_size != 224:\n",
    "            out = tf.image.resize(out, [self.output_size, self.output_size], method='bilinear')\n",
    "        return out\n",
    "\n",
    "class NetD(Model):\n",
    "\tdef __init__(self, ndf):\n",
    "\t\tsuper(NetD, self).__init__()\n",
    "\t\tself.d_blocks = [D_Block(768, 768, is_res=True, clip_feat=True) for _ in range(2)]\n",
    "\t\tself.main = D_Block(768, 512, is_res=True, clip_feat=False)\n",
    "\tdef call(self, h):\n",
    "\t\tout = h[:, 0]\n",
    "\t\tfor idx in range(len(self.d_blocks)): out = self.d_blocks[idx](out, h[:, idx+1])\n",
    "\t\tout = self.main(out)\n",
    "\t\treturn out\n",
    "\n",
    "class NetC(Model):\n",
    "\tdef __init__(self, ndf, cond_dim):\n",
    "\t\tsuper(NetC, self).__init__()\n",
    "\t\tself.cond_dim = cond_dim\n",
    "\t\tself.joint_conv = tf.keras.Sequential([\n",
    "\t\t\tlayers.Conv2D(ndf * 2, 4, strides=1, padding='valid', use_bias=False, kernel_initializer=pytorch_kaiming_uniform()),\n",
    "\t\t\tlayers.LeakyReLU(0.2),\n",
    "\t\t\tlayers.Conv2D(1, 4, strides=1, padding='valid', use_bias=False, kernel_initializer=pytorch_kaiming_uniform())\n",
    "\t\t])\n",
    "\tdef call(self, out, cond):\n",
    "\t\tB = tf.shape(out)[0]\n",
    "\t\tcond = tf.reshape(cond, [B, 1, 1, self.cond_dim])\n",
    "\t\tcond = tf.tile(cond, [1, 7, 7, 1])\n",
    "\t\th_c = tf.concat([out, cond], axis=3)\n",
    "\t\treturn self.joint_conv(h_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "\t# Hinge loss for Discriminator\n",
    "\t# Real: min(0, -1 + real) -> ReLU(1 - real)\n",
    "\t# Fake: min(0, -1 - fake) -> ReLU(1 + fake)\n",
    "\treal_loss = tf.reduce_mean(tf.nn.relu(1.0 - real_logits))\n",
    "\tfake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake_logits))\n",
    "\treturn real_loss + fake_loss\n",
    "\n",
    "def generator_adversarial_loss(fake_logits):\n",
    "\t# Hinge loss for Generator (maximize D's output)\n",
    "\treturn -tf.reduce_mean(fake_logits)\n",
    "  \n",
    "  \n",
    "def clip_matching_loss(image_emb, text_emb):\n",
    "\t# image_emb: [B, 512] (from CLIP_Image_Encoder global_emb)\n",
    "\t# text_emb: [B, 512] (from CLIP_Text_Encoder sent_emb)\n",
    "\t\n",
    "\t# 1. Normalize embeddings (Crucial for Cosine Similarity)\n",
    "\timage_emb = tf.nn.l2_normalize(image_emb, axis=1)\n",
    "\ttext_emb = tf.nn.l2_normalize(text_emb, axis=1)\n",
    "\t\n",
    "\t# 2. Compute Cosine Distance (1 - Cosine Similarity)\n",
    "\t# Reducing across the batch dimension\n",
    "\tsim = tf.reduce_sum(image_emb * text_emb, axis=1)\n",
    "\tloss = 1.0 - sim\n",
    "\t\n",
    "\treturn tf.reduce_mean(loss)\n",
    "  \n",
    "  \n",
    "def MA_GP(discriminator, net_c, CLIP_real, sent_emb, pred_real):\n",
    "\t\"\"\"\n",
    "\tMatching-Aware Gradient Penalty (MA-GP).\n",
    "\tCalculates gradient of D(real, text) w.r.t. input features.\n",
    "\tTarget: Penalize gradients to enforce Lipschitz continuity.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tdiscriminator: NetD model\n",
    "\t\tnet_c: NetC model\n",
    "\t\tCLIP_real: [B, 3, 7, 7, 768] Real image CLIP features\n",
    "\t\tsent_emb: [B, 512] Text embeddings\n",
    "\t\tpred_real: [B, 1, 1, 1] The validity score (output of NetC)\n",
    "\t\t\n",
    "\tReturns:\n",
    "\t\tgradient_penalty: Scalar tensor\n",
    "\t\"\"\"\n",
    "\t# In TensorFlow, we need the tape to verify the gradients.\n",
    "\t# We assume this function is called INSIDE the GradientTape where \n",
    "\t# CLIP_real and sent_emb were watched.\n",
    "\t\n",
    "\t# 1. Get gradients of the prediction w.r.t inputs\n",
    "\t# Note: We need gradients w.r.t BOTH visual features and text embeddings\n",
    "\tgrads = tf.gradients(pred_real, [CLIP_real, sent_emb])\n",
    "\t\n",
    "\t# 2. Flatten and Concatenate\n",
    "\tgrad_img = tf.reshape(grads[0], [tf.shape(grads[0])[0], -1]) # [B, N_img]\n",
    "\tgrad_txt = tf.reshape(grads[1], [tf.shape(grads[1])[0], -1]) # [B, N_txt]\n",
    "\tgrad = tf.concat([grad_img, grad_txt], axis=1) # [B, N_total]\n",
    "\t\n",
    "\t# 3. Calculate Norm\n",
    "\tgrad_l2norm = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "\t\n",
    "\t# 4. Apply Penalty (Power of 6 is specific to DF-GAN/GALIP)\n",
    "\t# Weight is typically 2.0 in their repo\n",
    "\td_loss_gp = 2.0 * tf.reduce_mean(tf.pow(grad_l2norm, 6))\n",
    "\t\n",
    "\treturn d_loss_gp\n",
    "\n",
    "def predict_loss(net_c, d_feats, sent_emb, negative=False):\n",
    "\t\"\"\"\n",
    "\tComputes Hinge Loss component.\n",
    "\tArgs:\n",
    "\t\tnegative: True for Fake/Mismatch (minimize -1 - D), False for Real (minimize -1 + D)\n",
    "\t\"\"\"\n",
    "\t# NetC output: [B, 1, 1, 1] -> Flatten to [B]\n",
    "\tlogits = net_c(d_feats, sent_emb, training=True)\n",
    "\tlogits = tf.reshape(logits, [-1])\n",
    "\t\n",
    "\tif negative:\n",
    "\t\t# Fake or Mismatch: ReLU(1 + D(x))\n",
    "\t\tloss = tf.reduce_mean(tf.nn.relu(1.0 + logits))\n",
    "\telse:\n",
    "\t\t# Real: ReLU(1 - D(x))\n",
    "\t\tloss = tf.reduce_mean(tf.nn.relu(1.0 - logits))\n",
    "\t\t\n",
    "\treturn logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def DiffAugment(x, policy='translation'):\n",
    "\t\"\"\"\n",
    "\tTensorFlow implementation of DiffAugment.\n",
    "\tSupports 'color', 'translation', 'cutout'.\n",
    "\t\"\"\"\n",
    "\tif policy:\n",
    "\t\tif 'color' in policy:\n",
    "\t\t\tx = rand_brightness(x)\n",
    "\t\t\tx = rand_saturation(x)\n",
    "\t\t\tx = rand_contrast(x)\n",
    "\t\tif 'translation' in policy:\n",
    "\t\t\tx = rand_translation(x)\n",
    "\t\tif 'cutout' in policy:\n",
    "\t\t\tx = rand_cutout(x)\n",
    "\treturn x\n",
    "\n",
    "# --- Augmentation Primitives ---\n",
    "def rand_brightness(x):\n",
    "\tmagnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=-0.5, maxval=0.5)\n",
    "\tx = x + magnitude\n",
    "\treturn tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_saturation(x):\n",
    "\tmagnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.0, maxval=2.0)\n",
    "\tx_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "\tx = (x - x_mean) * magnitude + x_mean\n",
    "\treturn tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_contrast(x):\n",
    "\tmagnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.5, maxval=1.5)\n",
    "\tx_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "\tx = (x - x_mean) * magnitude + x_mean\n",
    "\treturn tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "\tbatch_size = tf.shape(x)[0]\n",
    "\timg_size = tf.shape(x)[1]\n",
    "\tshift = int(64 * ratio)\n",
    "\t\n",
    "\t# Pad the image with reflection\n",
    "\tx_padded = tf.pad(x, [[0, 0], [shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "\t\n",
    "\t# Vectorized Random Crop using crop_and_resize\n",
    "\tpadded_size = tf.cast(img_size + 2*shift, tf.float32)\n",
    "\tmax_offset = 2 * shift\n",
    "\t\n",
    "\toffsets_y = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "\toffsets_x = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "\t\n",
    "\toffsets_y = tf.cast(offsets_y, tf.float32)\n",
    "\toffsets_x = tf.cast(offsets_x, tf.float32)\n",
    "\t\n",
    "\t# Normalize coordinates to [0, 1] for crop_and_resize\n",
    "\ty1 = offsets_y / padded_size\n",
    "\tx1 = offsets_x / padded_size\n",
    "\ty2 = (offsets_y + tf.cast(img_size, tf.float32)) / padded_size\n",
    "\tx2 = (offsets_x + tf.cast(img_size, tf.float32)) / padded_size\n",
    "\t\n",
    "\tboxes = tf.stack([y1, x1, y2, x2], axis=1) # [B, 4]\n",
    "\tbox_indices = tf.range(batch_size)\n",
    "\t\n",
    "\tx_translated = tf.image.crop_and_resize(\n",
    "\t\tx_padded, \n",
    "\t\tboxes, \n",
    "\t\tbox_indices, \n",
    "\t\tcrop_size=[img_size, img_size]\n",
    "\t)\n",
    "\t\n",
    "\treturn x_translated\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "\tbatch_size = tf.shape(x)[0]\n",
    "\timg_size = tf.shape(x)[1]\n",
    "\tcutout_size = int(64 * ratio // 2) * 2\n",
    "\t\n",
    "\tiy, ix = tf.meshgrid(tf.range(img_size), tf.range(img_size), indexing='ij')\n",
    "\tiy = tf.expand_dims(iy, 0) \n",
    "\tix = tf.expand_dims(ix, 0)\n",
    "\t\n",
    "\toffset_x = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "\toffset_y = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "\t\n",
    "\tmask_x = tf.math.logical_and(ix >= offset_x, ix < offset_x + cutout_size)\n",
    "\tmask_y = tf.math.logical_and(iy >= offset_y, iy < offset_y + cutout_size)\n",
    "\tmask_box = tf.math.logical_and(mask_x, mask_y)\n",
    "\t\n",
    "\tmask_keep = tf.cast(tf.math.logical_not(mask_box), x.dtype)\n",
    "\tmask_keep = tf.expand_dims(mask_keep, -1) \n",
    "\t\n",
    "\treturn x * mask_keep\n",
    "\n",
    "def save_sample_images(generator, text_encoder, tokenizer, fixed_noise, epoch, save_dir):\n",
    "    \"\"\"Generates images using fixed base sentences.\"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    base_sentences = [\n",
    "        \"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "        \"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "        \"the petals on this flower are white with a yellow center\",\n",
    "        \"this flower has a lot of small round pink petals.\",\n",
    "        \"this flower is orange in color, and has petals that are ruffled and rounded.\",\n",
    "        \"the flower has yellow petals and the center of it is brown.\",\n",
    "        \"this flower has petals that are blue and white.\",\n",
    "        \"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "    ]\n",
    "    \n",
    "    encodings = tokenizer(base_sentences, padding='max_length', truncation=True, max_length=77, return_tensors='tf')\n",
    "    text_embeds, _ = text_encoder(encodings['input_ids'], attention_mask=encodings['attention_mask'], training=False)\n",
    "    fake_imgs = generator([fixed_noise, text_embeds], training=False)\n",
    "    \n",
    "    fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "    fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(fake_imgs[i])\n",
    "        ax.set_title(base_sentences[i][:35] + \"...\", fontsize=7)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:03d}.png'), dpi=150)\n",
    "    plt.close()\n",
    "# ==============================================================================\n",
    "# 5. OPTIMIZED TRAINING PIPELINE (Fixed)\n",
    "# ==============================================================================\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import TFCLIPModel, CLIPTokenizer\n",
    "\n",
    "# --- A. Pre-computation Logic ---\n",
    "def precompute_data(df_path, image_encoder, tokenizer, word2Id_dict, id2word_dict):\n",
    "\tprint(\"--- üöÄ Starting Pre-computation (One-time Setup) ---\")\n",
    "\t\n",
    "\tdf = pd.read_pickle(df_path)\n",
    "\tunique_image_paths = df['ImagePath'].unique()\n",
    "\tpath_to_index = {path: i for i, path in enumerate(unique_image_paths)}\n",
    "\tprint(f\"Found {len(unique_image_paths)} unique images.\")\n",
    "\t\n",
    "\t# Helper to load image\n",
    "\tdef load_img(path):\n",
    "\t\timg = tf.io.read_file(path)\n",
    "\t\timg = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "\t\timg = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\t\timg = tf.image.resize(img, [64, 64], method='bicubic')\n",
    "\t\timg = (img * 2.0) - 1.0\n",
    "\t\treturn img\n",
    "\n",
    "\t# Extract Features\n",
    "\tprint(\"Extracting Image Features (This caches CLIP outputs to VRAM)...\")\n",
    "\timg_ds = tf.data.Dataset.from_tensor_slices(unique_image_paths)\n",
    "\timg_ds = img_ds.map(load_img, num_parallel_calls=tf.data.AUTOTUNE).batch(128)\n",
    "\t\n",
    "\tall_img_features = []\n",
    "\tfor batch_imgs in tqdm(img_ds, desc=\"Encoding Images\"):\n",
    "\t\t# We store the 7x7 local features needed by Discriminator\n",
    "\t\tlocal_feats, _ = image_encoder(batch_imgs) \n",
    "\t\tall_img_features.append(local_feats)\n",
    "\t\t\n",
    "\tcached_img_features = tf.concat(all_img_features, axis=0)\n",
    "\tprint(f\"‚úì Image Features Cached. Shape: {cached_img_features.shape}\")\n",
    "\n",
    "\t# Process Text\n",
    "\tprint(\"Tokenizing Text...\")\n",
    "\tcaptions_ids = df['Captions'].values\n",
    "\timage_paths = df['ImagePath'].values\n",
    "\t\n",
    "\tdataset_indices = []\n",
    "\tdataset_captions = []\n",
    "\t\n",
    "\tdef decode_ids(id_list):\n",
    "\t\twords = [id2word_dict.get(str(i), '') for i in id_list]\n",
    "\t\treturn ' '.join([w for w in words if w and w != '<PAD>'])\n",
    "\n",
    "\tfor caps, path in zip(captions_ids, image_paths):\n",
    "\t\timg_idx = path_to_index[path]\n",
    "\t\tfor cap_ids in caps:\n",
    "\t\t\ttext = decode_ids(cap_ids)\n",
    "\t\t\tdataset_indices.append(img_idx)\n",
    "\t\t\tdataset_captions.append(text)\n",
    "\t\t\t\n",
    "\tencodings = tokenizer(\n",
    "\t\tdataset_captions, padding='max_length', truncation=True, max_length=77, return_tensors='np'\n",
    "\t)\n",
    "\t\n",
    "\tcached_input_ids = encodings['input_ids']\n",
    "\tcached_masks = encodings['attention_mask']\n",
    "\tcached_indices = np.array(dataset_indices, dtype=np.int32)\n",
    "\t\n",
    "\tprint(f\"‚úì Text Prepared. Total Samples: {len(cached_indices)}\")\n",
    "\treturn cached_img_features, cached_input_ids, cached_masks, cached_indices\n",
    "\n",
    "# --- B. Fast Dataset Generator ---\n",
    "def fast_dataset_generator(img_feats, input_ids, masks, indices, batch_size):\n",
    "\tdataset = tf.data.Dataset.from_tensor_slices((indices, input_ids, masks))\n",
    "\tdataset = dataset.shuffle(len(indices), reshuffle_each_iteration=True)\n",
    "\tdataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\t\n",
    "\t@tf.function\n",
    "\tdef lookup_data(b_indices, b_ids, b_masks):\n",
    "\t\tb_imgs = tf.gather(img_feats, b_indices)\n",
    "\t\treturn b_imgs, b_ids, b_masks\n",
    "\t\n",
    "\tdataset = dataset.map(lookup_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\treturn dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "## ==============================================================================\n",
    "# OPTIMIZED TRAIN STEP (Compatible with your Logging)\n",
    "# ==============================================================================\n",
    "@tf.function\n",
    "def train_step_fast(real_img_feats, input_ids, attention_mask, \n",
    "\t\t\t\t\tgenerator, discriminator, net_c, \n",
    "\t\t\t\t\ttext_encoder, image_encoder,\n",
    "\t\t\t\t\tg_optimizer, d_optimizer, \n",
    "\t\t\t\t\tbatch_size, z_dim, sim_w=4.0):\n",
    "\t\n",
    "\t# 1. Encode Text (Fast)\n",
    "\tsent_emb, _ = text_encoder(input_ids, attention_mask=attention_mask)\n",
    "\tnoise = tf.random.normal([batch_size, z_dim])\n",
    "\t\n",
    "\t# ====================\n",
    "\t# Train Discriminator\n",
    "\t# ====================\n",
    "\twith tf.GradientTape() as d_tape:\n",
    "\t\t# Inner tape for MA-GP (Gradient Penalty)\n",
    "\t\twith tf.GradientTape() as gp_tape:\n",
    "\t\t\tgp_tape.watch([real_img_feats, sent_emb])\n",
    "\t\t\t\n",
    "\t\t\t# D(Real) - Using Cached Features!\n",
    "\t\t\treal_feats = discriminator(real_img_feats, training=True)\n",
    "\t\t\tpred_real = net_c(real_feats, sent_emb, training=True)\n",
    "\t\t\t\n",
    "\t\t# 1. MA-GP Calculation\n",
    "\t\t# Calculate gradient of output w.r.t inputs (Image + Text)\n",
    "\t\tgrads = gp_tape.gradient(pred_real, [real_img_feats, sent_emb])\n",
    "\t\t\n",
    "\t\tgrad_img = tf.reshape(grads[0], [batch_size, -1])\n",
    "\t\tgrad_txt = tf.reshape(grads[1], [batch_size, -1])\n",
    "\t\tgrad_flat = tf.concat([grad_img, grad_txt], axis=1)\n",
    "\t\tgrad_norm = tf.sqrt(tf.reduce_sum(tf.square(grad_flat), axis=1) + 1e-8)\n",
    "\t\terrD_MAGP = 2.0 * tf.reduce_mean(tf.pow(grad_norm, 6))\n",
    "\t\t\n",
    "\t\t# 2. Real Loss\n",
    "\t\terrD_real = tf.reduce_mean(tf.nn.relu(1.0 - pred_real))\n",
    "\t\t\n",
    "\t\t# 3. Mismatch Loss\n",
    "\t\tmis_sent_emb = tf.roll(sent_emb, shift=1, axis=0)\n",
    "\t\tpred_mis = net_c(real_feats, mis_sent_emb, training=True)\n",
    "\t\terrD_mis = tf.reduce_mean(tf.nn.relu(1.0 + pred_mis))\n",
    "\t\t\n",
    "\t\t# 4. Fake Loss\n",
    "\t\tfake_images = generator([noise, sent_emb], training=True)\n",
    "\t\tfake_images_stopped = tf.stop_gradient(fake_images)\n",
    "\t\tCLIP_fake, _ = image_encoder(fake_images_stopped)\n",
    "\t\t\n",
    "\t\tfake_feats = discriminator(CLIP_fake, training=True)\n",
    "\t\tpred_fake = net_c(fake_feats, sent_emb, training=True)\n",
    "\t\terrD_fake = tf.reduce_mean(tf.nn.relu(1.0 + pred_fake))\n",
    "\t\t\n",
    "\t\t# Total D Loss\n",
    "\t\td_loss = errD_real + (errD_fake + errD_mis) / 2.0 + errD_MAGP\n",
    "\n",
    "\t# Update D (Using Tape, not Optimizer directly)\n",
    "\td_vars = discriminator.trainable_variables + net_c.trainable_variables\n",
    "\td_grads = d_tape.gradient(d_loss, d_vars)\n",
    "\td_optimizer.apply_gradients(zip(d_grads, d_vars))\n",
    "\t\n",
    "\t# ====================\n",
    "\t# Train Generator\n",
    "\t# ====================\n",
    "\twith tf.GradientTape() as g_tape:\n",
    "\t\tfake_images_g = generator([noise, sent_emb], training=True)\n",
    "\t\tCLIP_fake_g, fake_emb_g = image_encoder(fake_images_g)\n",
    "\t\t\n",
    "\t\tfake_feats_g = discriminator(CLIP_fake_g, training=True)\n",
    "\t\toutput = net_c(fake_feats_g, sent_emb, training=True)\n",
    "\t\tg_adv_loss = -tf.reduce_mean(output)\n",
    "\t\t\n",
    "\t\t# CLIP Similarity\n",
    "\t\tfake_norm = tf.nn.l2_normalize(fake_emb_g, axis=1)\n",
    "\t\tsent_norm = tf.nn.l2_normalize(sent_emb, axis=1)\n",
    "\t\ttext_img_sim = tf.reduce_mean(tf.reduce_sum(fake_norm * sent_norm, axis=1))\n",
    "\t\t\n",
    "\t\tg_loss = g_adv_loss - (sim_w * text_img_sim)\n",
    "\t\n",
    "\t# Update G\n",
    "\tg_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "\tg_optimizer.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "\t\n",
    "\t# Return EXACTLY what your loop expects\n",
    "\treturn {\n",
    "\t\t'd_loss': d_loss,\n",
    "\t\t'g_loss': g_loss,\n",
    "\t\t'errD_real': errD_real,\n",
    "\t\t'errD_fake': errD_fake,\n",
    "\t\t'errD_mis': errD_mis,\n",
    "\t\t'errD_MAGP': errD_MAGP,\n",
    "\t\t'text_img_sim': text_img_sim,\n",
    "\t\t'acc_sim': text_img_sim  # Alias for compatibility if your loop uses it\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import TFCLIPModel, CLIPTokenizer\n",
    "\n",
    "def train(args, word2Id_dict, id2word_dict):\n",
    "\t\t\"\"\"\n",
    "\t\t100% Faithful TensorFlow replication of PyTorch GALIP training loop.\n",
    "\t\tOPTIMIZED: Uses 'Cache & Freeze' strategy for 20x speedup.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\t# ==========================================================================\n",
    "\t\t# 1. Initialization & Logging Setup\n",
    "\t\t# ==========================================================================\n",
    "\t\tprint(f\"--- Initializing Models (Image Size: {args['IMAGE_SIZE']}) ---\")\n",
    "\t\t\n",
    "\t\t# Load CLIP Models\n",
    "\t\tprint(\"--- Loading CLIP Models ---\")\n",
    "\t\ttry:\n",
    "\t\t\t\tclip_model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\t\t\t\ttokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\") # Added Tokenizer loading\n",
    "\t\t\t\tprint(\"‚úì CLIP Model & Tokenizer loaded\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"‚ö† Error loading CLIP model: {e}\")\n",
    "\t\t\t\treturn\n",
    "\n",
    "\t\t# Initialize Encoders\n",
    "\t\ttext_encoder = CLIP_Text_Encoder(clip_model)\n",
    "\t\timage_encoder = CLIP_Image_Encoder(clip_model)\n",
    "\t\t\n",
    "\t\t# Initialize GAN Models\n",
    "\t\tgenerator = NetG(ngf=args['NGF'], nz=args['Z_DIM'], cond_dim=args['EMBED_DIM'], clip_model=clip_model)\n",
    "\t\tdiscriminator = NetD(ndf=args['NDF'])\n",
    "\t\tnet_c = NetC(ndf=args['NDF'], cond_dim=args['EMBED_DIM'])\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t# Optimizers\n",
    "\t\tg_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_G'], beta_1=0.0, beta_2=0.9, epsilon=ADAM_EPSILON)\n",
    "\t\td_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_D'], beta_1=0.0, beta_2=0.9, epsilon=ADAM_EPSILON)\n",
    "\n",
    "\t\t# Checkpoints\n",
    "\t\tcheckpoint_dir = os.path.join(args['RUN_DIR'], 'checkpoints')\n",
    "\t\tcheckpoint = tf.train.Checkpoint(\n",
    "\t\t\t\tgenerator=generator, discriminator=discriminator, net_c=net_c,\n",
    "\t\t\t\tg_optimizer=g_optimizer, d_optimizer=d_optimizer\n",
    "\t\t)\n",
    "\t\tmanager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "\n",
    "\t\t# TensorBoard Setup\n",
    "\t\tlog_dir = os.path.join(args['RUN_DIR'], 'logs')\n",
    "\t\tsummary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\t\ttensorboard_process = subprocess.Popen(\n",
    "\t\t\t\t\t\t[sys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir]\n",
    "\t\t\t\t)\n",
    "\t\t\t\tprint(f\"‚úì TensorBoard launched (PID: {tensorboard_process.pid})\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"‚ö† Could not launch TensorBoard: {e}\")\n",
    "\n",
    "\t\t# ==========================================================================\n",
    "\t\t# 2. OPTIMIZATION: Pre-computation & Fast Dataset\n",
    "\t\t# ==========================================================================\n",
    "\t\t# Instead of using the slow 'dataset' passed in, we create a fast one here.\n",
    "\t\t\n",
    "\t\t# Path to your dataframe (Constructed from args)\n",
    "\t\tdf_path = os.path.join(args['DATA_PATH'], 'text2ImgData.pkl')\n",
    "\t\t\n",
    "\t\t# Run the cache logic (Defined in Cell 5)\n",
    "\t\tcached_img_feats, cached_ids, cached_masks, cached_indices = precompute_data(\n",
    "\t\t\t\tdf_path, image_encoder, tokenizer, word2Id_dict, id2word_dict\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Create the Zero-Latency Dataset\n",
    "\t\tfast_dataset = fast_dataset_generator(\n",
    "\t\t\t\tcached_img_feats, cached_ids, cached_masks, cached_indices, args['BATCH_SIZE']\n",
    "\t\t)\n",
    "\n",
    "\t\t# ==========================================================================\n",
    "\t\t# 3. Training Loop Setup\n",
    "\t\t# ==========================================================================\n",
    "\t\t\n",
    "\t\t# Fixed noise for visualization\n",
    "\t\tfixed_noise = tf.random.normal([8, args['Z_DIM']])\n",
    "\t\t\n",
    "\t\t# Get fixed text from first batch of FAST dataset\n",
    "\t\t# Note: fast_dataset yields (img_features, input_ids, mask)\n",
    "\t\tfor _, fixed_input_ids, fixed_mask in fast_dataset.take(1):\n",
    "\t\t\t\tfixed_input_ids = fixed_input_ids[:8]\n",
    "\t\t\t\tfixed_mask = fixed_mask[:8]\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tstart_epoch = 0\n",
    "\t\tif manager.latest_checkpoint:\n",
    "\t\t\t\tcheckpoint.restore(manager.latest_checkpoint)\n",
    "\t\t\t\tprint(f\"Restored from {manager.latest_checkpoint}\")\n",
    "\t\t\t\t\n",
    "\t\tprint(f\"Starting training for {args['MAX_EPOCH']} epochs...\")\n",
    "\t\t\n",
    "\t\t# ==========================================================================\n",
    "\t\t# 4. Training Loop\n",
    "\t\t# ==========================================================================\n",
    "\t\tfor epoch in range(start_epoch, args['MAX_EPOCH']):\n",
    "\t\t\t\tstart_time = time.time()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Progress bar over FAST dataset\n",
    "\t\t\t\tpbar = tqdm(fast_dataset, desc=f\"Epoch {epoch+1}/{args['MAX_EPOCH']}\")\n",
    "\t\t\t\t\n",
    "\t\t\t\td_losses = []\n",
    "\t\t\t\tg_losses = []\n",
    "\t\t\t\t\n",
    "\t\t\t\t# UPDATED UNPACKING: Accepts pre-computed features!\n",
    "\t\t\t\tfor step, (real_img_feats, input_ids, attention_mask) in enumerate(pbar):\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Use the FAST train step\n",
    "\t\t\t\t\t\tlosses = train_step_fast(\n",
    "\t\t\t\t\t\t\t\treal_img_feats, # Passing VRAM-cached features\n",
    "\t\t\t\t\t\t\t\tinput_ids, \n",
    "\t\t\t\t\t\t\t\tattention_mask, \n",
    "\t\t\t\t\t\t\t\tgenerator, \n",
    "\t\t\t\t\t\t\t\tdiscriminator, \n",
    "\t\t\t\t\t\t\t\tnet_c, \n",
    "\t\t\t\t\t\t\t\ttext_encoder, \n",
    "\t\t\t\t\t\t\t\timage_encoder,\n",
    "\t\t\t\t\t\t\t\tg_optimizer, \n",
    "\t\t\t\t\t\t\t\td_optimizer, \n",
    "\t\t\t\t\t\t\t\targs['BATCH_SIZE'], \n",
    "\t\t\t\t\t\t\t\targs['Z_DIM'],\n",
    "\t\t\t\t\t\t\t\tsim_w=args.get('SIM_W', 4.0) # Default to 4.0\n",
    "\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Map keys for logging (handling renaming from train_step_fast)\n",
    "\t\t\t\t\t\td_loss_val = float(losses['d_loss'])\n",
    "\t\t\t\t\t\tg_loss_val = float(losses['g_loss'])\n",
    "\t\t\t\t\t\tclip_sim_val = float(losses['acc_sim']) # Mapped from 'acc_sim'\n",
    "\t\t\t\t\t\tmagp_val = float(losses['errD_MAGP'])\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\td_losses.append(d_loss_val)\n",
    "\t\t\t\t\t\tg_losses.append(g_loss_val)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Update pbar\n",
    "\t\t\t\t\t\tpbar.set_postfix({\n",
    "\t\t\t\t\t\t\t\t'D': f\"{d_loss_val:.4f}\", \n",
    "\t\t\t\t\t\t\t\t'G': f\"{g_loss_val:.4f}\",\n",
    "\t\t\t\t\t\t\t\t'MAGP': f\"{magp_val:.4f}\",\n",
    "\t\t\t\t\t\t\t\t'CLIP': f\"{clip_sim_val:.4f}\"\n",
    "\t\t\t\t\t\t})\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Log to TensorBoard\n",
    "\t\t\t\t\t\twith summary_writer.as_default():\n",
    "\t\t\t\t\t\t\t\tstep_global = epoch * len(cached_indices) // args['BATCH_SIZE'] + step\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Loss/D_total', d_loss_val, step=step_global)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Loss/G_total', g_loss_val, step=step_global)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Loss/MA_GP', magp_val, step=step_global)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Loss/CLIP_sim', clip_sim_val, step=step_global)\n",
    "\n",
    "\t\t\t\t# End of Epoch\n",
    "\t\t\t\tavg_d_loss = np.mean(d_losses)\n",
    "\t\t\t\tavg_g_loss = np.mean(g_losses)\n",
    "\t\t\t\tprint(f\"Epoch {epoch+1} done. D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}, Time: {time.time()-start_time:.1f}s\")\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Save Checkpoint\n",
    "\t\t\t\tif (epoch + 1) % args['SAVE_FREQ'] == 0:\n",
    "\t\t\t\t\t\tsave_path = manager.save()\n",
    "\t\t\t\t\t\tprint(f\"Saved checkpoint for epoch {epoch+1}: {save_path}\")\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t# Save Sample Images\n",
    "\t\t\t\tif (epoch + 1) % args['SAMPLE_FREQ'] == 0:\n",
    "\t\t\t\t\t\tsave_sample_images(generator, text_encoder, tokenizer, fixed_noise, epoch+1, os.path.join(args['RUN_DIR'], 'samples'))\n",
    "\n",
    "\t\tprint(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define configuration for training\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create a unique run directory\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"./runs/{run_id}\"\n",
    "if not os.path.exists(run_dir):\n",
    "\tos.makedirs(run_dir)\n",
    "\n",
    "# User provided config - 100% faithful to PyTorch GALIP\n",
    "config = {\n",
    "\t'IMAGE_SIZE': [64, 64, 3],\n",
    "\t'NGF': 64,                # nf in PyTorch\n",
    "\t'NDF': 64,                # nf in PyTorch  \n",
    "\t'Z_DIM': 100,             # z_dim in PyTorch\n",
    "\t'EMBED_DIM': 512,         # cond_dim in PyTorch (CLIP embedding dimension)\n",
    "\t'LR_G': 0.0001,           # lr_g in PyTorch\n",
    "\t'LR_D': 0.0004,           # lr_d in PyTorch\n",
    "\t'SIM_W': 4.0,             # sim_w in PyTorch (CLIP similarity loss weight) - GALIP default is 4.0\n",
    "\t'BATCH_SIZE': BATCH_SIZE,\n",
    "\t'MAX_EPOCH': 600,\n",
    "\t'RUN_DIR': run_dir,\n",
    "\t'SAVE_FREQ': 3,\n",
    "\t'SAMPLE_FREQ': 1,\n",
    "\t'DATA_PATH': data_path,\n",
    "\t'N_SAMPLE': num_training_sample if 'num_training_sample' in locals() else 7370\n",
    "}\n",
    "\n",
    "# Save config for reproducibility\n",
    "with open(os.path.join(run_dir, 'config.json'), 'w') as f:\n",
    "\t# Filter for JSON serializable values\n",
    "\tjson_config = {k: v for k, v in config.items() if isinstance(v, (int, float, str, list, bool))}\n",
    "\tjson.dump(json_config, f, indent=4)\n",
    "\n",
    "print(f\"Training Run Directory: {run_dir}\")\n",
    "print(f\"Config: {json.dumps(json_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the dictionaries directly\n",
    "train(config, word2Id_dict, id2word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¬∂</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¬∂</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string\n",
    "\t\t\t\tindex: Test sample ID\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tinput_ids, attention_mask, index\n",
    "\t\t\"\"\"\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing dataset generator - decodes IDs to raw text\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_pickle('./dataset/testData.pkl')\n",
    "\t\tcaptions_ids = data['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\tchosen_caption_ids = captions_ids[i]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\tindex = data['ID'].values\n",
    "\t\tindex = np.asarray(index)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "\t\tdataset = dataset.batch(batch_size)\n",
    "\t\t\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference directory inside the run directory\n",
    "inference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "if not os.path.exists(inference_dir):\n",
    "\tos.makedirs(inference_dir)\n",
    "print(f\"Inference Directory: {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, config):\n",
    "\tprint(\"--- Starting Inference (Corrected for GALIP) ---\")\n",
    "\t\n",
    "\t# 1. Re-initialize CLIP (Required for Tokenizer and Encoder)\n",
    "\tprint(\"Loading CLIP components...\")\n",
    "\ttry:\n",
    "\t\tfrom transformers import TFCLIPModel, CLIPTokenizer\n",
    "\t\tclip_model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\t\ttokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\t\t\n",
    "\t\t# Re-create the specific Encoder wrapper used in training\n",
    "\t\ttext_encoder = CLIP_Text_Encoder(clip_model)\n",
    "\t\tprint(\"‚úì CLIP Model & Encoder loaded\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"‚ö† Error loading CLIP: {e}\")\n",
    "\t\treturn\n",
    "\n",
    "\t# 2. Load Generator\n",
    "\tprint(\"Loading Generator...\")\n",
    "\t# Initialize with same args as training\n",
    "\tgenerator = NetG(ngf=config['NGF'], nz=config['Z_DIM'], cond_dim=config['EMBED_DIM'], clip_model=clip_model)\n",
    "\t\n",
    "\t# Run a dummy forward pass to initialize variables before loading weights\n",
    "\tdummy_noise = tf.random.normal([1, config['Z_DIM']])\n",
    "\tdummy_text = tf.random.normal([1, config['EMBED_DIM']])\n",
    "\t_ = generator([dummy_noise, dummy_text], training=False)\n",
    "\t\n",
    "\t# Restore Checkpoint\n",
    "\tcheckpoint_dir = os.path.join(config['RUN_DIR'], 'checkpoints')\n",
    "\t# Use expect_partial() because we are strictly loading the generator, \n",
    "\t# ignoring optimizer states or discriminator if they exist in the ckpt\n",
    "\tcheckpoint = tf.train.Checkpoint(generator=generator)\n",
    "\tlatest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\t\n",
    "\tif latest_ckpt:\n",
    "\t\tprint(f\"Loading weights from: {latest_ckpt}\")\n",
    "\t\tstatus = checkpoint.restore(latest_ckpt).expect_partial()\n",
    "\t\tstatus.assert_existing_objects_matched()\n",
    "\t\tprint(\"‚úì Generator weights loaded successfully\")\n",
    "\telse:\n",
    "\t\tprint(\"‚ö† NO CHECKPOINT FOUND! Generating with random weights.\")\n",
    "\n",
    "\t# 3. Inference Loop\n",
    "\tinference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "\tif not os.path.exists(inference_dir): os.makedirs(inference_dir)\n",
    "\t\t\n",
    "\ttotal_images = 0\n",
    "\t\n",
    "\t# Iterate over the testing dataset\n",
    "\t# Note: testing_dataset yields (caption_texts, ids)\n",
    "\tfor step, (caption_texts, image_ids) in enumerate(tqdm(dataset, desc='Generating')):\n",
    "\t\t\n",
    "\t\tbatch_size_curr = len(caption_texts)\n",
    "\t\t\n",
    "\t\t# --- A. Tokenize using CLIP (Not dictionary!) ---\n",
    "\t\t# Convert tensors to string list\n",
    "\t\ttext_list = [t.numpy().decode('utf-8') for t in caption_texts]\n",
    "\t\t\n",
    "\t\tenc = tokenizer(\n",
    "\t\t\ttext_list,\n",
    "\t\t\tpadding='max_length',\n",
    "\t\t\ttruncation=True,\n",
    "\t\t\tmax_length=77, # CLIP default\n",
    "\t\t\treturn_tensors='tf'\n",
    "\t\t)\n",
    "\t\tinput_ids = enc['input_ids']\n",
    "\t\tattention_mask = enc['attention_mask']\n",
    "\t\t\n",
    "\t\t# --- B. Encode Text using CLIP (Not RNN!) ---\n",
    "\t\tsent_emb, _ = text_encoder(input_ids, attention_mask=attention_mask)\n",
    "\t\t\n",
    "\t\t# --- C. Generate ---\n",
    "\t\tnoise = tf.random.normal([batch_size_curr, config['Z_DIM']])\n",
    "\t\tfake_imgs = generator([noise, sent_emb], training=False)\n",
    "\t\t\n",
    "\t\t# Post-process\n",
    "\t\tfake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "\t\tfake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "\t\t\n",
    "\t\t# Save\n",
    "\t\tfor i in range(batch_size_curr):\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# Handle ID decoding safely\n",
    "\t\t\t\timg_id_val = image_ids[i].numpy()\n",
    "\t\t\t\tif isinstance(img_id_val, bytes):\n",
    "\t\t\t\t\timg_id = img_id_val.decode('utf-8')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\timg_id = str(img_id_val)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tsave_path = os.path.join(inference_dir, f'inference_{img_id}.jpg')\n",
    "\t\t\t\tplt.imsave(save_path, fake_imgs[i])\n",
    "\t\t\t\ttotal_images += 1\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"Error saving image {i}: {e}\")\n",
    "\t\t\t\t\n",
    "\tprint(f\"Inference Complete. Saved {total_images} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "\t\tprint(f'‚ö† No images found in {inference_dir}')\n",
    "\t\tprint('Please run the inference cell first!')\n",
    "else:\n",
    "\t\t# Randomly sample 20 images\n",
    "\t\tnp.random.seed(42)  # For reproducibility\n",
    "\t\tnum_samples = min(20, len(image_files))\n",
    "\t\tsample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "\t\tsample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "\t\t# Create 4x5 grid\n",
    "\t\tfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\t\taxes = axes.flatten()\n",
    "\n",
    "\t\tfor idx, img_path in enumerate(sample_files):\n",
    "\t\t\t\t# Extract image ID from filename\n",
    "\t\t\t\timg_id = int(Path(img_path).stem.split('_')[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Find caption\n",
    "\t\t\t\tcaption_idx = np.where(test_ids == img_id)[0][0]\n",
    "\t\t\t\tcaption_ids = test_captions[caption_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode caption\n",
    "\t\t\t\tcaption_text = ''\n",
    "\t\t\t\tfor word_id in caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':\n",
    "\t\t\t\t\t\t\t\tcaption_text += word + ' '\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Load and display image\n",
    "\t\t\t\timg = plt.imread(img_path)\n",
    "\t\t\t\taxes[idx].imshow(img)\n",
    "\t\t\t\taxes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\t# Hide unused subplots if less than 20 images\n",
    "\t\tfor idx in range(num_samples, 20):\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\tprint(f'\\nTotal generated images: {len(image_files)}')\n",
    "\t\tprint(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-comp3)",
   "language": "python",
   "name": "dl-comp3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
