{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttry:\n",
    "\t\t# Restrict TensorFlow to only use the first GPU\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\t\t# Currently, memory growth needs to be the same across GPUs\n",
    "\t\tfor gpu in gpus:\n",
    "\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\n",
    "\t\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\t\tprint(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\texcept RuntimeError as e:\n",
    "\t\t# Memory growth must be set before GPUs have been initialized\n",
    "\t\tprint(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¬∂</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using CLIP tokenizer (sent2IdList removed)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úì Using CLIP tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¬∂</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lee_eason/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CLIP Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. DATASET GENERATOR (Adapted for CLIP)\n",
    "# ==============================================================================\n",
    "\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAX_SEQ_LENGTH = 77 # CLIP default\n",
    "\n",
    "# Initialize CLIP Tokenizer\n",
    "try:\n",
    "    from transformers import CLIPTokenizer\n",
    "    # Use the same model name as the vision/text models we will load later\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    print(\"‚úì CLIP Tokenizer loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading CLIP Tokenizer: {e}\")\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "    \"\"\"\n",
    "    Data generator using CLIP Tokenizer\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string\n",
    "        image_path: Path to image file\n",
    "    \n",
    "    Returns:\n",
    "        img, input_ids, attention_mask\n",
    "    \"\"\"\n",
    "    # ============= IMAGE PROCESSING =============\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    # Normalize to [-1, 1] to match generator's tanh output\n",
    "    img = (img * 2.0) - 1.0\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    # ============= TEXT PROCESSING =============\n",
    "    # Tokenize using CLIP\n",
    "    # We use py_function because tokenizer is Python code\n",
    "    def tokenize(text):\n",
    "        text_str = text.numpy().decode('utf-8')\n",
    "        # CLIP Tokenizer handles padding and truncation\n",
    "        enc = tokenizer(\n",
    "            text_str, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=MAX_SEQ_LENGTH, \n",
    "            return_tensors='np'\n",
    "        )\n",
    "        return enc['input_ids'][0], enc['attention_mask'][0]\n",
    "        \n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize, \n",
    "        inp=[caption_text], \n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    input_ids.set_shape([MAX_SEQ_LENGTH])\n",
    "    attention_mask.set_shape([MAX_SEQ_LENGTH])\n",
    "    \n",
    "    return img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator, word2Id_dict, id2word_dict, expand_captions=True):\n",
    "    \"\"\"\n",
    "    Dataset generator that decodes IDs to text for CLIP\n",
    "    \"\"\"\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions_ids = df['Captions'].values\n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    print(f\"Loading dataset from {filenames}...\")\n",
    "    \n",
    "    # Helper to decode IDs to text\n",
    "    def decode_ids(id_list):\n",
    "        words = []\n",
    "        for i in id_list:\n",
    "            word = id2word_dict.get(str(i), '')\n",
    "            if word and word != '<PAD>':\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    all_captions_text = []\n",
    "    all_paths = []\n",
    "\n",
    "    if expand_captions:\n",
    "        # Expand: Create a sample for every caption\n",
    "        print(\"Expanding captions (one sample per caption)...\")\n",
    "        for caps, path in zip(captions_ids, image_paths):\n",
    "            for cap_ids in caps:\n",
    "                text = decode_ids(cap_ids)\n",
    "                all_captions_text.append(text)\n",
    "                all_paths.append(path)\n",
    "    else:\n",
    "        # Random Select: Pick one random caption per image (static for this generator call)\n",
    "        # Note: Ideally we'd do random selection at runtime, but decoding text in graph is hard.\n",
    "        # For simplicity/performance, we pick one now. \n",
    "        # To get true randomness per epoch, we'd need to re-create the dataset or use py_function logic.\n",
    "        print(\"Selecting one random caption per image...\")\n",
    "        for caps, path in zip(captions_ids, image_paths):\n",
    "            cap_ids = random.choice(caps)\n",
    "            text = decode_ids(cap_ids)\n",
    "            all_captions_text.append(text)\n",
    "            all_paths.append(path)\n",
    "            \n",
    "    all_captions_text = np.array(all_captions_text)\n",
    "    all_paths = np.array(all_paths)\n",
    "    \n",
    "    print(f\"Dataset size: {len(all_captions_text)} samples\")\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_captions_text, all_paths))\n",
    "    dataset = dataset.shuffle(len(all_captions_text))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./dataset/text2ImgData.pkl...\n",
      "Expanding captions (one sample per caption)...\n",
      "Dataset size: 70504 samples\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "# We use expand_captions=False to keep epoch size manageable (same as number of images)\n",
    "# or True for more training data. Let's use False for faster epochs initially, or True for better quality.\n",
    "# Given the small dataset (7k images), expanding is probably better (70k samples).\n",
    "dataset = dataset_generator(\n",
    "    data_path + '/text2ImgData.pkl', \n",
    "    BATCH_SIZE, \n",
    "    training_data_generator,\n",
    "    word2Id_dict,\n",
    "    id2word_dict,\n",
    "    expand_captions=True \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¬∂</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¬∂</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Transformers Version: 4.57.3\n",
      "PyTorch-compatible settings:\n",
      "  ADAM_EPSILON = 1e-08\n",
      "  LAYER_NORM_EPSILON = 1e-05\n",
      "  Weight init: he_uniform (Kaiming Uniform)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "from transformers import TFCLIPVisionModel, TFCLIPTextModel, CLIPProcessor, CLIPConfig\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"Transformers Version:\", transformers.__version__)\n",
    "except ImportError:\n",
    "    print(\"Transformers not installed. Please install it.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PYTORCH-TENSORFLOW COMPATIBILITY CONSTANTS\n",
    "# ==============================================================================\n",
    "# These constants ensure numerical equivalence between PyTorch and TensorFlow\n",
    "# implementations of GALIP.\n",
    "\n",
    "# 1. Optimizer epsilon: PyTorch Adam default is 1e-8, TensorFlow default is 1e-7\n",
    "#    Using 1e-7 can cause subtle numerical divergence over training.\n",
    "ADAM_EPSILON = 1e-8  # Match PyTorch default\n",
    "\n",
    "# 2. LayerNorm epsilon: PyTorch default is 1e-5, TensorFlow default is 1e-3\n",
    "#    This affects CLIP and any custom LayerNorm layers.\n",
    "LAYER_NORM_EPSILON = 1e-5  # Match PyTorch default\n",
    "\n",
    "# 3. Weight initialization: PyTorch Linear/Conv2d use Kaiming Uniform (He)\n",
    "#    TensorFlow defaults to Glorot Uniform (Xavier).\n",
    "#    All our layers now use kernel_initializer='he_uniform' explicitly.\n",
    "\n",
    "print(f\"PyTorch-compatible settings:\")\n",
    "print(f\"  ADAM_EPSILON = {ADAM_EPSILON}\")\n",
    "print(f\"  LAYER_NORM_EPSILON = {LAYER_NORM_EPSILON}\")\n",
    "print(f\"  Weight init: he_uniform (Kaiming Uniform)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# HELPER FUNCTION: Create PyTorch-compatible Adam optimizer\n",
    "# ==============================================================================\n",
    "def create_pytorch_compatible_adam(learning_rate, beta_1=0.0, beta_2=0.9):\n",
    "    \"\"\"\n",
    "    Creates an Adam optimizer with PyTorch-equivalent settings.\n",
    "    \n",
    "    PyTorch defaults:\n",
    "        - lr: required\n",
    "        - betas: (0.9, 0.999) but GALIP uses (0.0, 0.9)\n",
    "        - eps: 1e-8\n",
    "        - weight_decay: 0\n",
    "        - amsgrad: False\n",
    "    \n",
    "    TensorFlow defaults that differ:\n",
    "        - epsilon: 1e-7 (10x larger than PyTorch!)\n",
    "    \n",
    "    Args:\n",
    "        learning_rate: Learning rate\n",
    "        beta_1: First moment decay (default 0.0 for GAN training)\n",
    "        beta_2: Second moment decay (default 0.9 for GAN training)\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.optimizers.Adam with PyTorch-equivalent settings\n",
    "    \"\"\"\n",
    "    return tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=beta_1,\n",
    "        beta_2=beta_2,\n",
    "        epsilon=ADAM_EPSILON  # CRITICAL: Match PyTorch 1e-8\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç Deep Inspection of openai/clip-vit-base-patch32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded successfully.\n",
      "\n",
      "[Checking visual_projection]\n",
      "  ‚úì Found via: Inside 'clip' Wrapper\n",
      "  ‚úì Layer Type: Dense\n",
      "  ‚Ä¢ Weight tensors found: 1\n",
      "  ‚úì PASS: Only kernel found. Bias=False. (100% PyTorch Faithful)\n",
      "\n",
      "[Checking text_projection]\n",
      "  ‚úì Found via: Inside 'clip' Wrapper\n",
      "  ‚úì Layer Type: Dense\n",
      "  ‚Ä¢ Weight tensors found: 1\n",
      "  ‚úì PASS: Only kernel found. Bias=False. (100% PyTorch Faithful)\n",
      "\n",
      "[LayerNorm Epsilon Analysis]\n",
      "  ‚ö† Could not locate vision_model.pre_layrnorm\n",
      "  Text LN epsilon:   1e-05\n",
      "  ‚úì Text Epsilon matches PyTorch (1e-5).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFCLIPModel, CLIPConfig\n",
    "\n",
    "def robust_verify_clip(model_name=\"openai/clip-vit-base-patch32\"):\n",
    "    print(f\"--- üîç Deep Inspection of {model_name} ---\")\n",
    "    \n",
    "    # 1. Load Model\n",
    "    try:\n",
    "        model = TFCLIPModel.from_pretrained(model_name)\n",
    "        print(\"‚úì Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CRITICAL ERROR loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Helper to find layers recursively (Handles the 'clip' wrapper)\n",
    "    def find_layer_recursive(model, layer_name):\n",
    "        # 1. Check top-level attributes\n",
    "        if hasattr(model, layer_name):\n",
    "            return getattr(model, layer_name), \"Top-level Attribute\"\n",
    "            \n",
    "        # 2. Check direct children layers\n",
    "        for layer in model.layers:\n",
    "            if layer.name == layer_name:\n",
    "                return layer, \"Direct Layer List\"\n",
    "                \n",
    "        # 3. CRITICAL: Check inside 'clip' wrapper if it exists\n",
    "        # This fixes the specific error you are seeing\n",
    "        if hasattr(model, 'clip'):\n",
    "            clip_layer = model.clip\n",
    "            if hasattr(clip_layer, layer_name):\n",
    "                return getattr(clip_layer, layer_name), \"Inside 'clip' Wrapper\"\n",
    "        \n",
    "        # 4. Check inside any layer named 'clip' in the layers list\n",
    "        for layer in model.layers:\n",
    "            if layer.name == 'clip':\n",
    "                if hasattr(layer, layer_name):\n",
    "                    return getattr(layer, layer_name), \"Inside 'clip' Layer (List)\"\n",
    "                    \n",
    "        return None, \"NOT FOUND\"\n",
    "\n",
    "    # 2. Inspect Projections (Visual & Text)\n",
    "    # ---------------------------------------------------------\n",
    "    for proj_name in [\"visual_projection\", \"text_projection\"]:\n",
    "        print(f\"\\n[Checking {proj_name}]\")\n",
    "        layer, source = find_layer_recursive(model, proj_name)\n",
    "        \n",
    "        if layer is None:\n",
    "            print(f\"  ‚ùå FAIL: Layer '{proj_name}' DOES NOT EXIST.\")\n",
    "            print(f\"     Top-level layers: {[l.name for l in model.layers]}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  ‚úì Found via: {source}\")\n",
    "        print(f\"  ‚úì Layer Type: {type(layer).__name__}\")\n",
    "        \n",
    "        # Check Bias\n",
    "        weights = layer.weights\n",
    "        print(f\"  ‚Ä¢ Weight tensors found: {len(weights)}\")\n",
    "        \n",
    "        if len(weights) == 1:\n",
    "            print(\"  ‚úì PASS: Only kernel found. Bias=False. (100% PyTorch Faithful)\")\n",
    "        elif len(weights) == 2:\n",
    "            bias_tensor = weights[1]\n",
    "            bias_sum = tf.reduce_sum(tf.abs(bias_tensor)).numpy()\n",
    "            print(f\"  ‚ö† WARNING: Bias vector exists!\")\n",
    "            print(f\"    Sum of absolute values: {bias_sum}\")\n",
    "            \n",
    "            if bias_sum < 1e-9:\n",
    "                print(\"    ‚úì PASS (Soft): Bias exists but is effectively ZERO.\")\n",
    "            else:\n",
    "                print(\"    ‚ùå FAIL: Non-zero bias found! Divergence risk.\")\n",
    "\n",
    "    # 3. LayerNorm Epsilon Check\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[LayerNorm Epsilon Analysis]\")\n",
    "    \n",
    "    # We use the same finder for the sub-models\n",
    "    vision_model, _ = find_layer_recursive(model, \"vision_model\")\n",
    "    text_model, _ = find_layer_recursive(model, \"text_model\")\n",
    "    \n",
    "    # Check Vision Pre-LayerNorm\n",
    "    if vision_model and hasattr(vision_model, \"pre_layrnorm\"):\n",
    "        eps = vision_model.pre_layrnorm.epsilon\n",
    "        print(f\"  Vision LN epsilon: {eps}\")\n",
    "        if abs(eps - 1e-5) < 1e-9:\n",
    "            print(\"  ‚úì Vision Epsilon matches PyTorch (1e-5).\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå FAIL: Vision Epsilon mismatch! Found {eps}.\")\n",
    "    else:\n",
    "        print(\"  ‚ö† Could not locate vision_model.pre_layrnorm\")\n",
    "\n",
    "    # Check Text Final-LayerNorm\n",
    "    if text_model and hasattr(text_model, \"final_layer_norm\"):\n",
    "        eps = text_model.final_layer_norm.epsilon\n",
    "        print(f\"  Text LN epsilon:   {eps}\")\n",
    "        if abs(eps - 1e-5) < 1e-9:\n",
    "            print(\"  ‚úì Text Epsilon matches PyTorch (1e-5).\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå FAIL: Text Epsilon mismatch! Found {eps}.\")\n",
    "    else:\n",
    "         print(\"  ‚ö† Could not locate text_model.final_layer_norm\")\n",
    "\n",
    "# Run the verification\n",
    "robust_verify_clip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. BASIC BLOCKS (DF-GAN & GALIP Components)\n",
    "# ==============================================================================\n",
    "\n",
    "class Affine(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's Affine layer.\n",
    "    \n",
    "    PyTorch signature: Affine(cond_dim, num_features)\n",
    "    \n",
    "    PyTorch structure:\n",
    "        fc_gamma: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        fc_beta:  Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "    \n",
    "    Initialization:\n",
    "        fc_gamma.linear2: weight=0, bias=1 (so initial gamma=1, identity scaling)\n",
    "        fc_beta.linear2:  weight=0, bias=0 (so initial beta=0, no shift)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, num_features):\n",
    "        super(Affine, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # fc_gamma: 2-layer MLP\n",
    "        # PyTorch: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        # First layer: cond_dim -> num_features, he_uniform init (matches PyTorch Linear default)\n",
    "        # Second layer: num_features -> num_features, zeros weight, ones bias\n",
    "        self.gamma_linear1 = layers.Dense(num_features, kernel_initializer='he_uniform')\n",
    "        self.gamma_linear2 = layers.Dense(\n",
    "            num_features, \n",
    "            kernel_initializer='zeros',\n",
    "            bias_initializer='ones'\n",
    "        )\n",
    "        \n",
    "        # fc_beta: 2-layer MLP\n",
    "        # PyTorch: Linear(cond_dim, num_features) -> ReLU -> Linear(num_features, num_features)\n",
    "        # First layer: cond_dim -> num_features, he_uniform init (matches PyTorch Linear default)\n",
    "        # Second layer: num_features -> num_features, zeros weight, zeros bias\n",
    "        self.beta_linear1 = layers.Dense(num_features, kernel_initializer='he_uniform')\n",
    "        self.beta_linear2 = layers.Dense(\n",
    "            num_features,\n",
    "            kernel_initializer='zeros',\n",
    "            bias_initializer='zeros'\n",
    "        )\n",
    "\n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, H, W, C] feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "        \"\"\"\n",
    "        # Compute gamma (scale)\n",
    "        gamma = self.gamma_linear1(y)\n",
    "        gamma = tf.nn.relu(gamma)\n",
    "        gamma = self.gamma_linear2(gamma)  # [B, num_features]\n",
    "        \n",
    "        # Compute beta (shift)\n",
    "        beta = self.beta_linear1(y)\n",
    "        beta = tf.nn.relu(beta)\n",
    "        beta = self.beta_linear2(beta)  # [B, num_features]\n",
    "        \n",
    "        # Reshape for broadcasting: [B, 1, 1, C]\n",
    "        gamma = tf.reshape(gamma, [-1, 1, 1, self.num_features])\n",
    "        beta = tf.reshape(beta, [-1, 1, 1, self.num_features])\n",
    "        \n",
    "        return gamma * x + beta\n",
    "\n",
    "\n",
    "class DFBLK(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's DFBLK.\n",
    "    \n",
    "    PyTorch signature: DFBLK(cond_dim, in_ch)\n",
    "    \n",
    "    Structure:\n",
    "        affine0 -> LeakyReLU(0.2) -> affine1 -> LeakyReLU(0.2)\n",
    "    \n",
    "    NO convolutions - just two affine transforms with activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch):\n",
    "        super(DFBLK, self).__init__()\n",
    "        # PyTorch: self.affine0 = Affine(cond_dim, in_ch)\n",
    "        # Pass cond_dim to match PyTorch signature exactly\n",
    "        self.affine0 = Affine(cond_dim, in_ch)\n",
    "        self.affine1 = Affine(cond_dim, in_ch)\n",
    "\n",
    "    def call(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, H, W, C] feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "        Returns:\n",
    "            [B, H, W, C] transformed feature map\n",
    "        \"\"\"\n",
    "        h = self.affine0(x, y)\n",
    "        h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "        h = self.affine1(h, y)\n",
    "        h = tf.nn.leaky_relu(h, alpha=0.2)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "class G_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's G_Block.\n",
    "    \n",
    "    PyTorch signature: G_Block(cond_dim, in_ch, out_ch, imsize)\n",
    "    \n",
    "    Structure:\n",
    "        1. Interpolate to target size\n",
    "        2. Residual path: fuse1(DFBLK) -> c1(conv) -> fuse2(DFBLK) -> c2(conv)\n",
    "        3. Shortcut path: c_sc(1x1 conv) if in_ch != out_ch\n",
    "        4. Output: shortcut + residual\n",
    "    \n",
    "    Note: imsize is handled dynamically via target_size parameter in call().\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch, out_ch):\n",
    "        super(G_Block, self).__init__()\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        \n",
    "        # PyTorch: nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.c1 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        self.c2 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        \n",
    "        # PyTorch: DFBLK(cond_dim, in_ch) and DFBLK(cond_dim, out_ch)\n",
    "        self.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        \n",
    "        # Shortcut: 1x1 conv only if channel dimensions change\n",
    "        # PyTorch: nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, h, y, target_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: [B, H, W, in_ch] input feature map\n",
    "            y: [B, cond_dim] conditioning vector\n",
    "            target_size: int, target spatial size for interpolation\n",
    "        Returns:\n",
    "            [B, target_size, target_size, out_ch] output feature map\n",
    "        \"\"\"\n",
    "        # PyTorch: h = F.interpolate(h, size=(self.imsize, self.imsize))\n",
    "        h = tf.image.resize(h, [target_size, target_size], method='nearest')\n",
    "        \n",
    "        # Residual path: fuse1 -> c1 -> fuse2 -> c2\n",
    "        # PyTorch: h = self.fuse1(h, y); h = self.c1(h); h = self.fuse2(h, y); h = self.c2(h)\n",
    "        res = self.fuse1(h, y)\n",
    "        res = self.c1(res)\n",
    "        res = self.fuse2(res, y)\n",
    "        res = self.c2(res)\n",
    "        \n",
    "        # Shortcut path\n",
    "        if self.learnable_sc:\n",
    "            sc = self.c_sc(h)\n",
    "        else:\n",
    "            sc = h\n",
    "            \n",
    "        return sc + res\n",
    "\n",
    "\n",
    "class D_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's D_Block.\n",
    "    \n",
    "    PyTorch signature: D_Block(fin, fout, k, s, p, res, CLIP_feat)\n",
    "    \n",
    "    PyTorch structure:\n",
    "        conv_r: Conv2D(fin, fout, k, s, p, bias=False) -> LeakyReLU(0.2) -> Conv2D(fout, fout, k, s, p, bias=False) -> LeakyReLU(0.2)\n",
    "        conv_s: Conv2D(fin, fout, 1, stride=1, padding=0) for shortcut\n",
    "        gamma: learnable scalar for residual (init=0)\n",
    "        beta: learnable scalar for CLIP features (init=0)\n",
    "    \n",
    "    Note: All PyTorch D_Block instantiations use k=3, s=1, p=1, so we hardcode these.\n",
    "    \"\"\"\n",
    "    def __init__(self, fin, fout, is_down=False, is_res=True, clip_feat=False):\n",
    "        super(D_Block, self).__init__()\n",
    "        self.is_res = is_res\n",
    "        self.clip_feat = clip_feat\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        \n",
    "        # Main conv path (PyTorch: k=3, s=1, p=1)\n",
    "        # CRITICAL: kernel_initializer='he_uniform' to match PyTorch Conv2d default\n",
    "        self.conv_r1 = layers.Conv2D(fout, 3, padding='same', use_bias=False, kernel_initializer='he_uniform')\n",
    "        self.conv_r2 = layers.Conv2D(fout, 3, padding='same', use_bias=False, kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Shortcut conv (PyTorch: 1x1, stride=1, padding=0)\n",
    "        # CRITICAL: padding='valid' to match PyTorch padding=0\n",
    "        self.conv_s = layers.Conv2D(fout, 1, padding='valid', kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Learnable scalars (initialized to 0, matching PyTorch torch.zeros(1))\n",
    "        if is_res:\n",
    "            self.gamma = tf.Variable(0.0, trainable=True, name='gamma')\n",
    "        if clip_feat:\n",
    "            self.beta = tf.Variable(0.0, trainable=True, name='beta')\n",
    "\n",
    "    def call(self, x, clip_f=None):\n",
    "        # Residual path\n",
    "        res = self.conv_r1(x)\n",
    "        res = tf.nn.leaky_relu(res, alpha=0.2)\n",
    "        res = self.conv_r2(res)\n",
    "        res = tf.nn.leaky_relu(res, alpha=0.2)\n",
    "        \n",
    "        # Shortcut\n",
    "        if self.learned_shortcut:\n",
    "            x = self.conv_s(x)\n",
    "        \n",
    "        # Combine based on flags\n",
    "        out = x\n",
    "        if self.is_res:\n",
    "\n",
    "            out = out + self.gamma * res     \n",
    "\n",
    "        if self.clip_feat and clip_f is not None:            \n",
    "            out = out + self.beta * clip_f\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CLIP ADAPTER (Robust Fix)\n",
    "# ==============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_layer_safe(model, layer_name):\n",
    "    \"\"\"\n",
    "    Robustly find a layer in a Keras model, handling the 'clip' wrapper case.\n",
    "    \"\"\"\n",
    "    # 1. Try direct access\n",
    "    if hasattr(model, layer_name):\n",
    "        return getattr(model, layer_name)\n",
    "    \n",
    "    # 2. Try inside 'clip' wrapper\n",
    "    if hasattr(model, 'clip') and hasattr(model.clip, layer_name):\n",
    "        return getattr(model.clip, layer_name)\n",
    "        \n",
    "    # 3. Search layer list (Fallback)\n",
    "    for layer in model.layers:\n",
    "        if layer.name == layer_name:\n",
    "            return layer\n",
    "        if layer.name == 'clip':\n",
    "             if hasattr(layer, layer_name):\n",
    "                 return getattr(layer, layer_name)\n",
    "                 \n",
    "    raise AttributeError(f\"Could not find '{layer_name}' in TFCLIPModel. Available: {[l.name for l in model.layers]}\")\n",
    "\n",
    "# Helper to find sub-attributes like pre_layrnorm inside vision_model\n",
    "def get_sublayer_safe(model, possible_names):\n",
    "    for name in possible_names:\n",
    "        if hasattr(model, name):\n",
    "            return getattr(model, name)\n",
    "    # If not found as attribute, check layers list\n",
    "    for layer in model.layers:\n",
    "        if layer.name in possible_names:\n",
    "            return layer\n",
    "    raise AttributeError(f\"Could not find any of {possible_names} in model. Available: {[l.name for l in model.layers]}\")\n",
    "\n",
    "\n",
    "class M_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's M_Block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, cond_dim, k, s, p):\n",
    "        super(M_Block, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(mid_ch, k, strides=s, padding='same', kernel_initializer='he_uniform')\n",
    "        self.fuse1 = DFBLK(cond_dim, mid_ch)\n",
    "        self.conv2 = layers.Conv2D(out_ch, k, strides=s, padding='same', kernel_initializer='he_uniform')\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, h, c):\n",
    "        res = self.conv1(h)\n",
    "        res = self.fuse1(res, c)\n",
    "        res = self.conv2(res)\n",
    "        res = self.fuse2(res, c)\n",
    "        if self.learnable_sc:\n",
    "            sc = self.c_sc(h)\n",
    "        else:\n",
    "            sc = h\n",
    "        return sc + res\n",
    "\n",
    "\n",
    "class CLIP_Mapper(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's CLIP_Mapper.\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model):\n",
    "        super(CLIP_Mapper, self).__init__()\n",
    "        \n",
    "        self.vision_model = get_layer_safe(clip_model, 'vision_model')\n",
    "        \n",
    "        # FIX: Find sub-layers robustly\n",
    "        self.embeddings = get_sublayer_safe(self.vision_model, ['embeddings'])\n",
    "        self.pre_layrnorm = get_sublayer_safe(self.vision_model, ['pre_layrnorm', 'pre_layernorm', 'layernorm_pre'])\n",
    "        # encoder is standard, usually .encoder\n",
    "        self.encoder = get_sublayer_safe(self.vision_model, ['encoder'])\n",
    "        \n",
    "        # Freeze\n",
    "        self.vision_model.trainable = False\n",
    "        \n",
    "    def call(self, img_feats, prompts):\n",
    "        B = tf.shape(img_feats)[0]\n",
    "        H = tf.shape(img_feats)[1]\n",
    "        W = tf.shape(img_feats)[2]\n",
    "        \n",
    "        prompts = tf.cast(prompts, img_feats.dtype)\n",
    "        x = tf.reshape(img_feats, [B, H * W, 768])\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_token = self.embeddings.class_embedding\n",
    "        cls_token = tf.cast(cls_token, x.dtype)\n",
    "        cls_token = tf.reshape(cls_token, [1, 1, 768])\n",
    "        cls_token = tf.tile(cls_token, [B, 1, 1])\n",
    "        x = tf.concat([cls_token, x], axis=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        pos_embed_obj = self.embeddings.position_embedding\n",
    "        if hasattr(pos_embed_obj, 'weights'):\n",
    "             pos_embed = pos_embed_obj.weights[0]\n",
    "        else:\n",
    "             pos_embed = pos_embed_obj\n",
    "             \n",
    "        pos_embed = tf.cast(pos_embed, x.dtype)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = x + pos_embed[:seq_len, :]\n",
    "        \n",
    "        # Pre-LayerNorm (Using found layer)\n",
    "        x = self.pre_layrnorm(x)\n",
    "        \n",
    "        selected = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        prompt_idx = 0\n",
    "        \n",
    "        for i, layer in enumerate(self.encoder.layers):\n",
    "            if i in selected:\n",
    "                p = prompts[:, prompt_idx, :]\n",
    "                p = tf.expand_dims(p, 1)\n",
    "                x = tf.concat([x, p], axis=1)\n",
    "                # Explicit None args\n",
    "                layer_out = layer(x, attention_mask=None, output_attentions=False, training=False, causal_attention_mask=None)\n",
    "                x = layer_out[0]\n",
    "                x = x[:, :-1, :]\n",
    "                prompt_idx += 1\n",
    "            else:\n",
    "                layer_out = layer(x, attention_mask=None, output_attentions=False, training=False, causal_attention_mask=None)\n",
    "                x = layer_out[0]\n",
    "        \n",
    "        x = x[:, 1:, :]\n",
    "        x = tf.reshape(x, [B, H, W, 768])\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP_Adapter(layers.Layer):\n",
    "    \"\"\"\n",
    "    100% Faithful replication of PyTorch GALIP's CLIP_Adapter.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num, clip_model):\n",
    "        super(CLIP_Adapter, self).__init__()\n",
    "        self.CLIP_ch = CLIP_ch\n",
    "        self.f_blocks = []\n",
    "        self.f_blocks.append(M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        for _ in range(map_num - 1):\n",
    "            self.f_blocks.append(M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        self.conv_fuse = layers.Conv2D(CLIP_ch, 5, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        self.CLIP_ViT = CLIP_Mapper(clip_model)\n",
    "        self.conv_out = layers.Conv2D(G_ch, 5, strides=1, padding='same', kernel_initializer='he_uniform')\n",
    "        self.fc_prompt = layers.Dense(CLIP_ch * 8, kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, out, c):\n",
    "        prompts = self.fc_prompt(c)\n",
    "        prompts = tf.reshape(prompts, [-1, 8, self.CLIP_ch])\n",
    "        for FBlock in self.f_blocks:\n",
    "            out = FBlock(out, c)\n",
    "        fuse_feat = self.conv_fuse(out)\n",
    "        map_feat = self.CLIP_ViT(fuse_feat, prompts)\n",
    "        return self.conv_out(fuse_feat + 0.1 * map_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. MODELS (Fixed & Robust)\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# --- Robust Layer Lookup Helpers ---\n",
    "def get_layer_safe(model, layer_name):\n",
    "    if hasattr(model, layer_name): return getattr(model, layer_name)\n",
    "    if hasattr(model, 'clip') and hasattr(model.clip, layer_name): return getattr(model.clip, layer_name)\n",
    "    for layer in model.layers:\n",
    "        if layer.name == layer_name: return layer\n",
    "        if layer.name == 'clip' and hasattr(layer, layer_name): return getattr(layer, layer_name)\n",
    "    raise AttributeError(f\"Could not find '{layer_name}' in TFCLIPModel.\")\n",
    "\n",
    "def get_sublayer_safe(model, possible_names):\n",
    "    for name in possible_names:\n",
    "        if hasattr(model, name): return getattr(model, name)\n",
    "    for layer in model.layers:\n",
    "        for name in possible_names:\n",
    "            if name in layer.name: return layer\n",
    "    raise AttributeError(f\"Could not find any of {possible_names} in model.\")\n",
    "\n",
    "# --- Encoders ---\n",
    "class CLIP_Text_Encoder(layers.Layer):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.text_model = get_layer_safe(clip_model, 'text_model')\n",
    "        self.text_projection = get_layer_safe(clip_model, 'text_projection')\n",
    "        self.text_model.trainable = False\n",
    "        self.text_projection.trainable = False\n",
    "        \n",
    "    def call(self, input_ids, attention_mask=None):\n",
    "        outputs = self.text_model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=None,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "        word_emb = outputs.last_hidden_state\n",
    "        eot_indices = tf.argmax(tf.cast(input_ids, tf.int32), axis=-1)\n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        batch_indices = tf.range(batch_size, dtype=tf.int64)\n",
    "        gather_indices = tf.stack([batch_indices, tf.cast(eot_indices, tf.int64)], axis=1)\n",
    "        pooled_output = tf.gather_nd(word_emb, gather_indices)\n",
    "        sent_emb = self.text_projection(pooled_output)\n",
    "        return sent_emb, word_emb\n",
    "    \n",
    "    @property\n",
    "    def trainable_weights(self): return []\n",
    "    @property  \n",
    "    def non_trainable_weights(self): return self.text_model.weights + self.text_projection.weights\n",
    "\n",
    "class CLIP_Image_Encoder(layers.Layer):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = get_layer_safe(clip_model, 'vision_model')\n",
    "        self.visual_projection = get_layer_safe(clip_model, 'visual_projection')\n",
    "        self.embeddings = get_sublayer_safe(self.vision_model, ['embeddings'])\n",
    "        self.pre_layrnorm = get_sublayer_safe(self.vision_model, ['pre_layrnorm', 'pre_layernorm', 'layernorm_pre'])\n",
    "        self.post_layernorm = get_sublayer_safe(self.vision_model, ['post_layernorm', 'post_layernorm', 'layernorm_post'])\n",
    "        self.encoder = get_sublayer_safe(self.vision_model, ['encoder'])\n",
    "        self.vision_model.trainable = False\n",
    "        self.visual_projection.trainable = False\n",
    "        \n",
    "    def transf_to_CLIP_input(self, inputs):\n",
    "        x = (inputs + 1.0) * 0.5\n",
    "        x = tf.image.resize(x, [224, 224], method='bicubic')\n",
    "        mean = tf.constant([0.48145466, 0.4578275, 0.40821073], dtype=x.dtype)\n",
    "        std = tf.constant([0.26862954, 0.26130258, 0.27577711], dtype=x.dtype)\n",
    "        x = (x - mean) / std\n",
    "        return x\n",
    "\n",
    "    def call(self, img):\n",
    "        x = self.transf_to_CLIP_input(img)\n",
    "        x = tf.transpose(x, [0, 3, 1, 2]) # Fix: Transpose to NCHW\n",
    "        x = self.embeddings(x)\n",
    "        x = self.pre_layrnorm(x)\n",
    "        local_features = []\n",
    "        selected = [1, 4, 8]\n",
    "        for i, layer in enumerate(self.encoder.layers):\n",
    "            layer_out = layer(x, attention_mask=None, causal_attention_mask=None, output_attentions=False, training=False)\n",
    "            x = layer_out[0]\n",
    "            if i in selected:\n",
    "                grid = x[:, 1:, :] \n",
    "                B = tf.shape(grid)[0]\n",
    "                grid = tf.reshape(grid, [B, 7, 7, 768])\n",
    "                local_features.append(grid)\n",
    "        cls_token = self.post_layernorm(x[:, 0, :])\n",
    "        global_emb = self.visual_projection(cls_token)\n",
    "        local_features = tf.stack(local_features, axis=1)\n",
    "        return local_features, global_emb\n",
    "\n",
    "# --- GAN Models ---\n",
    "class NetG(Model):\n",
    "    def __init__(self, ngf, nz, cond_dim, clip_model):\n",
    "        super(NetG, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.code_sz, self.code_ch, self.mid_ch = 7, 64, 32\n",
    "        self.CLIP_ch = 768\n",
    "        self.fc_code = layers.Dense(self.code_sz * self.code_sz * self.code_ch, kernel_initializer='he_uniform')\n",
    "        self.mapping = CLIP_Adapter(self.code_ch, self.mid_ch, self.code_ch, ngf * 8, self.CLIP_ch, cond_dim + nz, 3, 1, 1, 4, clip_model)\n",
    "        self.g_blocks = []\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 8))\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 8, ngf * 4))\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 4, ngf * 2))\n",
    "        self.g_blocks.append(G_Block(cond_dim + nz, ngf * 2, ngf * 1))\n",
    "        self.target_sizes = [8, 16, 32, 64]\n",
    "        self.to_rgb = tf.keras.Sequential([layers.LeakyReLU(0.2), layers.Conv2D(3, 3, padding='same', kernel_initializer='he_uniform')])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        noise, c = inputs\n",
    "        cond = tf.concat([noise, c], axis=1)\n",
    "        out = self.fc_code(noise)\n",
    "        out = tf.reshape(out, [-1, self.code_sz, self.code_sz, self.code_ch])\n",
    "        out = self.mapping(out, cond)\n",
    "        for block, target_size in zip(self.g_blocks, self.target_sizes):\n",
    "            out = block(out, cond, target_size=target_size) # Fix: Keyword arg\n",
    "        out = self.to_rgb(out)\n",
    "        out = tf.nn.tanh(out)\n",
    "        return out\n",
    "\n",
    "class NetD(Model):\n",
    "    def __init__(self, ndf):\n",
    "        super(NetD, self).__init__()\n",
    "        self.d_blocks = [D_Block(768, 768, is_res=True, clip_feat=True) for _ in range(2)]\n",
    "        self.main = D_Block(768, 512, is_res=True, clip_feat=False)\n",
    "    def call(self, h):\n",
    "        out = h[:, 0]\n",
    "        for idx in range(len(self.d_blocks)): out = self.d_blocks[idx](out, h[:, idx+1])\n",
    "        out = self.main(out)\n",
    "        return out\n",
    "\n",
    "class NetC(Model):\n",
    "    def __init__(self, ndf, cond_dim):\n",
    "        super(NetC, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.joint_conv = tf.keras.Sequential([\n",
    "            layers.Conv2D(ndf * 2, 4, strides=1, padding='valid', use_bias=False, kernel_initializer='he_uniform'),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Conv2D(1, 4, strides=1, padding='valid', use_bias=False, kernel_initializer='he_uniform')\n",
    "        ])\n",
    "    def call(self, out, cond):\n",
    "        B = tf.shape(out)[0]\n",
    "        cond = tf.reshape(cond, [B, 1, 1, self.cond_dim])\n",
    "        cond = tf.tile(cond, [1, 7, 7, 1])\n",
    "        h_c = tf.concat([out, cond], axis=3)\n",
    "        return self.joint_conv(h_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # Hinge loss for Discriminator\n",
    "    # Real: min(0, -1 + real) -> ReLU(1 - real)\n",
    "    # Fake: min(0, -1 - fake) -> ReLU(1 + fake)\n",
    "    real_loss = tf.reduce_mean(tf.nn.relu(1.0 - real_logits))\n",
    "    fake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake_logits))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_adversarial_loss(fake_logits):\n",
    "    # Hinge loss for Generator (maximize D's output)\n",
    "    return -tf.reduce_mean(fake_logits)\n",
    "  \n",
    "  \n",
    "def clip_matching_loss(image_emb, text_emb):\n",
    "    # image_emb: [B, 512] (from CLIP_Image_Encoder global_emb)\n",
    "    # text_emb: [B, 512] (from CLIP_Text_Encoder sent_emb)\n",
    "    \n",
    "    # 1. Normalize embeddings (Crucial for Cosine Similarity)\n",
    "    image_emb = tf.nn.l2_normalize(image_emb, axis=1)\n",
    "    text_emb = tf.nn.l2_normalize(text_emb, axis=1)\n",
    "    \n",
    "    # 2. Compute Cosine Distance (1 - Cosine Similarity)\n",
    "    # Reducing across the batch dimension\n",
    "    sim = tf.reduce_sum(image_emb * text_emb, axis=1)\n",
    "    loss = 1.0 - sim\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "  \n",
    "  \n",
    "def MA_GP(discriminator, net_c, CLIP_real, sent_emb, pred_real):\n",
    "    \"\"\"\n",
    "    Matching-Aware Gradient Penalty (MA-GP).\n",
    "    Calculates gradient of D(real, text) w.r.t. input features.\n",
    "    Target: Penalize gradients to enforce Lipschitz continuity.\n",
    "    \n",
    "    Args:\n",
    "        discriminator: NetD model\n",
    "        net_c: NetC model\n",
    "        CLIP_real: [B, 3, 7, 7, 768] Real image CLIP features\n",
    "        sent_emb: [B, 512] Text embeddings\n",
    "        pred_real: [B, 1, 1, 1] The validity score (output of NetC)\n",
    "        \n",
    "    Returns:\n",
    "        gradient_penalty: Scalar tensor\n",
    "    \"\"\"\n",
    "    # In TensorFlow, we need the tape to verify the gradients.\n",
    "    # We assume this function is called INSIDE the GradientTape where \n",
    "    # CLIP_real and sent_emb were watched.\n",
    "    \n",
    "    # 1. Get gradients of the prediction w.r.t inputs\n",
    "    # Note: We need gradients w.r.t BOTH visual features and text embeddings\n",
    "    grads = tf.gradients(pred_real, [CLIP_real, sent_emb])\n",
    "    \n",
    "    # 2. Flatten and Concatenate\n",
    "    grad_img = tf.reshape(grads[0], [tf.shape(grads[0])[0], -1]) # [B, N_img]\n",
    "    grad_txt = tf.reshape(grads[1], [tf.shape(grads[1])[0], -1]) # [B, N_txt]\n",
    "    grad = tf.concat([grad_img, grad_txt], axis=1) # [B, N_total]\n",
    "    \n",
    "    # 3. Calculate Norm\n",
    "    grad_l2norm = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=1))\n",
    "    \n",
    "    # 4. Apply Penalty (Power of 6 is specific to DF-GAN/GALIP)\n",
    "    # Weight is typically 2.0 in their repo\n",
    "    d_loss_gp = 2.0 * tf.reduce_mean(tf.pow(grad_l2norm, 6))\n",
    "    \n",
    "    return d_loss_gp\n",
    "\n",
    "def predict_loss(net_c, d_feats, sent_emb, negative=False):\n",
    "    \"\"\"\n",
    "    Computes Hinge Loss component.\n",
    "    Args:\n",
    "        negative: True for Fake/Mismatch (minimize -1 - D), False for Real (minimize -1 + D)\n",
    "    \"\"\"\n",
    "    # NetC output: [B, 1, 1, 1] -> Flatten to [B]\n",
    "    logits = net_c(d_feats, sent_emb, training=True)\n",
    "    logits = tf.reshape(logits, [-1])\n",
    "    \n",
    "    if negative:\n",
    "        # Fake or Mismatch: ReLU(1 + D(x))\n",
    "        loss = tf.reduce_mean(tf.nn.relu(1.0 + logits))\n",
    "    else:\n",
    "        # Real: ReLU(1 - D(x))\n",
    "        loss = tf.reduce_mean(tf.nn.relu(1.0 - logits))\n",
    "        \n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def DiffAugment(x, policy='translation'):\n",
    "    \"\"\"\n",
    "    TensorFlow implementation of DiffAugment.\n",
    "    Supports 'color', 'translation', 'cutout'.\n",
    "    \"\"\"\n",
    "    if policy:\n",
    "        if 'color' in policy:\n",
    "            x = rand_brightness(x)\n",
    "            x = rand_saturation(x)\n",
    "            x = rand_contrast(x)\n",
    "        if 'translation' in policy:\n",
    "            x = rand_translation(x)\n",
    "        if 'cutout' in policy:\n",
    "            x = rand_cutout(x)\n",
    "    return x\n",
    "\n",
    "# --- Augmentation Primitives ---\n",
    "def rand_brightness(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=-0.5, maxval=0.5)\n",
    "    x = x + magnitude\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_saturation(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.0, maxval=2.0)\n",
    "    x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_contrast(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.5, maxval=1.5)\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    shift = int(64 * ratio)\n",
    "    \n",
    "    # Pad the image with reflection\n",
    "    x_padded = tf.pad(x, [[0, 0], [shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "    \n",
    "    # Vectorized Random Crop using crop_and_resize\n",
    "    padded_size = tf.cast(img_size + 2*shift, tf.float32)\n",
    "    max_offset = 2 * shift\n",
    "    \n",
    "    offsets_y = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    offsets_x = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    \n",
    "    offsets_y = tf.cast(offsets_y, tf.float32)\n",
    "    offsets_x = tf.cast(offsets_x, tf.float32)\n",
    "    \n",
    "    # Normalize coordinates to [0, 1] for crop_and_resize\n",
    "    y1 = offsets_y / padded_size\n",
    "    x1 = offsets_x / padded_size\n",
    "    y2 = (offsets_y + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    x2 = (offsets_x + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    \n",
    "    boxes = tf.stack([y1, x1, y2, x2], axis=1) # [B, 4]\n",
    "    box_indices = tf.range(batch_size)\n",
    "    \n",
    "    x_translated = tf.image.crop_and_resize(\n",
    "        x_padded, \n",
    "        boxes, \n",
    "        box_indices, \n",
    "        crop_size=[img_size, img_size]\n",
    "    )\n",
    "    \n",
    "    return x_translated\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    cutout_size = int(64 * ratio // 2) * 2\n",
    "    \n",
    "    iy, ix = tf.meshgrid(tf.range(img_size), tf.range(img_size), indexing='ij')\n",
    "    iy = tf.expand_dims(iy, 0) \n",
    "    ix = tf.expand_dims(ix, 0)\n",
    "    \n",
    "    offset_x = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    offset_y = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    \n",
    "    mask_x = tf.math.logical_and(ix >= offset_x, ix < offset_x + cutout_size)\n",
    "    mask_y = tf.math.logical_and(iy >= offset_y, iy < offset_y + cutout_size)\n",
    "    mask_box = tf.math.logical_and(mask_x, mask_y)\n",
    "    \n",
    "    mask_keep = tf.cast(tf.math.logical_not(mask_box), x.dtype)\n",
    "    mask_keep = tf.expand_dims(mask_keep, -1) \n",
    "    \n",
    "    return x * mask_keep\n",
    "\n",
    "def save_sample_images(generator, text_encoder, fixed_input_ids, fixed_noise, epoch, save_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a grid of images using fixed noise/text for consistency.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Encode text\n",
    "    text_embeds, _ = text_encoder(fixed_input_ids, training=False)\n",
    "    \n",
    "    # Generate\n",
    "    fake_imgs = generator([fixed_noise, text_embeds], training=False)\n",
    "    \n",
    "    # Convert to [0, 1] for plotting\n",
    "    fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "    fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "    \n",
    "    # Plot Grid\n",
    "    n = int(np.sqrt(len(fake_imgs)))\n",
    "    if n * n != len(fake_imgs): n = 8 \n",
    "    \n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(min(8, len(fake_imgs))):\n",
    "        plt.subplot(1, 8, i+1)\n",
    "        plt.imshow(fake_imgs[i])\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:03d}.png'))\n",
    "    plt.close()\n",
    "# ==============================================================================\n",
    "# 5. OPTIMIZED TRAINING PIPELINE\n",
    "# ==============================================================================\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import TFCLIPModel, CLIPTokenizer\n",
    "\n",
    "# --- A. Pre-computation Logic ---\n",
    "def precompute_data(df_path, image_encoder, tokenizer, word2Id_dict, id2word_dict):\n",
    "    print(\"--- üöÄ Starting Pre-computation (One-time Setup) ---\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df = pd.read_pickle(df_path)\n",
    "    unique_image_paths = df['ImagePath'].unique()\n",
    "    path_to_index = {path: i for i, path in enumerate(unique_image_paths)}\n",
    "    print(f\"Found {len(unique_image_paths)} unique images.\")\n",
    "    \n",
    "    # 2. Extract Image Features (Run CLIP ViT ONCE)\n",
    "    print(\"Extracting Image Features (This caches CLIP outputs to VRAM)...\")\n",
    "    \n",
    "    def load_img(path):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [64, 64], method='bicubic')\n",
    "        img = (img * 2.0) - 1.0\n",
    "        return img\n",
    "\n",
    "    img_ds = tf.data.Dataset.from_tensor_slices(unique_image_paths)\n",
    "    img_ds = img_ds.map(load_img, num_parallel_calls=tf.data.AUTOTUNE).batch(128)\n",
    "    \n",
    "    all_img_features = []\n",
    "    for batch_imgs in tqdm(img_ds, desc=\"Encoding Images\"):\n",
    "        # We store the 7x7 local features needed by Discriminator\n",
    "        local_feats, _ = image_encoder(batch_imgs) \n",
    "        all_img_features.append(local_feats)\n",
    "        \n",
    "    cached_img_features = tf.concat(all_img_features, axis=0)\n",
    "    print(f\"‚úì Image Features Cached. Shape: {cached_img_features.shape}\")\n",
    "\n",
    "    # 3. Process Text (Tokenize all captions)\n",
    "    print(\"Tokenizing Text...\")\n",
    "    captions_ids = df['Captions'].values\n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    dataset_indices = []\n",
    "    dataset_captions = []\n",
    "    \n",
    "    def decode_ids(id_list):\n",
    "        words = [id2word_dict.get(str(i), '') for i in id_list]\n",
    "        return ' '.join([w for w in words if w and w != '<PAD>'])\n",
    "\n",
    "    # Expand all captions\n",
    "    for caps, path in zip(captions_ids, image_paths):\n",
    "        img_idx = path_to_index[path]\n",
    "        for cap_ids in caps:\n",
    "            text = decode_ids(cap_ids)\n",
    "            dataset_indices.append(img_idx)\n",
    "            dataset_captions.append(text)\n",
    "            \n",
    "    # Batch Tokenize using Hugging Face (CPU)\n",
    "    encodings = tokenizer(\n",
    "        dataset_captions, padding='max_length', truncation=True, max_length=77, return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    cached_input_ids = encodings['input_ids']\n",
    "    cached_masks = encodings['attention_mask']\n",
    "    cached_indices = np.array(dataset_indices, dtype=np.int32)\n",
    "    \n",
    "    print(f\"‚úì Text Prepared. Total Samples: {len(cached_indices)}\")\n",
    "    return cached_img_features, cached_input_ids, cached_masks, cached_indices\n",
    "\n",
    "# --- B. Fast Dataset Generator ---\n",
    "def fast_dataset_generator(img_feats, input_ids, masks, indices, batch_size):\n",
    "    \"\"\"Yields (cached_img_features, input_ids, attention_mask) instantly.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((indices, input_ids, masks))\n",
    "    dataset = dataset.shuffle(len(indices), reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    @tf.function\n",
    "    def lookup_data(b_indices, b_ids, b_masks):\n",
    "        # Lookup image features from GPU memory using indices\n",
    "        b_imgs = tf.gather(img_feats, b_indices)\n",
    "        return b_imgs, b_ids, b_masks\n",
    "    \n",
    "    dataset = dataset.map(lookup_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# --- C. Optimized Train Step ---\n",
    "@tf.function\n",
    "def train_step_fast(real_img_feats, input_ids, attention_mask, \n",
    "                    generator, discriminator, net_c, \n",
    "                    text_encoder, image_encoder,\n",
    "                    g_optimizer, d_optimizer, \n",
    "                    batch_size, z_dim, sim_w=4.0):\n",
    "    \n",
    "    # 1. Encode Text (Fast)\n",
    "    sent_emb, _ = text_encoder(input_ids, attention_mask=attention_mask)\n",
    "    noise = tf.random.normal([batch_size, z_dim])\n",
    "    \n",
    "    # ====================\n",
    "    # Train Discriminator\n",
    "    # ====================\n",
    "    with tf.GradientTape(persistent=True) as d_tape:\n",
    "        d_tape.watch(sent_emb)\n",
    "        # OPTIMIZATION: Use cached features directly! No image_encoder call for real.\n",
    "        CLIP_real = real_img_feats \n",
    "        d_tape.watch(CLIP_real)\n",
    "        \n",
    "        real_feats = discriminator(CLIP_real, training=True)\n",
    "        pred_real = net_c(real_feats, sent_emb, training=True)\n",
    "        errD_real = tf.reduce_mean(tf.nn.relu(1.0 - pred_real))\n",
    "        \n",
    "        # Mismatch\n",
    "        mis_sent_emb = tf.roll(sent_emb, shift=1, axis=0)\n",
    "        pred_mis = net_c(real_feats, mis_sent_emb, training=True)\n",
    "        errD_mis = tf.reduce_mean(tf.nn.relu(1.0 + pred_mis))\n",
    "        \n",
    "        # Fake (Still need encoder as images are generated on fly)\n",
    "        fake_images = generator([noise, sent_emb], training=True)\n",
    "        fake_images_stopped = tf.stop_gradient(fake_images)\n",
    "        CLIP_fake, _ = image_encoder(fake_images_stopped)\n",
    "        \n",
    "        fake_feats = discriminator(CLIP_fake, training=True)\n",
    "        pred_fake = net_c(fake_feats, sent_emb, training=True)\n",
    "        errD_fake = tf.reduce_mean(tf.nn.relu(1.0 + pred_fake))\n",
    "    \n",
    "    # MA-GP\n",
    "    grads = d_tape.gradient(pred_real, [CLIP_real, sent_emb])\n",
    "    grad_flat = tf.concat([tf.reshape(grads[0], [batch_size, -1]), tf.reshape(grads[1], [batch_size, -1])], axis=1)\n",
    "    grad_norm = tf.sqrt(tf.reduce_sum(tf.square(grad_flat), axis=1) + 1e-8)\n",
    "    errD_MAGP = 2.0 * tf.reduce_mean(tf.pow(grad_norm, 6))\n",
    "    \n",
    "    d_loss = errD_real + (errD_fake + errD_mis) / 2.0 + errD_MAGP\n",
    "    del d_tape\n",
    "    \n",
    "    d_vars = discriminator.trainable_variables + net_c.trainable_variables\n",
    "    d_grads = d_optimizer.get_gradients(d_loss, d_vars)\n",
    "    d_optimizer.apply_gradients(zip(d_grads, d_vars))\n",
    "    \n",
    "    # ====================\n",
    "    # Train Generator\n",
    "    # ====================\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        fake_images_g = generator([noise, sent_emb], training=True)\n",
    "        CLIP_fake_g, fake_emb_g = image_encoder(fake_images_g)\n",
    "        \n",
    "        fake_feats_g = discriminator(CLIP_fake_g, training=True)\n",
    "        output = net_c(fake_feats_g, sent_emb, training=True)\n",
    "        g_adv_loss = -tf.reduce_mean(output)\n",
    "        \n",
    "        fake_norm = tf.nn.l2_normalize(fake_emb_g, axis=1)\n",
    "        sent_norm = tf.nn.l2_normalize(sent_emb, axis=1)\n",
    "        text_img_sim = tf.reduce_mean(tf.reduce_sum(fake_norm * sent_norm, axis=1))\n",
    "        \n",
    "        g_loss = g_adv_loss - (sim_w * text_img_sim)\n",
    "    \n",
    "    g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    \n",
    "    return {'d_loss': d_loss, 'g_loss': g_loss, 'errD_MAGP': errD_MAGP, 'clip_sim': text_img_sim}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import TFCLIPModel, CLIPTokenizer\n",
    "\n",
    "def train(args, word2Id_dict, id2word_dict):\n",
    "    \"\"\"\n",
    "    100% Faithful TensorFlow replication of PyTorch GALIP training loop.\n",
    "    OPTIMIZED: Uses 'Cache & Freeze' strategy for 20x speedup.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. Initialization & Logging Setup\n",
    "    # ==========================================================================\n",
    "    print(f\"--- Initializing Models (Image Size: {args['IMAGE_SIZE']}) ---\")\n",
    "    \n",
    "    # Load CLIP Models\n",
    "    print(\"--- Loading CLIP Models ---\")\n",
    "    try:\n",
    "        clip_model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\") # Added Tokenizer loading\n",
    "        print(\"‚úì CLIP Model & Tokenizer loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error loading CLIP model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize Encoders\n",
    "    text_encoder = CLIP_Text_Encoder(clip_model)\n",
    "    image_encoder = CLIP_Image_Encoder(clip_model)\n",
    "    \n",
    "    # Initialize GAN Models\n",
    "    generator = NetG(ngf=args['NGF'], nz=args['Z_DIM'], cond_dim=args['EMBED_DIM'], clip_model=clip_model)\n",
    "    discriminator = NetD(ndf=args['NDF'])\n",
    "    net_c = NetC(ndf=args['NDF'], cond_dim=args['EMBED_DIM'])\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_G'], beta_1=0.0, beta_2=0.9, epsilon=ADAM_EPSILON)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_D'], beta_1=0.0, beta_2=0.9, epsilon=ADAM_EPSILON)\n",
    "\n",
    "    # Checkpoints\n",
    "    checkpoint_dir = os.path.join(args['RUN_DIR'], 'checkpoints')\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        generator=generator, discriminator=discriminator, net_c=net_c,\n",
    "        g_optimizer=g_optimizer, d_optimizer=d_optimizer\n",
    "    )\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    # TensorBoard Setup\n",
    "    log_dir = os.path.join(args['RUN_DIR'], 'logs')\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    try:\n",
    "        tensorboard_process = subprocess.Popen(\n",
    "            [sys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir]\n",
    "        )\n",
    "        print(f\"‚úì TensorBoard launched (PID: {tensorboard_process.pid})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not launch TensorBoard: {e}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. OPTIMIZATION: Pre-computation & Fast Dataset\n",
    "    # ==========================================================================\n",
    "    # Instead of using the slow 'dataset' passed in, we create a fast one here.\n",
    "    \n",
    "    # Path to your dataframe (Constructed from args)\n",
    "    df_path = os.path.join(args['DATA_PATH'], 'text2ImgData.pkl')\n",
    "    \n",
    "    # Run the cache logic (Defined in Cell 5)\n",
    "    cached_img_feats, cached_ids, cached_masks, cached_indices = precompute_data(\n",
    "        df_path, image_encoder, tokenizer, word2Id_dict, id2word_dict\n",
    "    )\n",
    "    \n",
    "    # Create the Zero-Latency Dataset\n",
    "    fast_dataset = fast_dataset_generator(\n",
    "        cached_img_feats, cached_ids, cached_masks, cached_indices, args['BATCH_SIZE']\n",
    "    )\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. Training Loop Setup\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # Fixed noise for visualization\n",
    "    fixed_noise = tf.random.normal([8, args['Z_DIM']])\n",
    "    \n",
    "    # Get fixed text from first batch of FAST dataset\n",
    "    # Note: fast_dataset yields (img_features, input_ids, mask)\n",
    "    for _, fixed_input_ids, fixed_mask in fast_dataset.take(1):\n",
    "        fixed_input_ids = fixed_input_ids[:8]\n",
    "        fixed_mask = fixed_mask[:8]\n",
    "        break\n",
    "\n",
    "    start_epoch = 0\n",
    "    if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint)\n",
    "        print(f\"Restored from {manager.latest_checkpoint}\")\n",
    "        \n",
    "    print(f\"Starting training for {args['MAX_EPOCH']} epochs...\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 4. Training Loop\n",
    "    # ==========================================================================\n",
    "    for epoch in range(start_epoch, args['MAX_EPOCH']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Progress bar over FAST dataset\n",
    "        pbar = tqdm(fast_dataset, desc=f\"Epoch {epoch+1}/{args['MAX_EPOCH']}\")\n",
    "        \n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        \n",
    "        # UPDATED UNPACKING: Accepts pre-computed features!\n",
    "        for step, (real_img_feats, input_ids, attention_mask) in enumerate(pbar):\n",
    "            \n",
    "            # Use the FAST train step\n",
    "            losses = train_step_fast(\n",
    "                real_img_feats, # Passing VRAM-cached features\n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                generator, \n",
    "                discriminator, \n",
    "                net_c, \n",
    "                text_encoder, \n",
    "                image_encoder,\n",
    "                g_optimizer, \n",
    "                d_optimizer, \n",
    "                args['BATCH_SIZE'], \n",
    "                args['Z_DIM'],\n",
    "                sim_w=args.get('SIM_W', 4.0) # Default to 4.0\n",
    "            )\n",
    "            \n",
    "            # Map keys for logging (handling renaming from train_step_fast)\n",
    "            d_loss_val = float(losses['d_loss'])\n",
    "            g_loss_val = float(losses['g_loss'])\n",
    "            clip_sim_val = float(losses['acc_sim']) # Mapped from 'acc_sim'\n",
    "            magp_val = float(losses['errD_MAGP'])\n",
    "            \n",
    "            d_losses.append(d_loss_val)\n",
    "            g_losses.append(g_loss_val)\n",
    "            \n",
    "            # Update pbar\n",
    "            pbar.set_postfix({\n",
    "                'D': f\"{d_loss_val:.4f}\", \n",
    "                'G': f\"{g_loss_val:.4f}\",\n",
    "                'MAGP': f\"{magp_val:.4f}\",\n",
    "                'CLIP': f\"{clip_sim_val:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            with summary_writer.as_default():\n",
    "                step_global = epoch * len(cached_indices) // args['BATCH_SIZE'] + step\n",
    "                tf.summary.scalar('Loss/D_total', d_loss_val, step=step_global)\n",
    "                tf.summary.scalar('Loss/G_total', g_loss_val, step=step_global)\n",
    "                tf.summary.scalar('Loss/MA_GP', magp_val, step=step_global)\n",
    "                tf.summary.scalar('Loss/CLIP_sim', clip_sim_val, step=step_global)\n",
    "\n",
    "        # End of Epoch\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"Epoch {epoch+1} done. D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}, Time: {time.time()-start_time:.1f}s\")\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        if (epoch + 1) % args['SAVE_FREQ'] == 0:\n",
    "            save_path = manager.save()\n",
    "            print(f\"Saved checkpoint for epoch {epoch+1}: {save_path}\")\n",
    "            \n",
    "        # Save Sample Images\n",
    "        if (epoch + 1) % args['SAMPLE_FREQ'] == 0:\n",
    "            save_sample_images(generator, text_encoder, fixed_input_ids, fixed_noise, epoch+1, os.path.join(args['RUN_DIR'], 'samples'))\n",
    "\n",
    "    print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Run Directory: ./runs/20251129-164545\n",
      "Config: {\n",
      "  \"IMAGE_SIZE\": [\n",
      "    64,\n",
      "    64,\n",
      "    3\n",
      "  ],\n",
      "  \"NGF\": 64,\n",
      "  \"NDF\": 64,\n",
      "  \"Z_DIM\": 100,\n",
      "  \"EMBED_DIM\": 512,\n",
      "  \"LR_G\": 0.0001,\n",
      "  \"LR_D\": 0.0004,\n",
      "  \"SIM_W\": 4.0,\n",
      "  \"BATCH_SIZE\": 32,\n",
      "  \"MAX_EPOCH\": 600,\n",
      "  \"RUN_DIR\": \"./runs/20251129-164545\",\n",
      "  \"SAVE_FREQ\": 25,\n",
      "  \"SAMPLE_FREQ\": 1,\n",
      "  \"DATA_PATH\": \"./dataset\",\n",
      "  \"N_SAMPLE\": 7370\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Define configuration for training\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create a unique run directory\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"./runs/{run_id}\"\n",
    "if not os.path.exists(run_dir):\n",
    "    os.makedirs(run_dir)\n",
    "\n",
    "# User provided config - 100% faithful to PyTorch GALIP\n",
    "config = {\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    'NGF': 64,                # nf in PyTorch\n",
    "    'NDF': 64,                # nf in PyTorch  \n",
    "    'Z_DIM': 100,             # z_dim in PyTorch\n",
    "    'EMBED_DIM': 512,         # cond_dim in PyTorch (CLIP embedding dimension)\n",
    "    'LR_G': 0.0001,           # lr_g in PyTorch\n",
    "    'LR_D': 0.0004,           # lr_d in PyTorch\n",
    "    'SIM_W': 4.0,             # sim_w in PyTorch (CLIP similarity loss weight) - GALIP default is 4.0\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'MAX_EPOCH': 600,\n",
    "    'RUN_DIR': run_dir,\n",
    "    'SAVE_FREQ': 25,\n",
    "    'SAMPLE_FREQ': 1,\n",
    "    'DATA_PATH': data_path,\n",
    "    'N_SAMPLE': num_training_sample if 'num_training_sample' in locals() else 7370\n",
    "}\n",
    "\n",
    "# Save config for reproducibility\n",
    "with open(os.path.join(run_dir, 'config.json'), 'w') as f:\n",
    "    # Filter for JSON serializable values\n",
    "    json_config = {k: v for k, v in config.items() if isinstance(v, (int, float, str, list, bool))}\n",
    "    json.dump(json_config, f, indent=4)\n",
    "\n",
    "print(f\"Training Run Directory: {run_dir}\")\n",
    "print(f\"Config: {json.dumps(json_config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Models (Image Size: [64, 64, 3]) ---\n",
      "--- Loading CLIP Models ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CLIP Model & Tokenizer loaded\n",
      "‚úì TensorBoard launched (PID: 97378)\n",
      "--- üöÄ Starting Pre-computation (One-time Setup) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lee_eason/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7370 unique images.\n",
      "Extracting Image Features (This caches CLIP outputs to VRAM)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Images:   0%|          | 0/58 [00:00<?, ?it/s]Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.20.0 at http://localhost:6009/ (Press CTRL+C to quit)\n",
      "Encoding Images:   3%|‚ñé         | 2/58 [00:16<07:36,  8.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8785\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8786\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8787\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   8788\u001b[0m         _ctx, \"Reshape\", name, tensor, shape)\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5h/n64mcyts207dxrlc16rx_5cw0000gn/T/ipykernel_17476/3916315380.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pass the dictionaries directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2Id_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5h/n64mcyts207dxrlc16rx_5cw0000gn/T/ipykernel_17476/2490625144.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(args, word2Id_dict, id2word_dict)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Path to your dataframe (Constructed from args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mdf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATA_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text2ImgData.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Run the cache logic (Defined in Cell 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     cached_img_feats, cached_ids, cached_masks, cached_indices = precompute_data(\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2Id_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5h/n64mcyts207dxrlc16rx_5cw0000gn/T/ipykernel_17476/3758301756.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df_path, image_encoder, tokenizer, word2Id_dict, id2word_dict)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mall_img_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_imgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Encoding Images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# We store the 7x7 local features needed by Discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mlocal_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mall_img_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mcached_img_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_img_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m                     \u001b[0;34m\"layers will not see the mask.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                 )\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;31m# Destroy call context if we created it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;31m# 8. Add a node in the graph for symbolic calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[1;32m     56\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             )\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Plain flow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5h/n64mcyts207dxrlc16rx_5cw0000gn/T/ipykernel_17476/2005642904.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_layrnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mlocal_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mlayer_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1153\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 ):\n\u001b[0;32m-> 1155\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_tf_clip.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions, training)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m    438\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         attention_outputs = self.self_attn(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mcausal_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1153\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 ):\n\u001b[0;32m-> 1155\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/transformers/models/clip/modeling_tf_clip.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions, training)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;31m# (batch_size, seq_len_q, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;31m# In TFBert, attention weights are returned after dropout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# However, in CLIP, they are returned before dropout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attention_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1153\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 ):\n\u001b[0;32m-> 1155\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tf_keras/src/layers/core/dense.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Broadcast kernel to inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0;31m# Reshape the output back to the original ndim of the input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a, b, axes, name)\u001b[0m\n\u001b[1;32m   5413\u001b[0m       if (ab_matmul.get_shape().is_fully_defined() and\n\u001b[1;32m   5414\u001b[0m           ab_matmul.get_shape().as_list() == a_free_dims + b_free_dims):\n\u001b[1;32m   5415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mab_matmul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5416\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5417\u001b[0;31m         return array_ops.reshape(\n\u001b[0m\u001b[1;32m   5418\u001b[0m             ab_matmul, a_free_dims + b_free_dims, name=name)\n\u001b[1;32m   5419\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5420\u001b[0m       \u001b[0ma_free_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   \"\"\"\n\u001b[0;32m--> 199\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m   \u001b[0mshape_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/DL_comp/DL_comp3/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8783\u001b[0m   \u001b[0m_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8784\u001b[0m   \u001b[0mtld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8785\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8786\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8787\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   8788\u001b[0m         _ctx, \"Reshape\", name, tensor, shape)\n\u001b[1;32m   8789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8790\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pass the dictionaries directly\n",
    "train(config, word2Id_dict, id2word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¬∂</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¬∂</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string\n",
    "\t\t\t\tindex: Test sample ID\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tinput_ids, attention_mask, index\n",
    "\t\t\"\"\"\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing dataset generator - decodes IDs to raw text\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_pickle('./dataset/testData.pkl')\n",
    "\t\tcaptions_ids = data['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\tchosen_caption_ids = captions_ids[i]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\tindex = data['ID'].values\n",
    "\t\tindex = np.asarray(index)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.repeat().batch(batch_size)\n",
    "\t\t\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¬∂</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference directory inside the run directory\n",
    "inference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "if not os.path.exists(inference_dir):\n",
    "    os.makedirs(inference_dir)\n",
    "print(f\"Inference Directory: {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, config):\n",
    "    print(\"--- Starting Inference (Corrected for GALIP) ---\")\n",
    "    \n",
    "    # 1. Re-initialize CLIP (Required for Tokenizer and Encoder)\n",
    "    print(\"Loading CLIP components...\")\n",
    "    try:\n",
    "        from transformers import TFCLIPModel, CLIPTokenizer\n",
    "        clip_model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Re-create the specific Encoder wrapper used in training\n",
    "        text_encoder = CLIP_Text_Encoder(clip_model)\n",
    "        print(\"‚úì CLIP Model & Encoder loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error loading CLIP: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Load Generator\n",
    "    print(\"Loading Generator...\")\n",
    "    # Initialize with same args as training\n",
    "    generator = NetG(ngf=config['NGF'], nz=config['Z_DIM'], cond_dim=config['EMBED_DIM'], clip_model=clip_model)\n",
    "    \n",
    "    # Run a dummy forward pass to initialize variables before loading weights\n",
    "    dummy_noise = tf.random.normal([1, config['Z_DIM']])\n",
    "    dummy_text = tf.random.normal([1, config['EMBED_DIM']])\n",
    "    _ = generator([dummy_noise, dummy_text], training=False)\n",
    "    \n",
    "    # Restore Checkpoint\n",
    "    checkpoint_dir = os.path.join(config['RUN_DIR'], 'checkpoints')\n",
    "    # Use expect_partial() because we are strictly loading the generator, \n",
    "    # ignoring optimizer states or discriminator if they exist in the ckpt\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    if latest_ckpt:\n",
    "        print(f\"Loading weights from: {latest_ckpt}\")\n",
    "        status = checkpoint.restore(latest_ckpt).expect_partial()\n",
    "        status.assert_existing_objects_matched()\n",
    "        print(\"‚úì Generator weights loaded successfully\")\n",
    "    else:\n",
    "        print(\"‚ö† NO CHECKPOINT FOUND! Generating with random weights.\")\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    inference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "    if not os.path.exists(inference_dir): os.makedirs(inference_dir)\n",
    "        \n",
    "    total_images = 0\n",
    "    \n",
    "    # Iterate over the testing dataset\n",
    "    # Note: testing_dataset yields (caption_texts, ids)\n",
    "    for step, (caption_texts, image_ids) in enumerate(tqdm(dataset, desc='Generating')):\n",
    "        \n",
    "        batch_size_curr = len(caption_texts)\n",
    "        \n",
    "        # --- A. Tokenize using CLIP (Not dictionary!) ---\n",
    "        # Convert tensors to string list\n",
    "        text_list = [t.numpy().decode('utf-8') for t in caption_texts]\n",
    "        \n",
    "        enc = tokenizer(\n",
    "            text_list,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=77, # CLIP default\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids = enc['input_ids']\n",
    "        attention_mask = enc['attention_mask']\n",
    "        \n",
    "        # --- B. Encode Text using CLIP (Not RNN!) ---\n",
    "        sent_emb, _ = text_encoder(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # --- C. Generate ---\n",
    "        noise = tf.random.normal([batch_size_curr, config['Z_DIM']])\n",
    "        fake_imgs = generator([noise, sent_emb], training=False)\n",
    "        \n",
    "        # Post-process\n",
    "        fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "        fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "        \n",
    "        # Save\n",
    "        for i in range(batch_size_curr):\n",
    "            try:\n",
    "                # Handle ID decoding safely\n",
    "                img_id_val = image_ids[i].numpy()\n",
    "                if isinstance(img_id_val, bytes):\n",
    "                    img_id = img_id_val.decode('utf-8')\n",
    "                else:\n",
    "                    img_id = str(img_id_val)\n",
    "                    \n",
    "                save_path = os.path.join(inference_dir, f'inference_{img_id}.jpg')\n",
    "                plt.imsave(save_path, fake_imgs[i])\n",
    "                total_images += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image {i}: {e}\")\n",
    "                \n",
    "    print(f\"Inference Complete. Saved {total_images} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "\t\tprint(f'‚ö† No images found in {inference_dir}')\n",
    "\t\tprint('Please run the inference cell first!')\n",
    "else:\n",
    "\t\t# Randomly sample 20 images\n",
    "\t\tnp.random.seed(42)  # For reproducibility\n",
    "\t\tnum_samples = min(20, len(image_files))\n",
    "\t\tsample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "\t\tsample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "\t\t# Create 4x5 grid\n",
    "\t\tfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\t\taxes = axes.flatten()\n",
    "\n",
    "\t\tfor idx, img_path in enumerate(sample_files):\n",
    "\t\t\t\t# Extract image ID from filename\n",
    "\t\t\t\timg_id = int(Path(img_path).stem.split('_')[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Find caption\n",
    "\t\t\t\tcaption_idx = np.where(test_ids == img_id)[0][0]\n",
    "\t\t\t\tcaption_ids = test_captions[caption_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode caption\n",
    "\t\t\t\tcaption_text = ''\n",
    "\t\t\t\tfor word_id in caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':\n",
    "\t\t\t\t\t\t\t\tcaption_text += word + ' '\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Load and display image\n",
    "\t\t\t\timg = plt.imread(img_path)\n",
    "\t\t\t\taxes[idx].imshow(img)\n",
    "\t\t\t\taxes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\t# Hide unused subplots if less than 20 images\n",
    "\t\tfor idx in range(num_samples, 20):\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\tprint(f'\\nTotal generated images: {len(image_files)}')\n",
    "\t\tprint(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-comp3)",
   "language": "python",
   "name": "dl-comp3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
