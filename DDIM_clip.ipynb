{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kdDsaGCeT_C"
   },
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcKxCGkCeT_I"
   },
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Text to Image</center></h1>\n",
    "\n",
    "<h2 id=\"Platform:-Kaggle\">Platform: <a href=\"https://www.kaggle.com/competitions/2025-datalab-cup-3-reverse-image-caption/overview\">Kaggle</a><a class=\"anchor-link\" href=\"#Platform:-Kaggle\">¶</a></h2>\n",
    "<h2 id=\"Overview\">Overview<a class=\"anchor-link\" href=\"#Overview\">¶</a></h2>\n",
    "<p>In this work, we are interested in translating text in the form of single-sentence human-written descriptions directly into image pixels. For example, \"<strong>this flower has petals that are yellow and has a ruffled stamen</strong>\" and \"<strong>this pink and yellow flower has a beautiful yellow center with many stamens</strong>\". You have to develop a novel deep architecture and GAN formulation to effectively translate visual concepts from characters to pixels.</p>\n",
    "\n",
    "<p>More specifically, given a set of texts, your task is to generate reasonable images with size 64x64x3 to illustrate the corresponding texts. Here we use <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford-102 flower dataset</a> and its <a href=\"https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view\">paired texts</a> as our training dataset.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/example.png\"/>\n",
    "\n",
    "<ul>\n",
    "<li>7370 images as training set, where each images is annotated with at most 10 texts.</li>\n",
    "<li>819 texts for testing. You must generate 1 64x64x3 image for each text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pKuTvS0eT_K"
   },
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN\">Conditional GAN<a class=\"anchor-link\" href=\"#Conditional-GAN\">¶</a></h2>\n",
    "<p>Given a text, in order to generate the image which can illustrate it, our model must meet several requirements:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Our model should have ability to understand and extract the meaning of given texts.<ul>\n",
    "<li>Use RNN or other language model, such as BERT, ELMo or XLNet, to capture the meaning of text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Our model should be able to generate image.<ul>\n",
    "<li>Use GAN to generate high quality image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>GAN-generated image should illustrate the text.<ul>\n",
    "<li>Use conditional-GAN to generate image conditioned on given text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information $y$. We can perform the conditioning by feeding $y$ into both the discriminator and generator as additional input layer.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/cGAN.png\" width=\"500\"/>\n",
    "\n",
    "<p>There are two motivations for using some extra information in a GAN model:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Improve GAN.</li>\n",
    "<li>Generate targeted image.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Additional information that is correlated with the input images, such as class labels, can be used to improve the GAN. This improvement may come in the form of more stable training, faster training, and/or generated images that have better quality.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/GANCLS.jpg\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKCxWc1jeqwu",
    "outputId": "53b625ed-5183-40d5-8e17-19a2deec847c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UHkMw2fZfA9I"
   },
   "outputs": [],
   "source": [
    "# BASE_PATH = '/content/drive/MyDrive/DL_datasets/Comp3/'\n",
    "BASE_PATH = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bv2atIhjeT_M"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_Y-rPtFeT_O",
    "outputId": "e3fa5b47-e458-4178-88f3-2496800f14a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuHu0EfdeT_P"
   },
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rt7lHfTyeT_Q",
    "outputId": "8276438e-b374-4866-dc25-444f3c9de4ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = BASE_PATH + '/dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n",
    "\n",
    "BATCH_SIZE = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qu2vtfyreT_Q",
    "outputId": "d2f7cdba-97b4-4ed3-fc3c-8f7429aa52fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using DistilBERT tokenizer (sent2IdList removed)\n"
     ]
    }
   ],
   "source": [
    "# This cell previously contained sent2IdList() function\n",
    "# It has been removed as we now use DistilBERT tokenizer instead\n",
    "# The id2word_dict is still available from cell 6 for visualization purposes\n",
    "\n",
    "print(\"✓ Using DistilBERT tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNKrOmtseT_R"
   },
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RuelprSeT_S",
    "outputId": "5286190e-2019-412d-8204-1ecfa972836c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = BASE_PATH + '/dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "BfBxQgXBeT_S",
    "outputId": "e1619403-9ace-43f5-e5b0-a4b5afbc143c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gVzuefyeT_T"
   },
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bc0MHBZ3eT_T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\DL_2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Load CLIP Tokenizer\n",
    "# \"openai/clip-vit-base-patch32\" is a standard, powerful model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def preprocess_text_clip(text, max_length=77):\n",
    "\t\tencoded = tokenizer(\n",
    "\t\t\t\ttext,\n",
    "\t\t\t\tpadding='max_length',\n",
    "\t\t\t\ttruncation=True,\n",
    "\t\t\t\tmax_length=max_length,\n",
    "\t\t\t\treturn_tensors='tf'\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t\t'input_ids': encoded['input_ids'],\n",
    "\t\t\t\t'attention_mask': encoded['attention_mask']\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mNZKjn9eT_T",
    "outputId": "8d9e272c-54f8-47bc-eecc-00009cabd694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DiffAugment functions loaded\n",
      "  Policies available: color, translation, cutout\n"
     ]
    }
   ],
   "source": [
    "def DiffAugment(x, policy='color,translation,cutout', channels_first=False, params=None):\n",
    "    \"\"\"\n",
    "    Differentiable augmentation for GANs\n",
    "\n",
    "    Args:\n",
    "        x: Input images [batch, H, W, C]\n",
    "        policy: Comma-separated augmentation policies\n",
    "        channels_first: If True, expects [batch, C, H, W]\n",
    "        params: Optional dict of pre-generated augmentation parameters for consistency\n",
    "\n",
    "    Returns:\n",
    "        Augmented images\n",
    "    \"\"\"\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            # TensorFlow format: [batch, H, W, C]\n",
    "            for p in policy.split(','):\n",
    "                for f in AUGMENT_FNS[p]:\n",
    "                    x = f(x, params)  # ← Pass params!\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x, params=None):\n",
    "    \"\"\"Random brightness adjustment\"\"\"\n",
    "    if params is not None and 'brightness' in params:\n",
    "        magnitude = params['brightness']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], -0.5, 0.5)\n",
    "    x = x + magnitude\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x, params=None):\n",
    "    \"\"\"Random saturation adjustment\"\"\"\n",
    "    if params is not None and 'saturation' in params:\n",
    "        magnitude = params['saturation']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], 0.0, 2.0)\n",
    "    x_mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x, params=None):\n",
    "    \"\"\"Random contrast adjustment\"\"\"\n",
    "    if params is not None and 'contrast' in params:\n",
    "        magnitude = params['contrast']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], 0.5, 1.5)\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return x\n",
    "\n",
    "def rand_translation(x, params=None, ratio=0.125):\n",
    "    \"\"\"Random translation (shift) - Fully vectorized for @tf.function\"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    image_size = tf.shape(x)[1]\n",
    "    shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "\n",
    "    # Random translation amounts for entire batch\n",
    "    if params is not None and 'translation_x' in params:\n",
    "        translation_x = params['translation_x']\n",
    "        translation_y = params['translation_y']\n",
    "    else:\n",
    "        translation_x = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "        translation_y = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "\n",
    "    def translate_single_image(args):\n",
    "        \"\"\"Translate a single image\"\"\"\n",
    "        img, tx, ty = args\n",
    "        img = tf.pad(img, [[shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "        img = tf.image.crop_to_bounding_box(img, shift + ty, shift + tx, image_size, image_size)\n",
    "        return img\n",
    "\n",
    "    # Use tf.map_fn (graph-mode compatible)\n",
    "    x_translated = tf.map_fn(\n",
    "        translate_single_image,\n",
    "        (x, translation_x, translation_y),\n",
    "        fn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "        parallel_iterations=10\n",
    "    )\n",
    "\n",
    "    return x_translated\n",
    "\n",
    "\n",
    "def rand_cutout(x, params=None, ratio=0.5):\n",
    "    \"\"\"\n",
    "    Random cutout - SIMPLIFIED vectorized version\n",
    "\n",
    "    Instead of complex per-pixel masking, we create rectangular masks\n",
    "    using broadcasting and boolean operations\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    image_size = tf.shape(x)[1]\n",
    "    channels = tf.shape(x)[3]\n",
    "\n",
    "    # Cutout size\n",
    "    cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "\n",
    "    # Random offset for cutout location\n",
    "    if params is not None and 'cutout_x' in params:\n",
    "        offset_x = params['cutout_x']\n",
    "        offset_y = params['cutout_y']\n",
    "    else:\n",
    "        offset_x = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "        offset_y = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "\n",
    "    def cutout_single_image(args):\n",
    "        \"\"\"Apply cutout to single image using simple slicing\"\"\"\n",
    "        img, ox, oy = args\n",
    "\n",
    "        # Create coordinate grids\n",
    "        height_range = tf.range(image_size)\n",
    "        width_range = tf.range(image_size)\n",
    "\n",
    "        # Create 2D grids\n",
    "        yy, xx = tf.meshgrid(height_range, width_range, indexing='ij')\n",
    "\n",
    "        # Create mask: True where we want to KEEP pixels\n",
    "        mask_y = tf.logical_or(yy < oy, yy >= oy + cutout_size)\n",
    "        mask_x = tf.logical_or(xx < ox, xx >= ox + cutout_size)\n",
    "        mask = tf.logical_or(mask_y, mask_x)\n",
    "\n",
    "        # Expand mask to all channels\n",
    "        mask = tf.expand_dims(mask, axis=-1)  # [H, W, 1]\n",
    "        mask = tf.tile(mask, [1, 1, channels])  # [H, W, C]\n",
    "\n",
    "        # Apply mask (convert bool to float)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return img * mask\n",
    "\n",
    "    # Use tf.map_fn\n",
    "    x_cutout = tf.map_fn(\n",
    "        cutout_single_image,\n",
    "        (x, offset_x, offset_y),\n",
    "        fn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "        parallel_iterations=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    return x_cutout\n",
    "\n",
    "\n",
    "# Augmentation function registry\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}\n",
    "\n",
    "\n",
    "print(\"✓ DiffAugment functions loaded\")\n",
    "print(\"  Policies available: color, translation, cutout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOlgnPmZeT_V"
   },
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def compute_embeddings(text_encoder, filenames):\n",
    "    all_captions = []\n",
    "    all_image_paths = []\n",
    "\n",
    "    files = pd.read_pickle(filenames)\n",
    "\n",
    "    for i in tqdm(range(len(files))):\n",
    "        caption_ids = random.choice(files['Captions'].iloc[i])\n",
    "        words = [id2word_dict[str(word_id)] for word_id in caption_ids if id2word_dict[str(word_id)] != '<PAD>']\n",
    "        caption_text = ' '.join(words)\n",
    "        all_captions.append(caption_text)\n",
    "\n",
    "        path = str(files['ImagePath'].iloc[i])\n",
    "        all_image_paths.append(path)\n",
    "    \n",
    "    batch_embedddings = []\n",
    "\n",
    "    for i in range (0, len(all_captions), BATCH_SIZE):\n",
    "        batch_texts = all_captions[i:i + BATCH_SIZE]\n",
    "        batch_encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "\n",
    "        input_ids = batch_encodings['input_ids']\n",
    "        attention_mask = batch_encodings['attention_mask']\n",
    "\n",
    "        embeddings = text_encoder(input_ids, attention_mask=attention_mask)\n",
    "        batch_embedddings.append(embeddings)\n",
    "      \n",
    "    final_embeddings = tf.concat(batch_embedddings, axis=0).numpy()\n",
    "    return all_image_paths, final_embeddings\n",
    "\n",
    "def training_data_generator(image_path, embeddings):\n",
    "    \"\"\"\n",
    "    Updated data generator using CLIP tokenization\n",
    "\n",
    "    Args:\n",
    "        caption_text: Raw text string (not IDs!)\n",
    "        image_path: Path to image file\n",
    "\n",
    "    Returns:\n",
    "        img, embeddings\n",
    "    \"\"\"\n",
    "    # ============= IMAGE PROCESSING (same as before) =============\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "\n",
    "    return img, embeddings\n",
    "\n",
    "def dataset_generator(filenames, batch_size, embeddings, data_generator):\n",
    "    \"\"\"\n",
    "    Updated dataset generator to work with raw text (decoded from IDs)\n",
    "    \"\"\"\n",
    "\n",
    "    # append BASE_PATH to image paths\n",
    "    processed_image_paths = []\n",
    "    for path in filenames:\n",
    "        path = str(path)\n",
    "        if path.startswith('./'):\n",
    "            # Remove './' and join with BASE_PATH\n",
    "            clean_path = path[2:]\n",
    "            full_path = os.path.join(BASE_PATH, clean_path)\n",
    "        else:\n",
    "            full_path = os.path.join(BASE_PATH, path)\n",
    "        processed_image_paths.append(full_path)\n",
    "\n",
    "    image_paths = processed_image_paths\n",
    "\n",
    "    # Verify same length\n",
    "    assert len(embeddings) == len(image_paths)\n",
    "\n",
    "    # Create dataset from raw text and image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, embeddings))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(len(embeddings)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "falnOG9CeT_W"
   },
   "source": [
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "64yDmLtQeT_W"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import TFCLIPTextModel, TFCLIPModel\n",
    "import math\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class ClipTextEncoder(tf.keras.Model):\n",
    "\t\tdef __init__(self, output_dim=512,freeze_clip=True):\n",
    "\t\t\t\tsuper(ClipTextEncoder, self).__init__()\n",
    "\t\t\t\t# Load Pre-trained CLIP Text Model\n",
    "\t\t\t\tself.clip = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\t\t\t\tif freeze_clip:\n",
    "\t\t\t\t\t\tself.clip.trainable = False\n",
    "\n",
    "\t\t\t# REMOVED: Projection, LayerNorm, Dropout to ensure RAW embeddings\n",
    "\n",
    "\t\tdef call(self, input_ids, attention_mask, training=False):\n",
    "\t\t\t# 1. Get the projected features (Aligned with images, e.g., 512-dim)\n",
    "\t\t\ttext_embeds = self.clip(\n",
    "\t\t\t\tinput_ids=input_ids,\n",
    "\t\t\t\tattention_mask=attention_mask,\n",
    "\t\t\t\ttraining=training\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# 2. CRITICAL FIX: Manually normalize to get the actual CLIP embedding\n",
    "\t\t\t# CLIP uses cosine similarity, so vectors must be unit length.\n",
    "\t\t\ttext_embeds = text_embeds.last_hidden_state\n",
    "\n",
    "\t\t\treturn text_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYQ18p1iDQDi"
   },
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xC6HCm5xeT_Z",
    "outputId": "84c3d3bd-a410-4650-a1ea-95d4d3c072df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters updated:\n",
      "  Batch size: 16\n",
      "  Learning rate: 0.0001\n",
      "  DiffAugment: False (translation,cutout)\n"
     ]
    }
   ],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,\n",
    "    'EMBED_DIM': 256,\n",
    "    'VOCAB_SIZE': len(word2Id_dict),\n",
    "    'RNN_HIDDEN_SIZE': 256,\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "\n",
    "    # ========== UPDATED FOR PHASE 1 ==========\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': 1e-4,\n",
    "    'BETA_1': 0.0,\n",
    "    'BETA_2': 0.9,\n",
    "\n",
    "    # ========== NEW: DIFFAUGMENT ==========\n",
    "    'USE_DIFFAUG': False,           # Enable DiffAugment\n",
    "    'DIFFAUG_POLICY': 'translation,cutout',  # Augmentation policies\n",
    "\n",
    "    # ========== OTHER ==========\n",
    "    'N_EPOCH': 1000,                # ← Extended from 100 (with early stopping)\n",
    "    'N_SAMPLE': num_training_sample,\n",
    "    'PRINT_FREQ': 2,\n",
    "\n",
    "    # ========== KID METRIC ==========\n",
    "    'KID_DIFFUSION_STEPS': 5,\n",
    "    'PLOT_DIFFUSION_STEPS': 50,\n",
    "\n",
    "    # ========== DDIM ==========\n",
    "    'WIDTHS' : [256, 512, 1024, 2048],\n",
    "    'BLOCK_DEPTH' : 2,\n",
    "    'NETWORK_DIM' : 256,\n",
    "    'EMBEDDING_MAX_FREQ' : 1000.0,\n",
    "    'DIFFUSION_STEPS' : 50,\n",
    "    'EMA' : 0.999,\n",
    "    'WEIGHT_DECAY' : 1e-4,\n",
    "\n",
    "    'MAX_SIGNAL_RATE' : 0.95,\n",
    "    'MIN_SIGNAL_RATE' : 0.02,\n",
    "}\n",
    "\n",
    "print(f\"✓ Hyperparameters updated:\")\n",
    "print(f\"  Batch size: {hparas['BATCH_SIZE']}\")\n",
    "print(f\"  Learning rate: {hparas['LR']}\")\n",
    "print(f\"  DiffAugment: {hparas['USE_DIFFAUG']} ({hparas['DIFFAUG_POLICY']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpDwzVK8DQDj"
   },
   "source": [
    "## Kernel Inception Distance (KID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tE8_WivgDQDj"
   },
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID is estimated per batch and is averaged across batches\n",
    "        self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)),\n",
    "                layers.Resizing(height=75, width=75),\n",
    "                layers.Rescaling(255.0),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(75, 75, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        # Use TensorFlow functions instead of ops\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=\"float32\")\n",
    "        return (tf.matmul(features_1, tf.transpose(features_2)) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(generated_features, generated_features)\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=\"float32\")\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(kernel_generated * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2BcRTlIeT_X"
   },
   "source": [
    "\n",
    "<h2 id=\"Generator\">Network Architecture<a class=\"anchor-link\" href=\"#Generator\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossAttention(x, context, num_heads, key_dim):\n",
    "    residual = x\n",
    "\n",
    "    _, h, w, c = x.shape\n",
    "\n",
    "    x_flat = layers.Reshape((h * w, c))(x)\n",
    "\n",
    "    # layer normalization\n",
    "    x_norm = layers.LayerNormalization()(x_flat)\n",
    "    context_norm = layers.LayerNormalization()(context)\n",
    "\n",
    "    # multi-head attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=key_dim,\n",
    "    )(query=x_norm, value=context_norm, key=context_norm)\n",
    "\n",
    "    # add and normalize\n",
    "    x_flat = layers.Add()([x_flat, attention_output])\n",
    "    x_norm = layers.LayerNormalization()(x_flat)\n",
    "\n",
    "    ffn = layers.Dense(c * 4, activation='gelu')(x_norm)\n",
    "    ffn = layers.Dense(c)(ffn)\n",
    "\n",
    "    x = layers.Add()([x_flat, ffn])\n",
    "    x = layers.Reshape((h, w, c))(x)\n",
    "\n",
    "    x = layers.Add()([x, residual])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Gv1UVlMfDQDj"
   },
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(layers.Layer):\n",
    "    \"\"\"Sinusoidal positional embedding layer\"\"\"\n",
    "    def __init__(self, embedding_dims=32, max_freq=1000.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.max_freq = max_freq\n",
    "        self.embedding_min_frequency = 1.0\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embedding_dims': self.embedding_dims,\n",
    "            'max_freq': self.max_freq,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x):\n",
    "        frequencies = tf.math.exp(\n",
    "            tf.linspace(\n",
    "                tf.math.log(self.embedding_min_frequency),\n",
    "                tf.math.log(self.max_freq),\n",
    "                self.embedding_dims // 2,\n",
    "            )\n",
    "        )\n",
    "        angular_speeds = tf.cast(2.0 * math.pi * frequencies, \"float32\")\n",
    "        embeddings = tf.concat(\n",
    "            [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def ResidualBlock(width, use_attention=False):\n",
    "    def apply(x, text_embedding=None):\n",
    "        input_width = x.shape[3]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
    "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\", activation=\"swish\")(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "\n",
    "        if use_attention and text_embedding is not None:\n",
    "            x = CrossAttention(x, text_embedding, num_heads=4, key_dim=width//4)\n",
    "        return x\n",
    "    \n",
    "    return apply\n",
    "\n",
    "\n",
    "def DownBlock(width, block_depth, use_attention=False):\n",
    "    def apply(x, text_embedding=None):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width, use_attention=use_attention)(x, text_embedding=text_embedding)\n",
    "            skips.append(x)\n",
    "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "        return x\n",
    "    return apply\n",
    "\n",
    "\n",
    "def UpBlock(width, block_depth, use_attention=False):\n",
    "    def apply(x, text_embedding=None):\n",
    "        x, skips = x\n",
    "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
    "        for _ in range(block_depth):\n",
    "            x = layers.Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(width, use_attention)(x, text_embedding)\n",
    "        return x\n",
    "    return apply\n",
    "\n",
    "\n",
    "def get_network(hparas):\n",
    "    # Extract parameters from hparas\n",
    "    image_size = hparas['IMAGE_SIZE'][0] # Assumes [64, 64, 3]\n",
    "    widths = hparas['WIDTHS']\n",
    "    block_depth = hparas['BLOCK_DEPTH']\n",
    "    embed_dim = hparas['NETWORK_DIM']\n",
    "    embed_max_freq = hparas['EMBEDDING_MAX_FREQ']\n",
    "\n",
    "    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n",
    "    noise_variances = keras.Input(shape=(1, 1, 1))\n",
    "\n",
    "    # Text Input: Matches the output of your Text Encoder (e.g., 512 for CLIP/T5)\n",
    "    t_emb = keras.Input(shape=(None, 512), dtype=\"float32\")\n",
    "\n",
    "    # Time Embedding - FIXED: Use custom layer instead of Lambda\n",
    "    e = SinusoidalEmbedding(embedding_dims=embed_dim, max_freq=embed_max_freq)(noise_variances)\n",
    "    e = layers.UpSampling2D(size=(image_size, image_size), interpolation=\"nearest\")(e)\n",
    "\n",
    "    # Initial Conv\n",
    "    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n",
    "\n",
    "    # CONCATENATE: Image + Time info + Text info\n",
    "    x = layers.Concatenate()([x, e])\n",
    "\n",
    "    skips = []\n",
    "    # Downsample\n",
    "    for i, width in enumerate(widths[:-1]):\n",
    "        use_attn = (i > 0)\n",
    "        x = DownBlock(width, block_depth, use_attention=use_attn)([x, skips], text_embedding=t_emb)\n",
    "\n",
    "    # Bottleneck\n",
    "    for _ in range(block_depth):\n",
    "        x = ResidualBlock(widths[-1], use_attention=True)(x, text_embedding=t_emb)\n",
    "\n",
    "    # Upsample\n",
    "    for i, width in enumerate(reversed(widths[:-1])):\n",
    "        use_attn = (i < len(widths) - 2)\n",
    "        x = UpBlock(width, block_depth, use_attention=use_attn)([x, skips], text_embedding=t_emb)\n",
    "        \n",
    "    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "\n",
    "    return keras.Model([noisy_images, noise_variances, t_emb], x, name=\"text_conditioned_unet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8ndhc9eT_Y"
   },
   "source": [
    "\n",
    "<h2 id=\"Discriminator\">DDIM<a class=\"anchor-link\" href=\"#Discriminator\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZWYwWh4DQDk"
   },
   "outputs": [],
   "source": [
    "class DiffusionModel(keras.Model):\n",
    "    def __init__(self, hparas):\n",
    "        super().__init__()\n",
    "        self.hparas = hparas\n",
    "        self.image_size = hparas['IMAGE_SIZE'][0]\n",
    "\n",
    "        # Initialize Network\n",
    "        self.normalizer = layers.Normalization()\n",
    "        self.network = get_network(hparas)\n",
    "        self.ema_network = keras.models.clone_model(self.network)\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n",
    "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
    "        self.kid = KID(name=\"kid_metric\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]\n",
    "\n",
    "    def denormalize(self, images):\n",
    "        # convert the pixel values back to 0-1 range\n",
    "        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n",
    "\n",
    "        return images\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        start_angle = tf.acos(self.hparas['MAX_SIGNAL_RATE'])\n",
    "        end_angle = tf.acos(self.hparas['MIN_SIGNAL_RATE'])\n",
    "\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates, text_embeddings, training):\n",
    "        if training:\n",
    "            network = self.network\n",
    "        else:\n",
    "            network = self.ema_network\n",
    "\n",
    "        # Predict noise using (Images, Time, Text)\n",
    "        pred_noises = network([noisy_images, noise_rates**2, text_embeddings], training=training)\n",
    "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps, text_embeddings):\n",
    "        # reverse diffusion = sampling\n",
    "        num_images = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        next_noisy_images = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            noisy_images = next_noisy_images\n",
    "\n",
    "            diffusion_times = tf.ones((num_images, 1, 1, 1))- step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "             \n",
    "            # Pass text_embeddings to denoise for conditional generation\n",
    "            pred_noises, pred_images = self.denoise(noisy_images, noise_rates, signal_rates, text_embeddings, training=False)\n",
    "\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(next_diffusion_times)\n",
    "            next_noisy_images = (\n",
    "\t\t\t\t\t\t\t\tnext_signal_rates * pred_images + next_noise_rates * pred_noises\n",
    "\t\t\t\t\t\t)\n",
    "\n",
    "        return pred_images\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data: images and text embeddings (pre-computed)\n",
    "        images, text_embeddings = data\n",
    "        \n",
    "        # normalize images to have standard deviation of 1, like the noises\n",
    "        images = self.normalizer(images, training=True)\n",
    "        noises = tf.random.normal(shape=(self.hparas['BATCH_SIZE'], IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(self.hparas['BATCH_SIZE'], 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # mix the images with noises accordingly\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # train the network to separate noisy images to their components\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, text_embeddings, training=True\n",
    "            )\n",
    "\n",
    "            noise_loss = self.loss(noises, pred_noises)  # used for training\n",
    "            image_loss = self.loss(images, pred_images)  # only used as metric\n",
    "\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "        self.image_loss_tracker.update_state(image_loss)\n",
    "\n",
    "        # track the exponential moving averages of weights\n",
    "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
    "            ema_weight.assign(self.hparas['EMA'] * ema_weight + (1 - self.hparas['EMA']) * weight)\n",
    "\n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics[:-1]}\n",
    "\n",
    "    # --- Generation Function (DDIM) ---\n",
    "    def generate(self, num_images, text_embeddings):\n",
    "        initial_noise = tf.random.normal(shape=(num_images, self.image_size, self.image_size, 3))\n",
    "        generated_images = self.reverse_diffusion(\n",
    "            initial_noise, diffusion_steps=self.hparas['PLOT_DIFFUSION_STEPS'], text_embeddings=text_embeddings\n",
    "        )\n",
    "        generated_images = self.denormalize(generated_images)\n",
    "        generated_images = tf.clip_by_value(generated_images, 0.0, 1.0)\n",
    "        return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhIQ87b-eT_Z",
    "outputId": "378b1fb5-4752-43a8-c3c0-346fe413c2f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Some layers from the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing TFCLIPTextModel: ['clip/vision_model/encoder/layers_._7/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._2/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._6/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._1/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._10/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._1/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._1/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._9/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/self_attn/k_proj/kernel:0', 'clip/text_projection/kernel:0', 'clip/vision_model/encoder/layers_._6/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._2/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._10/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._3/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._9/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._8/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._7/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._2/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._10/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._5/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._3/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._0/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._7/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._8/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._7/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._2/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._0/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._3/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._9/self_attn/q_proj/kernel:0', 'clip/vision_model/embeddings/position_embedding/embeddings:0', 'clip/vision_model/encoder/layers_._4/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._10/layer_norm1/beta:0', 'clip/vision_model/post_layernorm/beta:0', 'clip/vision_model/encoder/layers_._7/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._5/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._8/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._8/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._10/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._2/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._1/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._7/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._11/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._2/mlp/fc2/bias:0', 'clip/logit_scale:0', 'clip/vision_model/encoder/layers_._8/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._0/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._1/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._3/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._3/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._0/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._6/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._6/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._11/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._9/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._5/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._6/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._0/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._0/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._3/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._11/layer_norm1/gamma:0', 'clip/vision_model/pre_layrnorm/gamma:0', 'clip/vision_model/encoder/layers_._2/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._1/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._11/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._11/self_attn/out_proj/kernel:0', 'clip/vision_model/pre_layrnorm/beta:0', 'clip/vision_model/encoder/layers_._9/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._8/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._4/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._0/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._11/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._10/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._6/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._0/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._9/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._1/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._11/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._7/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._2/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._1/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._11/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._10/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._3/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._0/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._1/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._6/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._3/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._3/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._6/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._7/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._11/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._10/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._7/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._5/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._7/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._11/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._0/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._3/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._5/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._3/self_attn/out_proj/kernel:0', 'clip/vision_model/post_layernorm/gamma:0', 'clip/vision_model/encoder/layers_._0/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._10/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._2/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._3/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._0/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._9/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._2/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._2/self_attn/v_proj/bias:0', 'clip/visual_projection/kernel:0', 'clip/vision_model/encoder/layers_._8/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._6/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._9/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._6/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._8/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._8/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._5/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._4/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._11/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._9/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._5/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._9/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._7/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._5/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._10/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._1/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._9/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._9/layer_norm1/beta:0', 'clip/vision_model/embeddings/class_embedding:0', 'clip/vision_model/encoder/layers_._6/layer_norm2/beta:0', 'clip/vision_model/embeddings/patch_embedding/kernel:0', 'clip/vision_model/encoder/layers_._4/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._1/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._4/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._10/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._4/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._6/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._10/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._3/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._9/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._11/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._2/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._7/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._6/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._3/self_attn/out_proj/bias:0', 'clip/vision_model/encoder/layers_._3/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._0/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._5/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._7/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._11/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._4/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._5/mlp/fc2/bias:0', 'clip/vision_model/encoder/layers_._0/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._2/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._1/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._2/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._8/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._4/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._5/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._10/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._5/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._2/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._0/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._0/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._7/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._5/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._8/self_attn/q_proj/bias:0', 'clip/vision_model/encoder/layers_._10/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._5/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._5/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._1/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._7/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._2/layer_norm1/gamma:0', 'clip/vision_model/encoder/layers_._1/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._8/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._6/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._6/mlp/fc2/kernel:0', 'clip/vision_model/encoder/layers_._7/self_attn/v_proj/bias:0', 'clip/vision_model/encoder/layers_._10/self_attn/k_proj/bias:0', 'clip/vision_model/encoder/layers_._4/self_attn/out_proj/kernel:0', 'clip/vision_model/encoder/layers_._11/self_attn/k_proj/kernel:0', 'clip/vision_model/encoder/layers_._1/layer_norm1/beta:0', 'clip/vision_model/encoder/layers_._1/mlp/fc1/kernel:0', 'clip/vision_model/encoder/layers_._10/layer_norm2/beta:0', 'clip/vision_model/encoder/layers_._3/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._11/self_attn/q_proj/kernel:0', 'clip/vision_model/encoder/layers_._5/layer_norm2/gamma:0', 'clip/vision_model/encoder/layers_._9/self_attn/v_proj/kernel:0', 'clip/vision_model/encoder/layers_._6/mlp/fc1/bias:0', 'clip/vision_model/encoder/layers_._11/self_attn/v_proj/kernel:0']\n",
      "- This IS expected if you are initializing TFCLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFCLIPTextModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPTextModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "\n",
    "text_encoder = ClipTextEncoder(output_dim=hparas['RNN_HIDDEN_SIZ' \\\n",
    "'E'], freeze_clip=True)\n",
    "model = DiffusionModel(hparas)\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=hparas['LR'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2'],\n",
    "    weight_decay=hparas['WEIGHT_DECAY'],\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZSUB8ydeT_a"
   },
   "source": [
    "\n",
    "<h2 id=\"Loss-Function-and-Optimization\">Loss Function and Optimization<a class=\"anchor-link\" href=\"#Loss-Function-and-Optimization\">¶</a></h2>\n",
    "<p>Although the conditional GAN model is quite complex, the loss function used to optimize the network is relatively simple. Actually, it is simply a binary classification task, thus we use cross entropy as our loss.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created NEW run directory: ./runs/20251128-170928\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create new run\n",
    "run_timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "run_dir = f'{BASE_PATH}/runs/{run_timestamp}'\n",
    "\n",
    "# All outputs for this run go in subdirectories\n",
    "checkpoint_dir = f'{run_dir}/checkpoints'\n",
    "best_models_dir = f'{run_dir}/best_models'\n",
    "samples_dir = f'{run_dir}/samples'\n",
    "inference_dir = f'{run_dir}/inference'\n",
    "\n",
    "# Create all directories\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(best_models_dir, exist_ok=True)\n",
    "os.makedirs(samples_dir, exist_ok=True)\n",
    "os.makedirs(inference_dir, exist_ok=True)\n",
    "\n",
    "print(f'✓ Created NEW run directory: {run_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WASa9YjveT_b"
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(\n",
    "    network=model.network,\n",
    "    ema_network=model.ema_network,\n",
    "    optimizer=model.optimizer,\n",
    "    normalizer=model.normalizer\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "best_ckpt_manager = tf.train.CheckpointManager(checkpoint, best_models_dir, max_to_keep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhlDZifxeT_c"
   },
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtQAXhqWeT_c"
   },
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size))\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(epoch, sample_text, attention_mask, num_rows=2, num_cols=8):\n",
    "    # plot random generated images for visual evaluation of generation quality\n",
    "    \n",
    "    # Compute embeddings for the samples\n",
    "    plot_embeddings = text_encoder(sample_text, attention_mask, training=False)    \n",
    "    \n",
    "    generated_images = model.generate(\n",
    "        num_images=num_rows * num_cols,\n",
    "        text_embeddings=plot_embeddings\n",
    "    )\n",
    "\n",
    "    save_images(\n",
    "        generated_images,\n",
    "        (num_rows, num_cols),\n",
    "        f'{samples_dir}/epoch_{epoch+1}.png'\n",
    "    )\n",
    "    \n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hpfj9LUUeT_d"
   },
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyKgBCmFeT_d",
    "outputId": "fb3ca071-2bc5-49e7-9010-8404bcbf25e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample data created:\n",
      "  Batch size: 16\n",
      "  Grid size (ni): 8 × 2 = 16\n",
      "  Sample sentences: 16 sentences\n",
      "  sample_input_ids shape: (16, 64)\n",
      "  sample_attention_mask shape: (16, 64)\n",
      "  NOTE: DDIM generates from random noise (no fixed seed)\n",
      "✓ All dimensions match!\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for visualization during training\n",
    "\n",
    "sample_size = hparas['BATCH_SIZE']  # Current: 64\n",
    "ni = sample_size // 8  # Grid size for visualization\n",
    "\n",
    "# REMOVED: sample_seed (DDIM generates from random noise internally)\n",
    "\n",
    "# Define 8 diverse sample sentences\n",
    "base_sentences = [\n",
    "    \"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "    \"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "    \"the petals on this flower are white with a yellow center\",\n",
    "    \"this flower has a lot of small round pink petals.\",\n",
    "    \"this flower is orange in color, and has petals that are ruffled and rounded.\",\n",
    "    \"the flower has yellow petals and the center of it is brown.\",\n",
    "    \"this flower has petals that are blue and white.\",\n",
    "    \"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "]\n",
    "\n",
    "# Repeat sentences to match sample_size (batch size)\n",
    "sample_sentences = []\n",
    "for i in range(sample_size):\n",
    "    sample_sentences.append(base_sentences[i % len(base_sentences)])\n",
    "\n",
    "# Tokenize with CLIP\n",
    "sample_encoded = preprocess_text_clip(sample_sentences, max_length=64)\n",
    "sample_input_ids = sample_encoded['input_ids']\n",
    "sample_attention_mask = sample_encoded['attention_mask']\n",
    "\n",
    "# Verify all dimensions match!\n",
    "print(f\"✓ Sample data created:\")\n",
    "print(f\"  Batch size: {sample_size}\")\n",
    "print(f\"  Grid size (ni): 8 × {ni} = {8*ni}\")\n",
    "print(f\"  Sample sentences: {len(sample_sentences)} sentences\")\n",
    "print(f\"  sample_input_ids shape: {sample_input_ids.shape}\")\n",
    "print(f\"  sample_attention_mask shape: {sample_attention_mask.shape}\")\n",
    "print(f\"  NOTE: DDIM generates from random noise (no fixed seed)\")\n",
    "\n",
    "# Check for dimension mismatches\n",
    "assert len(sample_sentences) == sample_size, f\"Mismatch: {len(sample_sentences)} != {sample_size}\"\n",
    "assert sample_input_ids.shape[0] == sample_size, f\"Mismatch: {sample_input_ids.shape[0]} != {sample_size}\"\n",
    "assert sample_attention_mask.shape[0] == sample_size, f\"Mismatch: {sample_attention_mask.shape[0]} != {sample_size}\"\n",
    "print(\"✓ All dimensions match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HHraI8ueT_e"
   },
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-compute embeddings for the entire dataset\n",
    "img_files, embeddings = compute_embeddings(text_encoder, data_path + '/text2ImgData.pkl')\n",
    "\n",
    "# Create Dataset\n",
    "dataset = dataset_generator(img_files, BATCH_SIZE, embeddings, training_data_generator)\n",
    "print(\"✓ Pre-computed dataset created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.normalizer.adapt(dataset.map(lambda images, embeddings: images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, model):\n",
    "    print(f\"Starting custom training loop for {epochs} epochs...\")\n",
    "    print(f\"Run directory: {run_dir}\")\n",
    "\n",
    "    # TensorBoard\n",
    "    log_dir = f'{run_dir}/logs'\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    # Track best loss\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        pbar = tqdm(dataset, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        # Reset metrics\n",
    "        for metric in model.metrics:\n",
    "            metric.reset_state()\n",
    "\n",
    "        for batch in pbar:\n",
    "            # Train step\n",
    "            metrics = model.train_step(batch)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({k: f'{v:.4f}' for k, v in metrics.items()})\n",
    "            \n",
    "            # Log to TensorBoard (batch level)\n",
    "            with summary_writer.as_default():\n",
    "                for name, value in metrics.items():\n",
    "                    tf.summary.scalar(f'Batch/{name}', value, step=global_step)\n",
    "            global_step += 1\n",
    "            \n",
    "        # End of epoch\n",
    "        epoch_time = time.time() - start\n",
    "        \n",
    "        # Get epoch metrics\n",
    "        epoch_metrics = {m.name: m.result() for m in model.metrics}\n",
    "        current_loss = epoch_metrics.get('n_loss', float('inf'))\n",
    "        \n",
    "        print(f'Epoch {epoch+1} finished in {epoch_time:.2f}s. Loss: {current_loss:.4f}')\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        with summary_writer.as_default():\n",
    "            for name, value in epoch_metrics.items():\n",
    "                tf.summary.scalar(f'Epoch/{name}', value, step=epoch)\n",
    "                \n",
    "        # Save regular checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_path = ckpt_manager.save(checkpoint_number=epoch+1)\n",
    "            print(f'  ✓ Saved checkpoint: {save_path}')\n",
    "            \n",
    "        # Save best model\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            save_path = best_ckpt_manager.save(checkpoint_number=epoch+1)\n",
    "            print(f'  ⭐ New best model (Loss: {best_loss:.4f}) saved to: {save_path}')\n",
    "            \n",
    "        # Visualization\n",
    "        if (epoch + 1) % hparas.get('PRINT_FREQ', 5) == 0:\n",
    "             generated_images = plot_images(\n",
    "\t\t\t\t\t\t\t\t epoch,\n",
    "\t\t\t\t\t\t\t\t sample_input_ids,\n",
    "\t\t\t\t\t\t\t\t sample_attention_mask,\n",
    "\t\t\t\t\t\t\t\t num_rows=BATCH_SIZE // 8,\n",
    "\t\t\t\t\t\t\t\t num_cols=8\n",
    "\t\t\t\t\t\t )\n",
    "             \n",
    "             # Log to TensorBoard\n",
    "             with summary_writer.as_default():\n",
    "                 tf.summary.image('Generated_Samples', (generated_images), step=epoch, max_outputs=1)\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataset, hparas['N_EPOCH'], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NHEUsSkeT_6"
   },
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Evaluation</center></h1>\n",
    "\n",
    "<p><code>dataset/testData.pkl</code> is a pandas dataframe containing testing text with attributes 'ID' and 'Captions'.</p>\n",
    "\n",
    "<ul>\n",
    "<li>'ID': text ID used to name generated image.</li>\n",
    "<li>'Captions': text used as condition to generate image.</li>\n",
    "</ul>\n",
    "\n",
    "<p>For each captions, you need to generate <strong>inference_ID.png</strong> to evaluate quality of generated image. You must name the generated image in this format, otherwise we cannot evaluate your images.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7DqZsEVeT_7"
   },
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "x1EHIB5LeT_7"
   },
   "outputs": [],
   "source": [
    "def testing_compute_embeddings(text_encoder, captions_file):\n",
    "    all_captions = []\n",
    "    all_index = []\n",
    "\n",
    "    files = pd.read_pickle(captions_file)\n",
    "    \n",
    "    for i in tqdm(range(len(files))):\n",
    "        caption_ids = files['Captions'].iloc[i]\n",
    "        words = [id2word_dict[str(word_id)] for word_id in caption_ids if id2word_dict[str(word_id)] != '<PAD>']\n",
    "        caption_text = ' '.join(words)\n",
    "        all_captions.append(caption_text)\n",
    "\n",
    "        idx = files['ID'].iloc[i]\n",
    "        all_index.append(idx)\n",
    "\t\t\t\n",
    "    np.asarray(all_index)\n",
    "    \n",
    "    batch_embedddings = []\n",
    "\n",
    "    for i in range (0, len(all_captions), BATCH_SIZE):\n",
    "        batch_texts = all_captions[i:i + BATCH_SIZE]\n",
    "        batch_encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "\n",
    "        input_ids = batch_encodings['input_ids']\n",
    "        attention_mask = batch_encodings['attention_mask']\n",
    "\n",
    "        embeddings = text_encoder(input_ids, attention_mask=attention_mask)\n",
    "        batch_embedddings.append(embeddings)\n",
    "      \n",
    "    final_embeddings = tf.concat(batch_embedddings, axis=0).numpy()\n",
    "    return final_embeddings, all_index\n",
    "\n",
    "def testing_dataset_generator(batch_size, embeddings, index):\n",
    "    \"\"\"\n",
    "    Updated testing dataset generator - decodes IDs to raw text\n",
    "    \"\"\"\n",
    "\n",
    "    # Create dataset from raw text\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((embeddings, index))\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "g_BYuJ2QeT_7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819/819 [00:00<00:00, 13283.48it/s]\n"
     ]
    }
   ],
   "source": [
    "testing_embeddings, testing_index = testing_compute_embeddings(text_encoder, data_path + '/testData.pkl')\n",
    "\n",
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_embeddings, testing_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZQBZi8ySeT_8"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(BASE_PATH + '/dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8qyIerPeT_8"
   },
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnkWuKb2eT_8"
   },
   "outputs": [],
   "source": [
    "# Restore BEST MODEL for inference\n",
    "testing_dir = run_dir\n",
    "testing_models_dir = best_models_dir\n",
    "testing_inference_dir = inference_dir\n",
    "\n",
    "# Or specify a different run directory for testing\n",
    "# testing_dir = BASE_PATH + '/runs/20251127-120843/'\n",
    "# testing_models_dir = testing_dir + '/best_models'\n",
    "# testing_inference_dir = testing_dir + '/inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3DvCI7_eT_8",
    "outputId": "de09085b-91bd-49cf-c5be-21075fb6f4ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for BEST model in: ./runs/20251127-120843//best_models\n"
     ]
    },
    {
     "ename": "OpError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DL_2\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:66\u001b[0m, in \u001b[0;36mget_tensor\u001b[1;34m(self, tensor_str)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpointReader_GetTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOpError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlatest_checkpoint(testing_models_dir)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_checkpoint:\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_checkpoint\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpect_partial()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m✓ Restored BEST model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DL_2\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py:2563\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   2560\u001b[0m   save_path \u001b[38;5;241m=\u001b[39m utils_impl\u001b[38;5;241m.\u001b[39mget_variables_path(save_path)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2563\u001b[0m   status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2564\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m   2565\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()  \u001b[38;5;66;03m# Ensure restore operations have completed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DL_2\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py:2441\u001b[0m, in \u001b[0;36mCheckpoint.read\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   2439\u001b[0m   save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(save_path)\n\u001b[0;32m   2440\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;129;01mor\u001b[39;00m checkpoint_options\u001b[38;5;241m.\u001b[39mCheckpointOptions()\n\u001b[1;32m-> 2441\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2442\u001b[0m metrics\u001b[38;5;241m.\u001b[39mAddCheckpointReadDuration(\n\u001b[0;32m   2443\u001b[0m     api_label\u001b[38;5;241m=\u001b[39m_CHECKPOINT_V2,\n\u001b[0;32m   2444\u001b[0m     microseconds\u001b[38;5;241m=\u001b[39m_get_duration_microseconds(start_time, time\u001b[38;5;241m.\u001b[39mtime()))\n\u001b[0;32m   2445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DL_2\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py:1455\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[1;34m(self, save_path, options)\u001b[0m\n\u001b[0;32m   1453\u001b[0m   dtype_map \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mget_variable_to_dtype_map()\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1455\u001b[0m   object_graph_string \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJECT_GRAPH_PROTO_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError:\n\u001b[0;32m   1457\u001b[0m   \u001b[38;5;66;03m# The object graph proto does not exist in this checkpoint. Try the\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m   \u001b[38;5;66;03m# name-based compatibility mode.\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m   restore_coordinator \u001b[38;5;241m=\u001b[39m _NameBasedRestoreCoordinator(\n\u001b[0;32m   1460\u001b[0m       save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[0;32m   1461\u001b[0m       dtype_map\u001b[38;5;241m=\u001b[39mdtype_map)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DL_2\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:71\u001b[0m, in \u001b[0;36mget_tensor\u001b[1;34m(self, tensor_str)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 71\u001b[0m   \u001b[43merror_translator\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DL_2\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:45\u001b[0m, in \u001b[0;36merror_translator\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     43\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mInternalError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mOpError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message, errors_impl\u001b[38;5;241m.\u001b[39mUNKNOWN)\n",
      "\u001b[1;31mOpError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Looking for BEST model in: {testing_models_dir}')\n",
    "\n",
    "# Re-create checkpoint object (must match training definition)\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    network=model.network,\n",
    "    ema_network=model.ema_network,\n",
    "    optimizer=model.optimizer,\n",
    "    normalizer=model.normalizer\n",
    ")\n",
    "  \n",
    "best_checkpoint = tf.train.latest_checkpoint(testing_models_dir)\n",
    "if best_checkpoint:\n",
    "    checkpoint.restore(best_checkpoint).expect_partial()\n",
    "    print(f'✓ Restored BEST model: {best_checkpoint}')\n",
    "else:\n",
    "    print('⚠ No best model found, trying regular checkpoints...')\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "        print(f'✓ Restored latest checkpoint: {latest_checkpoint}')\n",
    "    else:\n",
    "        print('⚠ No checkpoint found at all, using fresh/untrained model')\n",
    "\n",
    "model.normalizer.adapt(dataset.map(lambda image, embeddings: image))\n",
    "print(\"Normalizer mean: \", model.normalizer.mean.numpy())\n",
    "print(\"Normalizer variance: \", model.normalizer.variance.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-zQ2dI7eT_9"
   },
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    \"\"\"\n",
    "    Generate 64x64x3 images for all test samples using DDIM\n",
    "    Each batch generates unique images from random noise\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    total_images = 0\n",
    "\n",
    "    # Progress bar for inference\n",
    "    pbar = tqdm(total=NUM_TEST, desc='Generating 64x64x3 images', unit='img')\n",
    "\n",
    "    # Unpack 3 values: input_ids, attention_mask, idx\n",
    "    for embeddings, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "\n",
    "        # Generate images (DDIM generates fresh noise internally)\n",
    "        fake_image = model.generate(\n",
    "            num_images=hparas['BATCH_SIZE'],\n",
    "            text_embeddings=embeddings\n",
    "        )\n",
    "        step += 1\n",
    "\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "          # save_images(image, (1, 1), f'{testing_inference_dir}/inference_{idx[i]:04d}.jpg')\n",
    "          img = fake_image[i].numpy()\n",
    "          plt.imsave(f'{testing_inference_dir}/inference_{idx[i]:04d}.jpg', img)\n",
    "          total_images += 1\n",
    "          pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    print(f'\\n✓ Generated {total_images} images (64x64x3) in {time.time()-start:.4f} sec')\n",
    "    print(f'✓ Images saved to: {testing_inference_dir}')\n",
    "    print(f'✓ Each batch generated with unique random noise for diversity!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== for verification, generate images for test set ====\n",
    "generated_images = plot_images(\n",
    "    10000,\n",
    "    sample_input_ids,\n",
    "    sample_attention_mask,\n",
    "    num_rows=BATCH_SIZE // 8,\n",
    "    num_cols=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OheUNG8yeT_9",
    "outputId": "700f80c4-938f-4954-fb40-3e7a4463b413"
   },
   "outputs": [],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvU01nAHeT_9",
    "outputId": "0e87ccf7-7eb2-4c9f-9428-117961bb2a56"
   },
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", testing_inference_dir, \"with\", testing_dir)\n",
    "!cd testing && python inception_score.py ../{testing_inference_dir}/ ../{testing_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTRFBnNyeT_-"
   },
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d9To9BTeT__"
   },
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4R3tn88OeT__",
    "outputId": "264b4c19-c931-44a0-fb1f-faf0bfc4356a"
   },
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "inference_dir = testing_inference_dir\n",
    "# Load test data\n",
    "data = pd.read_pickle(BASE_PATH + '/dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(f'⚠ No images found in {inference_dir}')\n",
    "    print('Please run the inference cell first!')\n",
    "else:\n",
    "    # Randomly sample 20 images\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    num_samples = min(20, len(image_files))\n",
    "    sample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "    sample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "    # Create 4x5 grid\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, img_path in enumerate(sample_files):\n",
    "        # Extract image ID from filename\n",
    "        img_id = int(Path(img_path).stem.split('_')[1])\n",
    "\n",
    "        # Find caption\n",
    "        caption_idx = np.where(test_ids == img_id)[0][0]\n",
    "        caption_ids = test_captions[caption_idx]\n",
    "\n",
    "        # Decode caption\n",
    "        caption_text = ''\n",
    "        for word_id in caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':\n",
    "                caption_text += word + ' '\n",
    "\n",
    "        # Load and display image\n",
    "        img = plt.imread(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    # Hide unused subplots if less than 20 images\n",
    "    for idx in range(num_samples, 20):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nTotal generated images: {len(image_files)}')\n",
    "    print(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
