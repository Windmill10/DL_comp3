{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttry:\n",
    "\t\t# Restrict TensorFlow to only use the first GPU\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\t\t# Currently, memory growth needs to be the same across GPUs\n",
    "\t\tfor gpu in gpus:\n",
    "\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\n",
    "\t\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\t\tprint(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\texcept RuntimeError as e:\n",
    "\t\t# Memory growth must be set before GPUs have been initialized\n",
    "\t\tprint(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ Using CLIP tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "# from transformers import CLIPTokenizer # Removed CLIP\n",
    "\n",
    "# Load CLIP Tokenizer - REMOVED\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def preprocess_text_clip(text, max_length=77):\n",
    "    # Placeholder or Removed\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. DATASET GENERATOR (Restored to use ID lists directly)\n",
    "# ==============================================================================\n",
    "\n",
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "MAX_SEQ_LENGTH = 20  # From Preprocess Text section\n",
    "\n",
    "def training_data_generator(caption_ids, image_path):\n",
    "    \"\"\"\n",
    "    Data generator using pre-processed ID lists (No CLIP tokenization)\n",
    "    \n",
    "    Args:\n",
    "        caption_ids: List of integer IDs (padded to MAX_SEQ_LENGTH)\n",
    "        image_path: Path to image file\n",
    "    \n",
    "    Returns:\n",
    "        img, caption_ids\n",
    "    \"\"\"\n",
    "    # ============= IMAGE PROCESSING =============\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    # Normalize to [-1, 1] to match generator's tanh output\n",
    "    img = (img * 2.0) - 1.0\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    # ============= TEXT PROCESSING =============\n",
    "    # caption_ids is already a tensor of IDs\n",
    "    caption_ids = tf.cast(caption_ids, tf.int32)\n",
    "    caption_ids.set_shape([MAX_SEQ_LENGTH])\n",
    "    \n",
    "    return img, caption_ids\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    \"\"\"\n",
    "    Dataset generator that loads pre-tokenized IDs directly\n",
    "    \"\"\"\n",
    "    # Load the training data\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions_ids = df['Captions'].values\n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    # Flatten the list of lists (choose one caption per image per epoch)\n",
    "    # But for tf.data, we need a fixed structure. \n",
    "    # Strategy: Create a dataset of (all_captions_list, image_path)\n",
    "    # and use a py_function to randomly select one caption inside the map.\n",
    "    \n",
    "    # However, to keep it simple and efficient like the template:\n",
    "    # We will expand the dataset or just pick one random caption per image *before* creating the dataset\n",
    "    # For better diversity, let's pick one random caption per image in Python\n",
    "    \n",
    "    chosen_captions = []\n",
    "    for caps in captions_ids:\n",
    "        # caps is a list of lists of IDs\n",
    "        chosen = random.choice(caps)\n",
    "        # Ensure padding to MAX_SEQ_LENGTH (20)\n",
    "        # The preprocessing description says they are already padded, but let's be safe\n",
    "        if len(chosen) < MAX_SEQ_LENGTH:\n",
    "            chosen = chosen + [word2Id_dict['<PAD>']] * (MAX_SEQ_LENGTH - len(chosen))\n",
    "        else:\n",
    "            chosen = chosen[:MAX_SEQ_LENGTH]\n",
    "        chosen_captions.append(chosen)\n",
    "        \n",
    "    chosen_captions = np.array(chosen_captions, dtype=np.int32)\n",
    "    \n",
    "    # Verify same length\n",
    "    assert len(chosen_captions) == len(image_paths)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((chosen_captions, image_paths))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(len(chosen_captions)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¶</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. RNN TEXT ENCODER (Replaces CLIP)\n",
    "# ==============================================================================\n",
    "# Based on DAMSM.py provided by user\n",
    "# Architecture: Embedding -> Dropout -> Bidirectional LSTM\n",
    "\n",
    "class RNN_Encoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Bi-Directional LSTM Text Encoder.\n",
    "    Matches DAMSM.py architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, ntoken, ninput=300, nhidden=256, nlayers=1, drop_prob=0.5):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.nhidden = nhidden // 2  # Because bidirectional doubles it\n",
    "        self.ninput = ninput\n",
    "        self.nlayers = nlayers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        # Embedding: vocab_size -> 300\n",
    "        self.embedding = layers.Embedding(ntoken, ninput,\n",
    "                                        embeddings_initializer=tf.initializers.RandomUniform(-0.1, 0.1))\n",
    "        self.drop = layers.Dropout(drop_prob)\n",
    "        \n",
    "        # Bi-LSTM: outputs nhidden*2 = 256\n",
    "        self.rnn = layers.Bidirectional(\n",
    "            layers.LSTM(self.nhidden, return_sequences=True, return_state=True, dropout=drop_prob)\n",
    "        )\n",
    "\n",
    "    def call(self, captions, cap_lens=None, training=False):\n",
    "        # captions: [B, Max_Seq_Len]\n",
    "        emb = self.embedding(captions)\n",
    "        emb = self.drop(emb, training=training)\n",
    "        \n",
    "        # Create mask if cap_lens provided\n",
    "        if cap_lens is not None:\n",
    "            mask = tf.sequence_mask(cap_lens, maxlen=tf.shape(captions)[1])\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        # RNN Forward\n",
    "        # output: [B, Seq, Hidden*2]\n",
    "        # states: forward_h, forward_c, backward_h, backward_c\n",
    "        output, f_h, f_c, b_h, b_c = self.rnn(emb, mask=mask, training=training)\n",
    "        \n",
    "        # Words Embedding: [B, Hidden*2, Seq]\n",
    "        # Transpose to match official PyTorch output [B, Hidden*2, Seq]\n",
    "        words_emb = tf.transpose(output, [0, 2, 1])\n",
    "        \n",
    "        # Sentence Embedding: [B, Hidden*2]\n",
    "        # Concatenate final hidden states of forward and backward\n",
    "        sent_emb = tf.concat([f_h, b_h], axis=1)\n",
    "        \n",
    "        return words_emb, sent_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, initializers\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. INITIALIZATION (MATCHING PYTORCH DEFAULTS)\n",
    "# ==============================================================================\n",
    "# PyTorch uses Uniform(-std, std) where std = 1/sqrt(fan_in).\n",
    "# To match this in TensorFlow's VarianceScaling:\n",
    "# limit = sqrt(3 * scale / n)  <-- TF formula\n",
    "# limit = sqrt(1 / n)          <-- PyTorch target\n",
    "# 3 * scale = 1  =>  scale = 1/3\n",
    "pytorch_init = tf.keras.initializers.VarianceScaling(scale=1.0/3.0, mode='fan_in', distribution='uniform')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER LAYERS & BLOCKS\n",
    "# ==============================================================================\n",
    "\n",
    "class Affine(layers.Layer):\n",
    "    \"\"\"\n",
    "    PyTorch: class Affine(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, num_features):\n",
    "        super(Affine, self).__init__()\n",
    "        \n",
    "        # PyTorch Default Init for the first Linear layer\n",
    "        self.fc_gamma = tf.keras.Sequential([\n",
    "            layers.Dense(num_features, activation='relu', kernel_initializer=pytorch_init),\n",
    "            layers.Dense(num_features, kernel_initializer='zeros', bias_initializer='ones')\n",
    "        ])\n",
    "\n",
    "        self.fc_beta = tf.keras.Sequential([\n",
    "            layers.Dense(num_features, activation='relu', kernel_initializer=pytorch_init),\n",
    "            layers.Dense(num_features, kernel_initializer='zeros', bias_initializer='zeros')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x: [B, H, W, C], y: [B, Cond_Dim]\n",
    "        x, y = inputs \n",
    "\n",
    "        # --- Gamma & Beta Calculation ---\n",
    "        weight = self.fc_gamma(y)\n",
    "        bias = self.fc_beta(y)\n",
    "\n",
    "        # --- Reshape for Broadcasting ---\n",
    "        weight = tf.reshape(weight, [-1, 1, 1, x.shape[-1]])\n",
    "        bias = tf.reshape(bias, [-1, 1, 1, x.shape[-1]])\n",
    "\n",
    "        return weight * x + bias\n",
    "\n",
    "\n",
    "class DFBLK(layers.Layer):\n",
    "    \"\"\"\n",
    "    PyTorch: class DFBLK(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch):\n",
    "        super(DFBLK, self).__init__()\n",
    "        self.affine0 = Affine(cond_dim, in_ch)\n",
    "        self.affine1 = Affine(cond_dim, in_ch)\n",
    "        self.act = layers.LeakyReLU(0.2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        \n",
    "        h = self.affine0([x, y])\n",
    "        h = self.act(h)\n",
    "        \n",
    "        h = self.affine1([h, y])\n",
    "        h = self.act(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "class G_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    PyTorch: class G_Block(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch, out_ch, upsample=True):\n",
    "        super(G_Block, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.learnable_sc = (in_ch != out_ch)\n",
    "        \n",
    "        # PyTorch Default Init\n",
    "        self.c1 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer=pytorch_init)\n",
    "        self.c2 = layers.Conv2D(out_ch, 3, strides=1, padding='same', kernel_initializer=pytorch_init)\n",
    "        \n",
    "        self.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        \n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid', kernel_initializer=pytorch_init)\n",
    "\n",
    "        self.upsample_layer = layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        \n",
    "        # --- Upsample ---\n",
    "        if self.upsample:\n",
    "            x = self.upsample_layer(x)\n",
    "\n",
    "        # --- Shortcut Path ---\n",
    "        h_sc = x\n",
    "        if self.learnable_sc:\n",
    "            h_sc = self.c_sc(h_sc)\n",
    "\n",
    "        # --- Residual Path ---\n",
    "        h_res = self.fuse1([x, y])\n",
    "        h_res = self.c1(h_res)\n",
    "        h_res = self.fuse2([h_res, y])\n",
    "        h_res = self.c2(h_res)\n",
    "\n",
    "        return h_sc + h_res\n",
    "\n",
    "\n",
    "class D_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    PyTorch: class D_Block(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, fin, fout, downsample=True):\n",
    "        super(D_Block, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        \n",
    "        # PyTorch Default Init\n",
    "        self.conv_r_1 = layers.Conv2D(fout, 4, strides=2, padding='same', use_bias=False, kernel_initializer=pytorch_init)\n",
    "        self.act_1 = layers.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv_r_2 = layers.Conv2D(fout, 3, strides=1, padding='same', use_bias=False, kernel_initializer=pytorch_init)\n",
    "        self.act_2 = layers.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv_s = layers.Conv2D(fout, 1, strides=1, padding='valid', kernel_initializer=pytorch_init)\n",
    "        \n",
    "        # Official: gamma=0.0\n",
    "        self.gamma = tf.Variable(0.0, trainable=True, dtype=tf.float32)\n",
    "        \n",
    "        self.avg_pool = layers.AveragePooling2D(pool_size=2, strides=2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # --- Residual Path ---\n",
    "        res = self.conv_r_1(x)\n",
    "        res = self.act_1(res)\n",
    "        res = self.conv_r_2(res)\n",
    "        res = self.act_2(res)\n",
    "        \n",
    "        # --- Shortcut Path ---\n",
    "        if self.learned_shortcut:\n",
    "            x = self.conv_s(x)\n",
    "            \n",
    "        if self.downsample:\n",
    "            x = self.avg_pool(x)\n",
    "            \n",
    "        return x + self.gamma * res\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MAIN NETWORKS (NetG, NetD, NetC)\n",
    "# ==============================================================================\n",
    "\n",
    "class NetG(Model):\n",
    "    \"\"\"\n",
    "    PyTorch: class NetG(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, ngf=32, nz=100, cond_dim=256, imsize=64, ch_size=3):\n",
    "        super(NetG, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.nz = nz\n",
    "        \n",
    "        self.fc = layers.Dense(ngf * 8 * 4 * 4, kernel_initializer=pytorch_init)\n",
    "        \n",
    "        self.block1 = G_Block(cond_dim + nz, ngf * 8, ngf * 8, upsample=True)\n",
    "        self.block2 = G_Block(cond_dim + nz, ngf * 8, ngf * 4, upsample=True)\n",
    "        self.block3 = G_Block(cond_dim + nz, ngf * 4, ngf * 2, upsample=True)\n",
    "        self.block4 = G_Block(cond_dim + nz, ngf * 2, ngf * 1, upsample=True)\n",
    "        \n",
    "        self.to_rgb_act = layers.LeakyReLU(0.2)\n",
    "        self.to_rgb_conv = layers.Conv2D(ch_size, 3, strides=1, padding='same', kernel_initializer=pytorch_init)\n",
    "        self.to_rgb_out = layers.Activation('tanh')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        noise, c = inputs\n",
    "        \n",
    "        out = self.fc(noise)\n",
    "        out = tf.reshape(out, [-1, 4, 4, self.ngf * 8])\n",
    "        cond = tf.concat([noise, c], axis=1)\n",
    "        \n",
    "        out = self.block1([out, cond])\n",
    "        out = self.block2([out, cond])\n",
    "        out = self.block3([out, cond])\n",
    "        out = self.block4([out, cond])\n",
    "        \n",
    "        out = self.to_rgb_act(out)\n",
    "        out = self.to_rgb_conv(out)\n",
    "        out = self.to_rgb_out(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class NetD(Model):\n",
    "    \"\"\"\n",
    "    PyTorch: class NetD(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, ndf=64, imsize=64, ch_size=3):\n",
    "        super(NetD, self).__init__()\n",
    "        \n",
    "        self.conv_img = layers.Conv2D(ndf, 3, strides=1, padding='same', kernel_initializer=pytorch_init)\n",
    "        \n",
    "        self.block1 = D_Block(ndf, ndf * 2, downsample=True)\n",
    "        self.block2 = D_Block(ndf * 2, ndf * 4, downsample=True)\n",
    "        self.block3 = D_Block(ndf * 4, ndf * 8, downsample=True)\n",
    "        self.block4 = D_Block(ndf * 8, ndf * 8, downsample=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        out = self.conv_img(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class NetC(Model):\n",
    "    \"\"\"\n",
    "    PyTorch: class NetC(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, ndf=64, cond_dim=256):\n",
    "        super(NetC, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        \n",
    "        self.joint_conv_1 = layers.Conv2D(ndf * 2, 3, strides=1, padding='same', use_bias=False, kernel_initializer=pytorch_init)\n",
    "        self.act = layers.LeakyReLU(0.2)\n",
    "        self.joint_conv_2 = layers.Conv2D(1, 4, strides=1, padding='valid', use_bias=False, kernel_initializer=pytorch_init)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out, y = inputs\n",
    "        y = tf.reshape(y, [-1, 1, 1, self.cond_dim])\n",
    "        y = tf.tile(y, [1, 4, 4, 1])\n",
    "        h_c_code = tf.concat([out, y], axis=-1)\n",
    "        \n",
    "        out = self.joint_conv_1(h_c_code)\n",
    "        out = self.act(out)\n",
    "        out = self.joint_conv_2(out)\n",
    "        \n",
    "        return tf.reshape(out, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def matching_aware_gradient_penalty(netD, netC, real_images, text_embeddings, p=6.0):\n",
    "    \"\"\"\n",
    "    Calculates the Matching Aware Gradient Penalty (MA-GP).\n",
    "    \n",
    "    Args:\n",
    "        netD: The Discriminator model (outputs features).\n",
    "        netC: The Compressor/Classifier model (takes features + text, outputs score).\n",
    "        real_images: Batch of real images [B, 64, 64, 3].\n",
    "        text_embeddings: Batch of matching text embeddings [B, Cond_Dim].\n",
    "        p: The power to raise the gradient norm to. Official DF-GAN uses p=6.\n",
    "        \n",
    "    Returns:\n",
    "        The gradient penalty scalar (averaged over batch).\n",
    "    \"\"\"\n",
    "    # 1. Watch BOTH real_images and text_embeddings\n",
    "    # Official DF-GAN penalizes gradients w.r.t both modalities\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(real_images)\n",
    "        tape.watch(text_embeddings)\n",
    "        \n",
    "        # 2. Forward pass through Discriminator and NetC\n",
    "        # Get features from image\n",
    "        features = netD(real_images, training=True)\n",
    "        \n",
    "        # Get scalar score from features + matching text\n",
    "        # NetC inputs are [features, text_embeddings]\n",
    "        pred_real = netC([features, text_embeddings], training=True)\n",
    "        \n",
    "    # 3. Calculate gradients of the prediction w.r.t BOTH inputs\n",
    "    grads = tape.gradient(pred_real, [real_images, text_embeddings])\n",
    "    grad_img = grads[0]\n",
    "    grad_text = grads[1]\n",
    "    \n",
    "    # 4. Flatten and Concatenate gradients\n",
    "    # Flatten: [B, -1]\n",
    "    grad_img_flat = tf.reshape(grad_img, [tf.shape(grad_img)[0], -1])\n",
    "    grad_text_flat = tf.reshape(grad_text, [tf.shape(grad_text)[0], -1])\n",
    "    \n",
    "    # Concatenate: [B, dim_img + dim_text]\n",
    "    grad_all = tf.concat([grad_img_flat, grad_text_flat], axis=1)\n",
    "    \n",
    "    # 5. Calculate L2 norm of the combined gradients\n",
    "    grad_norms = tf.norm(grad_all, axis=1)\n",
    "    \n",
    "    # 6. Calculate Penalty: 2.0 * E[||grad||^p]\n",
    "    # Official implementation includes a factor of 2.0\n",
    "    penalty = 2.0 * tf.reduce_mean(tf.pow(grad_norms, p))\n",
    "    \n",
    "    return penalty\n",
    "\n",
    "def discriminator_hinge_loss(real_score, fake_score, wrong_score=None):\n",
    "    \"\"\"\n",
    "    Hinge Loss for Discriminator.\n",
    "    L_D = E[max(0, 1 - D(real, text))] + E[max(0, 1 + D(fake, text))] \n",
    "          + (Optional) E[max(0, 1 + D(real, mismatch_text))]\n",
    "    \"\"\"\n",
    "    # Real Image + Matching Text: Should be > 1\n",
    "    real_loss = tf.reduce_mean(tf.nn.relu(1.0 - real_score))\n",
    "    \n",
    "    # Fake Image + Matching Text: Should be < -1\n",
    "    fake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake_score))\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    # (Optional) Real Image + Mismatched Text: Should be < -1\n",
    "    if wrong_score is not None:\n",
    "        wrong_loss = tf.reduce_mean(tf.nn.relu(1.0 + wrong_score))\n",
    "        total_loss += wrong_loss\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "def generator_hinge_loss(fake_score):\n",
    "    \"\"\"\n",
    "    Hinge Loss for Generator.\n",
    "    L_G = -E[D(fake, text)]\n",
    "    \"\"\"\n",
    "    # Generator wants D(fake) to be large (positive)\n",
    "    return -tf.reduce_mean(fake_score)\n",
    "\n",
    "def logit_loss(output, negative=False):\n",
    "    \"\"\"\n",
    "    BCE Loss (Logit Loss) for GANs.\n",
    "    Alternative to Hinge Loss, used in official DF-GAN implementation.\n",
    "    \n",
    "    Args:\n",
    "        output: Logits from the discriminator/compressor [B, 1]\n",
    "        negative: Boolean. False for real samples (label 1), True for fake samples (label 0).\n",
    "    \"\"\"\n",
    "    # PyTorch: output = nn.Sigmoid()(output); err = nn.BCELoss()(output, labels)\n",
    "    # TF: Use from_logits=True for numerical stability which combines Sigmoid + BCE\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    if not negative:\n",
    "        # Real labels: 1.0\n",
    "        labels = tf.ones_like(output)\n",
    "    else:\n",
    "        # Fake labels: 0.0\n",
    "        labels = tf.zeros_like(output)\n",
    "        \n",
    "    return bce(labels, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Helper for individual hinge loss components (local to training logic)\n",
    "def hinge_loss(output, negative=False):\n",
    "    if not negative:\n",
    "        return tf.reduce_mean(tf.nn.relu(1.0 - output))\n",
    "    else:\n",
    "        return tf.reduce_mean(tf.nn.relu(1.0 + output))\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    real_images, \n",
    "    input_ids, \n",
    "    attention_mask, \n",
    "    generator, \n",
    "    discriminator, \n",
    "    net_c, \n",
    "    text_encoder, \n",
    "    g_optimizer, \n",
    "    d_optimizer, \n",
    "    batch_size, \n",
    "    z_dim,\n",
    "    lambda_ma_gp=2.0,\n",
    "    diff_augment_fn=None  # Added argument\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training step for DF-GAN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Encode Text\n",
    "    # Compute lengths from input_ids (assuming 0 is padding)\n",
    "    cap_lens = tf.reduce_sum(tf.cast(tf.not_equal(input_ids, 0), tf.int32), axis=1)\n",
    "    \n",
    "    # text_encoder returns (words_emb, sent_emb)\n",
    "    # We only need sent_emb for DF-GAN conditioning\n",
    "    _, text_embeddings = text_encoder(input_ids, cap_lens=cap_lens, training=False)\n",
    "\n",
    "    # 2. Train Discriminator (NetD + NetC)\n",
    "    with tf.GradientTape() as d_tape:\n",
    "        # --- Apply DiffAugment to Real Images ---\n",
    "        if diff_augment_fn is not None:\n",
    "            real_images = diff_augment_fn(real_images)\n",
    "        \n",
    "        # --- A. Real Image + Matching Text ---\n",
    "        real_features = discriminator(real_images, training=True)\n",
    "        real_score = net_c([real_features, text_embeddings], training=True)\n",
    "        errD_real = hinge_loss(real_score, negative=False)\n",
    "        \n",
    "        # --- B. Real Image + Mismatched Text ---\n",
    "        # Shift text to create mismatch\n",
    "        mismatched_text = tf.roll(text_embeddings, shift=1, axis=0)\n",
    "        # Note: PyTorch shifts features, we shift text. Result is equivalent (mismatched pairs).\n",
    "        wrong_score = net_c([real_features, mismatched_text], training=True)\n",
    "        errD_mis = hinge_loss(wrong_score, negative=True)\n",
    "        \n",
    "        # --- C. Fake Image + Matching Text ---\n",
    "        noise = tf.random.normal([batch_size, z_dim])\n",
    "        fake_images = generator([noise, text_embeddings], training=True)\n",
    "        \n",
    "        # --- Apply DiffAugment to Fake Images ---\n",
    "        if diff_augment_fn is not None:\n",
    "            fake_images = diff_augment_fn(fake_images)\n",
    "        \n",
    "        fake_features = discriminator(fake_images, training=True)\n",
    "        fake_score = net_c([fake_features, text_embeddings], training=True)\n",
    "        errD_fake = hinge_loss(fake_score, negative=True)\n",
    "        \n",
    "        # --- D. Matching Aware Gradient Penalty ---\n",
    "        # PyTorch: errD_MAGP = MA_GP(imgs, sent_emb, pred_real)\n",
    "        # Note: In TF we re-calculate pred_real inside this function to capture gradients correctly\n",
    "        errD_MAGP = matching_aware_gradient_penalty(\n",
    "            discriminator, net_c, real_images, text_embeddings, p=6.0\n",
    "        )\n",
    "        \n",
    "        # --- E. Total D Loss ---\n",
    "        # PyTorch: errD = errD_real + (errD_fake + errD_mis)/2.0 + errD_MAGP\n",
    "        # CRITICAL FIX: Added the / 2.0 weighting to match your repo\n",
    "        # CRITICAL FIX: Multiply MA-GP by lambda_ma_gp (default 2.0)\n",
    "        # Note: matching_aware_gradient_penalty returns the raw penalty * 2.0 (hardcoded).\n",
    "        # We should trust the function's internal 2.0 factor as \"part of the formula\" \n",
    "        # OR we should assume lambda_ma_gp replaces that 2.0.\n",
    "        # Official code: errD_MAGP = MA_GP(...) * 2.0 is NOT how it works.\n",
    "        # Official code: MA_GP returns (grad_norm ** p).mean() * 2.0\n",
    "        # So we just add it.\n",
    "        d_loss = errD_real + (errD_fake + errD_mis) / 2.0 + errD_MAGP\n",
    "\n",
    "    # Calculate and Apply Gradients for D\n",
    "    d_vars = discriminator.trainable_variables + net_c.trainable_variables\n",
    "    d_grads = d_tape.gradient(d_loss, d_vars)\n",
    "    d_optimizer.apply_gradients(zip(d_grads, d_vars))\n",
    "\n",
    "    # 3. Train Generator\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        # Re-generate noise/images for G update\n",
    "        noise = tf.random.normal([batch_size, z_dim])\n",
    "        fake_images = generator([noise, text_embeddings], training=True)\n",
    "        \n",
    "        # --- Apply DiffAugment to Fake Images (for G loss) ---\n",
    "        if diff_augment_fn is not None:\n",
    "            fake_images = diff_augment_fn(fake_images)\n",
    "        \n",
    "        fake_features = discriminator(fake_images, training=True)\n",
    "        fake_score = net_c([fake_features, text_embeddings], training=True)\n",
    "        \n",
    "        # G Loss: -mean(fake_score)\n",
    "        g_loss = generator_hinge_loss(fake_score)\n",
    "\n",
    "    # Calculate and Apply Gradients for G\n",
    "    g_vars = generator.trainable_variables\n",
    "    g_grads = g_tape.gradient(g_loss, g_vars)\n",
    "    g_optimizer.apply_gradients(zip(g_grads, g_vars))\n",
    "\n",
    "    return {\n",
    "        \"d_loss\": d_loss,\n",
    "        \"g_loss\": g_loss,\n",
    "        \"ma_gp\": errD_MAGP,\n",
    "        \"errD_real\": errD_real,\n",
    "        \"errD_fake\": errD_fake,\n",
    "        \"errD_mis\": errD_mis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "# from transformers import TFCLIPModel # Removed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def DiffAugment(x, policy='translation'):\n",
    "    \"\"\"\n",
    "    TensorFlow implementation of DiffAugment.\n",
    "    Supports 'color', 'translation', 'cutout'.\n",
    "    \"\"\"\n",
    "    if policy:\n",
    "        if 'color' in policy:\n",
    "            x = rand_brightness(x)\n",
    "            x = rand_saturation(x)\n",
    "            x = rand_contrast(x)\n",
    "        if 'translation' in policy:\n",
    "            x = rand_translation(x)\n",
    "        if 'cutout' in policy:\n",
    "            x = rand_cutout(x)\n",
    "    return x\n",
    "\n",
    "# --- Augmentation Primitives ---\n",
    "def rand_brightness(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=-0.5, maxval=0.5)\n",
    "    x = x + magnitude\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_saturation(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.0, maxval=2.0)\n",
    "    x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_contrast(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.5, maxval=1.5)\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    shift = int(64 * ratio)\n",
    "    \n",
    "    # Pad the image with reflection\n",
    "    x_padded = tf.pad(x, [[0, 0], [shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "    \n",
    "    # Vectorized Random Crop using crop_and_resize\n",
    "    # We generate random top-left corners for the crop\n",
    "    padded_size = tf.cast(img_size + 2*shift, tf.float32)\n",
    "    max_offset = 2 * shift\n",
    "    \n",
    "    offsets_y = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    offsets_x = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    \n",
    "    offsets_y = tf.cast(offsets_y, tf.float32)\n",
    "    offsets_x = tf.cast(offsets_x, tf.float32)\n",
    "    \n",
    "    # Normalize coordinates to [0, 1] for crop_and_resize\n",
    "    # Box: [y1, x1, y2, x2]\n",
    "    y1 = offsets_y / padded_size\n",
    "    x1 = offsets_x / padded_size\n",
    "    y2 = (offsets_y + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    x2 = (offsets_x + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    \n",
    "    boxes = tf.stack([y1, x1, y2, x2], axis=1) # [B, 4]\n",
    "    box_indices = tf.range(batch_size)\n",
    "    \n",
    "    # Perform crop and resize (which acts as crop here since size matches)\n",
    "    x_translated = tf.image.crop_and_resize(\n",
    "        x_padded, \n",
    "        boxes, \n",
    "        box_indices, \n",
    "        crop_size=[img_size, img_size]\n",
    "    )\n",
    "    \n",
    "    return x_translated\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    cutout_size = int(64 * ratio // 2) * 2\n",
    "    \n",
    "    # Vectorized mask generation\n",
    "    # Create grid [1, H, W]\n",
    "    iy, ix = tf.meshgrid(tf.range(img_size), tf.range(img_size), indexing='ij')\n",
    "    iy = tf.expand_dims(iy, 0) # [1, H, W]\n",
    "    ix = tf.expand_dims(ix, 0)\n",
    "    \n",
    "    # Random top-left corners for the cutout box [B, 1, 1]\n",
    "    offset_x = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    offset_y = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    \n",
    "    # Create boolean masks [B, H, W]\n",
    "    mask_x = tf.math.logical_and(ix >= offset_x, ix < offset_x + cutout_size)\n",
    "    mask_y = tf.math.logical_and(iy >= offset_y, iy < offset_y + cutout_size)\n",
    "    mask_box = tf.math.logical_and(mask_x, mask_y)\n",
    "    \n",
    "    # Invert mask (keep regions outside box) and cast to float\n",
    "    mask_keep = tf.cast(tf.math.logical_not(mask_box), x.dtype)\n",
    "    mask_keep = tf.expand_dims(mask_keep, -1) # [B, H, W, 1]\n",
    "    \n",
    "    return x * mask_keep\n",
    "\n",
    "def save_sample_images(generator, text_encoder, fixed_ids, fixed_mask, fixed_noise, epoch, save_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a grid of images using fixed noise/text for consistency.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Encode text\n",
    "    # Compute lengths\n",
    "    cap_lens = tf.reduce_sum(tf.cast(tf.not_equal(fixed_ids, 0), tf.int32), axis=1)\n",
    "    _, text_embeds = text_encoder(fixed_ids, cap_lens=cap_lens, training=False)\n",
    "    \n",
    "    # Generate\n",
    "    fake_imgs = generator([fixed_noise, text_embeds], training=False)\n",
    "    \n",
    "    # Convert to [0, 1] for plotting\n",
    "    fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "    fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "    \n",
    "    # Plot Grid (assuming batch size 8 or similar)\n",
    "    n = int(np.sqrt(len(fake_imgs)))\n",
    "    if n * n != len(fake_imgs): n = 8 # Fallback default\n",
    "    \n",
    "    # Save first 8 images or so\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(min(8, len(fake_imgs))):\n",
    "        plt.subplot(1, 8, i+1)\n",
    "        plt.imshow(fake_imgs[i])\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:03d}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(dataset, args):\n",
    "    \"\"\"\n",
    "    Main training loop for DF-GAN with TensorBoard logging and LR decay.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. Initialization & Logging Setup\n",
    "    # ==========================================================================\n",
    "    print(f\"--- Initializing Models (Image Size: {args['IMAGE_SIZE']}) ---\")\n",
    "    \n",
    "    # Models\n",
    "    generator = NetG(ngf=args['NGF'], nz=args['Z_DIM'], cond_dim=args['EMBED_DIM'])\n",
    "    discriminator = NetD(ndf=args['NDF'])\n",
    "    net_c = NetC(ndf=args['NDF'], cond_dim=args['EMBED_DIM'])\n",
    "    \n",
    "    print(\"--- Loading RNN Text Encoder ---\")\n",
    "    # ntoken is len(vocab) which is global\n",
    "    # nhidden=256 to match DAMSM training (internally becomes 128 per direction)\n",
    "    # Force vocab_size=5429 to match the converted weights\n",
    "    vocab_size = 5429 \n",
    "    text_encoder = RNN_Encoder(ntoken=vocab_size, ninput=300, nhidden=256, nlayers=1)\n",
    "    \n",
    "    # Load Pretrained Weights from DAMSM Stage 1\n",
    "    # Keras 3 requires .weights.h5 extension\n",
    "    damsm_weights_path = './damsm_checkpoints/text_encoder.weights.h5'\n",
    "    \n",
    "    if os.path.exists(damsm_weights_path):\n",
    "        print(f\"✓ Loading pretrained DAMSM weights from {damsm_weights_path}\")\n",
    "        # We need to build the model first by calling it on dummy data\n",
    "        dummy_input = tf.zeros((1, 20), dtype=tf.int32)\n",
    "        text_encoder(dummy_input)\n",
    "        try:\n",
    "            text_encoder.load_weights(damsm_weights_path)\n",
    "            print(\"✓ Weights loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error loading weights: {e}\")\n",
    "            print(\"  Ensure convert_weights.ipynb was run with the same TensorFlow version/architecture.\")\n",
    "    else:\n",
    "        print(\"⚠ WARNING: No pretrained DAMSM weights found! Encoder is random.\")\n",
    "        print(\"  Please run Train_DAMSM.ipynb first.\")\n",
    "\n",
    "    # Optimizers\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_G'], beta_1=0.0, beta_2=0.9)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_D'], beta_1=0.0, beta_2=0.9)\n",
    "\n",
    "    # Checkpoints\n",
    "    checkpoint_dir = os.path.join(args['RUN_DIR'], 'checkpoints')\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        generator=generator, discriminator=discriminator, net_c=net_c,\n",
    "        g_optimizer=g_optimizer, d_optimizer=d_optimizer\n",
    "    )\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    # TensorBoard Setup\n",
    "    log_dir = os.path.join(args['RUN_DIR'], 'logs')\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    try:\n",
    "        tensorboard_process = subprocess.Popen(\n",
    "            [sys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir]\n",
    "        )\n",
    "        print(f\"✓ TensorBoard launched (PID: {tensorboard_process.pid})\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not launch TensorBoard: {e}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. DiffAugment Setup\n",
    "    # ==========================================================================\n",
    "    diff_augment_fn = None\n",
    "    if args.get('USE_DIFFAUG', False):\n",
    "        print(f\"--- DiffAugment Enabled: {args['DIFFAUG_POLICY']} ---\")\n",
    "        def da_fn(imgs): return DiffAugment(imgs, policy=args['DIFFAUG_POLICY'])\n",
    "        diff_augment_fn = da_fn\n",
    "    else:\n",
    "        print(\"--- DiffAugment Disabled ---\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. Training Loop\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # Fixed noise/text for visualization\n",
    "    fixed_noise = tf.random.normal([8, args['Z_DIM']])\n",
    "    # Take first batch for fixed text\n",
    "    for fixed_ids, _ in dataset.take(1):\n",
    "        fixed_ids = fixed_ids[:8]\n",
    "        break\n",
    "    # Create dummy mask (all ones) for fixed text\n",
    "    fixed_mask = tf.ones((8, 20), dtype=tf.int32)\n",
    "\n",
    "    start_epoch = 0\n",
    "    if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint)\n",
    "        print(f\"Restored from {manager.latest_checkpoint}\")\n",
    "        # Try to parse epoch from checkpoint name if possible, else 0\n",
    "        # Usually we save epoch in checkpoint or just continue\n",
    "        \n",
    "    print(f\"Starting training for {args['MAX_EPOCH']} epochs...\")\n",
    "    \n",
    "    for epoch in range(start_epoch, args['MAX_EPOCH']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(dataset, desc=f\"Epoch {epoch+1}/{args['MAX_EPOCH']}\")\n",
    "        \n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        \n",
    "        for step, (real_images, input_ids) in enumerate(pbar):\n",
    "            # Create dummy mask (all ones) - RNN ignores it mostly or we compute lengths inside\n",
    "            attention_mask = tf.ones_like(input_ids)\n",
    "            \n",
    "            losses = train_step(\n",
    "                real_images, \n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                generator, \n",
    "                discriminator, \n",
    "                net_c, \n",
    "                text_encoder, \n",
    "                g_optimizer, \n",
    "                d_optimizer, \n",
    "                args['BATCH_SIZE'], \n",
    "                args['Z_DIM'],\n",
    "                lambda_ma_gp=args['LAMBDA_MA_GP'],\n",
    "                diff_augment_fn=diff_augment_fn\n",
    "            )\n",
    "            \n",
    "            d_losses.append(losses['d_loss'])\n",
    "            g_losses.append(losses['g_loss'])\n",
    "            \n",
    "            # Update pbar\n",
    "            pbar.set_postfix({\n",
    "                'D': f\"{losses['d_loss']:.4f}\", \n",
    "                'G': f\"{losses['g_loss']:.4f}\",\n",
    "                'MA': f\"{losses['ma_gp']:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            with summary_writer.as_default():\n",
    "                step_global = epoch * len(dataset) + step\n",
    "                tf.summary.scalar('Loss/D', losses['d_loss'], step=step_global)\n",
    "                tf.summary.scalar('Loss/G', losses['g_loss'], step=step_global)\n",
    "                tf.summary.scalar('Loss/MA_GP', losses['ma_gp'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_Real', losses['errD_real'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_Fake', losses['errD_fake'], step=step_global)\n",
    "                tf.summary.scalar('Loss/D_Mis', losses['errD_mis'], step=step_global)\n",
    "\n",
    "        # End of Epoch\n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        print(f\"Epoch {epoch+1} done. D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}, Time: {time.time()-start_time:.1f}s\")\n",
    "        \n",
    "        # Save Checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_path = manager.save()\n",
    "            print(f\"Saved checkpoint for epoch {epoch+1}: {save_path}\")\n",
    "            \n",
    "        # Save Sample Images\n",
    "        save_sample_images(generator, text_encoder, fixed_ids, fixed_mask, fixed_noise, epoch+1, os.path.join(args['RUN_DIR'], 'samples'))\n",
    "\n",
    "    print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define configuration for training\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create a unique run directory\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"./runs/{run_id}\"\n",
    "if not os.path.exists(run_dir):\n",
    "    os.makedirs(run_dir)\n",
    "\n",
    "# User provided config\n",
    "config = {\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    'NGF': 64,\n",
    "    'NDF': 64,\n",
    "    'Z_DIM': 100,\n",
    "    'EMBED_DIM': 512,\n",
    "    'LR_G': 0.0001,\n",
    "    'LR_D': 0.0004,          # MATCHED to LR_G to prevent D from overpowering G\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'MAX_EPOCH': 600,          # Updated to 600 for good results\n",
    "    'LAMBDA_MA_GP': 2.0,\n",
    "    'RUN_DIR': run_dir,\n",
    "    'SAVE_FREQ': 25,         # Save less frequently to save space\n",
    "    'SAMPLE_FREQ': 1,        # Sample every epoch\n",
    "    'USE_DIFFAUG': False,    # DISABLED: To strictly match official DF-GAN and avoid MA-GP conflicts\n",
    "    'DIFFAUG_POLICY': 'translation',\n",
    "    'N_SAMPLE': num_training_sample if 'num_training_sample' in locals() else 7370\n",
    "}\n",
    "\n",
    "# Save config for reproducibility\n",
    "with open(os.path.join(run_dir, 'config.json'), 'w') as f:\n",
    "    # Filter for JSON serializable values\n",
    "    json_config = {k: v for k, v in config.items() if isinstance(v, (int, float, str, list, bool))}\n",
    "    json.dump(json_config, f, indent=4)\n",
    "\n",
    "print(f\"Training Run Directory: {run_dir}\")\n",
    "print(f\"Config: {json.dumps(json_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dataset' is the tf.data.Dataset object you created in the notebook\n",
    "train(dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "\t\th, w = images.shape[1], images.shape[2]\n",
    "\t\timg = np.zeros((h * size[0], w * size[1], 3))\n",
    "\t\tfor idx, image in enumerate(images):\n",
    "\t\t\t\ti = idx % size[1]\n",
    "\t\t\t\tj = idx // size[1]\n",
    "\t\t\t\timg[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\t\treturn img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "\t\t# getting the pixel values between [0, 1] to save it\n",
    "\t\treturn plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "\t\treturn imsave(images, size, image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string\n",
    "\t\t\t\tindex: Test sample ID\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tinput_ids, attention_mask, index\n",
    "\t\t\"\"\"\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing dataset generator - decodes IDs to raw text\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_pickle('./dataset/testData.pkl')\n",
    "\t\tcaptions_ids = data['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\tchosen_caption_ids = captions_ids[i]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\tindex = data['ID'].values\n",
    "\t\tindex = np.asarray(index)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.repeat().batch(batch_size)\n",
    "\t\t\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference directory inside the run directory\n",
    "inference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "if not os.path.exists(inference_dir):\n",
    "    os.makedirs(inference_dir)\n",
    "print(f\"Inference Directory: {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, config):\n",
    "    print(\"--- Starting Inference ---\")\n",
    "    \n",
    "    # 1. Re-initialize Models\n",
    "    # We need to re-create the models to load weights into them\n",
    "    print(\"Loading models...\")\n",
    "    generator = NetG(ngf=config['NGF'], nz=config['Z_DIM'], cond_dim=config['EMBED_DIM'])\n",
    "    \n",
    "    # Use RNN_Encoder instead of ClipTextEncoder\n",
    "    # nhidden=256 to match training\n",
    "    # Force vocab_size=5429\n",
    "    vocab_size = 5429\n",
    "    text_encoder = RNN_Encoder(ntoken=vocab_size, ninput=300, nhidden=256, nlayers=1)\n",
    "    \n",
    "    # Load Pretrained Weights for Text Encoder\n",
    "    # Keras 3 requires .weights.h5 extension\n",
    "    damsm_weights_path = './damsm_checkpoints/text_encoder.weights.h5'\n",
    "    \n",
    "    if os.path.exists(damsm_weights_path):\n",
    "        print(f\"✓ Loading pretrained DAMSM weights from {damsm_weights_path}\")\n",
    "        dummy_input = tf.zeros((1, 20), dtype=tf.int32)\n",
    "        text_encoder(dummy_input)\n",
    "        try:\n",
    "            text_encoder.load_weights(damsm_weights_path)\n",
    "            print(\"✓ Weights loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error loading weights: {e}\")\n",
    "    else:\n",
    "        print(\"⚠ WARNING: No pretrained DAMSM weights found! Encoder is random.\")\n",
    "\n",
    "    # Dummy call to build the model (optional but good practice)\n",
    "    # generator.build((None, config['Z_DIM'])) \n",
    "    \n",
    "    # 2. Load Checkpoint\n",
    "    checkpoint_dir = os.path.join(config['RUN_DIR'], 'checkpoints')\n",
    "    \n",
    "    # We need to restore the generator. \n",
    "    # Note: We must define the checkpoint object exactly as it was saved to restore correctly,\n",
    "    # or use expect_partial() if we only care about specific parts (like generator).\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    \n",
    "    latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_ckpt:\n",
    "        print(f\"Loading weights from: {latest_ckpt}\")\n",
    "        status = checkpoint.restore(latest_ckpt).expect_partial()\n",
    "        status.assert_existing_objects_matched()\n",
    "        print(\"✓ Weights loaded successfully\")\n",
    "    else:\n",
    "        print(\"⚠ NO CHECKPOINT FOUND! Generating with random weights (Garbage output).\")\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    total_images = 0\n",
    "    pbar = tqdm(total=NUM_TEST, desc='Generating images', unit='img')\n",
    "    \n",
    "    for step, (caption_texts, image_ids) in enumerate(dataset):\n",
    "        # caption_texts: [B, 10] (list of strings? No, dataset generator returns strings?)\n",
    "        # Wait, testing_dataset_generator returns (caption_texts, index)\n",
    "        # caption_texts is a list of strings.\n",
    "        # We need to tokenize them.\n",
    "        \n",
    "        # Actually, let's check testing_dataset_generator.\n",
    "        # It returns caption_texts which are strings.\n",
    "        # We need to convert to IDs.\n",
    "        \n",
    "        # Tokenize\n",
    "        # We need to map words to IDs using word2Id_dict\n",
    "        # This is slow in loop, but fine for inference.\n",
    "        \n",
    "        batch_size_curr = len(caption_texts)\n",
    "        input_ids_list = []\n",
    "        \n",
    "        for cap in caption_texts:\n",
    "            # cap is a tensor string, need to decode\n",
    "            cap_str = cap.numpy().decode('utf-8')\n",
    "            \n",
    "            # Preprocess (simple split and map)\n",
    "            # Remove punctuation\n",
    "            cap_str = cap_str.translate(str.maketrans('', '', string.punctuation))\n",
    "            words = cap_str.lower().split()\n",
    "            \n",
    "            ids = []\n",
    "            for w in words:\n",
    "                if w in word2Id_dict:\n",
    "                    ids.append(word2Id_dict[w])\n",
    "                else:\n",
    "                    ids.append(word2Id_dict['<RARE>'])\n",
    "            \n",
    "            # Pad/Truncate\n",
    "            if len(ids) > MAX_SEQ_LENGTH:\n",
    "                ids = ids[:MAX_SEQ_LENGTH]\n",
    "            else:\n",
    "                ids = ids + [word2Id_dict['<PAD>']] * (MAX_SEQ_LENGTH - len(ids))\n",
    "            \n",
    "            input_ids_list.append(ids)\n",
    "            \n",
    "        input_ids = tf.convert_to_tensor(input_ids_list, dtype=tf.int32)\n",
    "        \n",
    "        # Encode Text\n",
    "        # Compute lengths\n",
    "        cap_lens = tf.reduce_sum(tf.cast(tf.not_equal(input_ids, 0), tf.int32), axis=1)\n",
    "        _, text_embeddings = text_encoder(input_ids, cap_lens=cap_lens, training=False)\n",
    "        \n",
    "        # Generate Noise\n",
    "        noise = tf.random.normal([batch_size_curr, config['Z_DIM']])\n",
    "        \n",
    "        # Generate Images\n",
    "        fake_imgs = generator([noise, text_embeddings], training=False)\n",
    "        \n",
    "        # Post-process\n",
    "        fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "        fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "        \n",
    "        # Save Images\n",
    "        for i in range(batch_size_curr):\n",
    "            img_id = image_ids[i].numpy().decode('utf-8')\n",
    "            save_path = os.path.join(inference_dir, f'inference_{img_id}.jpg')\n",
    "            plt.imsave(save_path, fake_imgs[i])\n",
    "            total_images += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "    pbar.close()\n",
    "    print(f\"Inference Complete. Saved {total_images} images to {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "\t\tprint(f'⚠ No images found in {inference_dir}')\n",
    "\t\tprint('Please run the inference cell first!')\n",
    "else:\n",
    "\t\t# Randomly sample 20 images\n",
    "\t\tnp.random.seed(42)  # For reproducibility\n",
    "\t\tnum_samples = min(20, len(image_files))\n",
    "\t\tsample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "\t\tsample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "\t\t# Create 4x5 grid\n",
    "\t\tfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\t\taxes = axes.flatten()\n",
    "\n",
    "\t\tfor idx, img_path in enumerate(sample_files):\n",
    "\t\t\t\t# Extract image ID from filename\n",
    "\t\t\t\timg_id = int(Path(img_path).stem.split('_')[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Find caption\n",
    "\t\t\t\tcaption_idx = np.where(test_ids == img_id)[0][0]\n",
    "\t\t\t\tcaption_ids = test_captions[caption_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode caption\n",
    "\t\t\t\tcaption_text = ''\n",
    "\t\t\t\tfor word_id in caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':\n",
    "\t\t\t\t\t\t\t\tcaption_text += word + ' '\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Load and display image\n",
    "\t\t\t\timg = plt.imread(img_path)\n",
    "\t\t\t\taxes[idx].imshow(img)\n",
    "\t\t\t\taxes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\t# Hide unused subplots if less than 20 images\n",
    "\t\tfor idx in range(num_samples, 20):\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\tprint(f'\\nTotal generated images: {len(image_files)}')\n",
    "\t\tprint(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
