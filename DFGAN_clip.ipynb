{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttry:\n",
    "\t\t# Restrict TensorFlow to only use the first GPU\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\t\t# Currently, memory growth needs to be the same across GPUs\n",
    "\t\tfor gpu in gpus:\n",
    "\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\n",
    "\t\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\t\tprint(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\texcept RuntimeError as e:\n",
    "\t\t# Memory growth must be set before GPUs have been initialized\n",
    "\t\tprint(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using CLIP tokenizer (sent2IdList removed)\n"
     ]
    }
   ],
   "source": [
    "print(\"✓ Using CLIP tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Load CLIP Tokenizer\n",
    "# \"openai/clip-vit-base-patch32\" is a standard, powerful model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def preprocess_text_clip(text, max_length=77):\n",
    "\t\tencoded = tokenizer(\n",
    "\t\t\t\ttext,\n",
    "\t\t\t\tpadding='max_length',\n",
    "\t\t\t\ttruncation=True,\n",
    "\t\t\t\tmax_length=max_length,\n",
    "\t\t\t\treturn_tensors='tf'\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t\t'input_ids': encoded['input_ids'],\n",
    "\t\t\t\t'attention_mask': encoded['attention_mask']\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string (not IDs!)\n",
    "\t\t\t\timage_path: Path to image file\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\timg, input_ids, attention_mask\n",
    "\t\t\"\"\"\n",
    "\t\t# ============= IMAGE PROCESSING (same as before) =============\n",
    "\t\timg = tf.io.read_file(image_path)\n",
    "\t\timg = tf.image.decode_image(img, channels=3)\n",
    "\t\timg = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "\t\timg.set_shape([None, None, 3])\n",
    "\t\timg = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "\t\t\n",
    "\t\t# Normalize to [-1, 1] to match generator's tanh output\n",
    "\t\timg = (img * 2.0) - 1.0\n",
    "\t\timg.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "\t\t\n",
    "\t\t# ============= TEXT PROCESSING (NEW: Use CLIP tokenizer) =============\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'  # Use numpy arrays for TF compatibility\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly for CLIP\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated dataset generator to work with raw text (decoded from IDs)\n",
    "\t\t\"\"\"\n",
    "\t\t# Load the training data\n",
    "\t\tdf = pd.read_pickle(filenames)\n",
    "\t\tcaptions_ids = df['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to raw text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\t# Randomly choose one caption (list of ID lists)\n",
    "\t\t\t\tchosen_caption_ids = random.choice(captions_ids[i])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\timage_paths = df['ImagePath'].values\n",
    "\t\t\n",
    "\t\t# Verify same length\n",
    "\t\tassert len(caption_texts) == len(image_paths)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text and image paths\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, image_paths))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.cache()\n",
    "\t\tdataset = dataset.shuffle(len(caption_texts)).batch(batch_size, drop_remainder=True)\n",
    "\t\tdataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¶</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import TFCLIPTextModel, TFCLIPModel\n",
    "\n",
    "class ClipTextEncoder(tf.keras.Model):\n",
    "\t\tdef __init__(self, output_dim=512, freeze_clip=True):\n",
    "\t\t\t\tsuper(ClipTextEncoder, self).__init__()\n",
    "\t\t\t\t# Load Pre-trained CLIP Text Model\n",
    "\t\t\t\tself.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\t\t\t\t\n",
    "\t\t\t\tif freeze_clip:\n",
    "\t\t\t\t\t\tself.clip.trainable = False\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t# REMOVED: Projection, LayerNorm, Dropout to ensure RAW embeddings\n",
    "\t\n",
    "\t\tdef call(self, input_ids, attention_mask, training=False):\n",
    "\t\t\t# 1. Get the projected features (Aligned with images, e.g., 512-dim)\n",
    "\t\t\ttext_embeds = self.clip.get_text_features(\n",
    "\t\t\t\tinput_ids=input_ids, \n",
    "\t\t\t\tattention_mask=attention_mask\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\t# 2. CRITICAL FIX: Manually normalize to get the actual CLIP embedding\n",
    "\t\t\t# CLIP uses cosine similarity, so vectors must be unit length.\n",
    "\t\t\ttext_embeds = tf.math.l2_normalize(text_embeds, axis=1)\n",
    "\t\t\t\n",
    "\t\t\treturn text_embeds\n",
    "\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, initializers\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER LAYERS & BLOCKS\n",
    "# ==============================================================================\n",
    "\n",
    "class Affine(layers.Layer):\n",
    "    \"\"\"\n",
    "    PyTorch: class Affine(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, num_features):\n",
    "        super(Affine, self).__init__()\n",
    "        \n",
    "        # PyTorch: self.fc_gamma = nn.Sequential(...)\n",
    "        # Linear -> ReLU -> Linear\n",
    "        self.fc_gamma = tf.keras.Sequential([\n",
    "            layers.Dense(num_features, activation='relu'),\n",
    "            layers.Dense(num_features, kernel_initializer='zeros', bias_initializer='ones')\n",
    "        ])\n",
    "\n",
    "        # PyTorch: self.fc_beta = nn.Sequential(...)\n",
    "        # Linear -> ReLU -> Linear\n",
    "        self.fc_beta = tf.keras.Sequential([\n",
    "            layers.Dense(num_features, activation='relu'),\n",
    "            layers.Dense(num_features, kernel_initializer='zeros', bias_initializer='zeros')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x: [B, H, W, C], y: [B, Cond_Dim]\n",
    "        x, y = inputs \n",
    "\n",
    "        # --- Gamma & Beta Calculation ---\n",
    "        weight = self.fc_gamma(y)\n",
    "        bias = self.fc_beta(y)\n",
    "\n",
    "        # --- Reshape for Broadcasting ---\n",
    "        # PyTorch: weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
    "        # TF (NHWC): We need [B, 1, 1, C] to broadcast over H, W\n",
    "        weight = tf.reshape(weight, [-1, 1, 1, x.shape[-1]])\n",
    "        bias = tf.reshape(bias, [-1, 1, 1, x.shape[-1]])\n",
    "\n",
    "        return weight * x + bias\n",
    "\n",
    "\n",
    "class DFBLK(layers.Layer):\n",
    "    \"\"\"\n",
    "    PyTorch: class DFBLK(nn.Module)\n",
    "    Structure: Affine -> ReLU -> Affine -> ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch):\n",
    "        super(DFBLK, self).__init__()\n",
    "        self.affine0 = Affine(cond_dim, in_ch)\n",
    "        self.affine1 = Affine(cond_dim, in_ch)\n",
    "        self.act = layers.LeakyReLU(0.2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        \n",
    "        h = self.affine0([x, y])\n",
    "        h = self.act(h)\n",
    "        \n",
    "        h = self.affine1([h, y])\n",
    "        h = self.act(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "class G_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    PyTorch: class G_Block(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, in_ch, out_ch, upsample=True):\n",
    "        super(G_Block, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.learnable_sc = (in_ch != out_ch)\n",
    "        \n",
    "        # PyTorch: self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        self.c1 = layers.Conv2D(out_ch, 3, strides=1, padding='same')\n",
    "        \n",
    "        # PyTorch: self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
    "        self.c2 = layers.Conv2D(out_ch, 3, strides=1, padding='same')\n",
    "        \n",
    "        self.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        \n",
    "        if self.learnable_sc:\n",
    "            # PyTorch: self.c_sc = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "            self.c_sc = layers.Conv2D(out_ch, 1, strides=1, padding='valid')\n",
    "\n",
    "        self.upsample_layer = layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        \n",
    "        # --- Upsample ---\n",
    "        if self.upsample:\n",
    "            x = self.upsample_layer(x)\n",
    "\n",
    "        # --- Shortcut Path ---\n",
    "        h_sc = x\n",
    "        if self.learnable_sc:\n",
    "            h_sc = self.c_sc(h_sc)\n",
    "\n",
    "        # --- Residual Path ---\n",
    "        # 1. fuse1 (DFBLK)\n",
    "        h_res = self.fuse1([x, y])\n",
    "        # 2. c1 (Conv)\n",
    "        h_res = self.c1(h_res)\n",
    "        # 3. fuse2 (DFBLK)\n",
    "        h_res = self.fuse2([h_res, y])\n",
    "        # 4. c2 (Conv)\n",
    "        h_res = self.c2(h_res)\n",
    "\n",
    "        return h_sc + h_res\n",
    "\n",
    "\n",
    "class D_Block(layers.Layer):\n",
    "    \"\"\"\n",
    "    PyTorch: class D_Block(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, fin, fout, downsample=True):\n",
    "        super(D_Block, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        \n",
    "        # PyTorch: self.conv_r = nn.Sequential(...)\n",
    "        # 1. Conv 4x4, stride 2, pad 1 (PyTorch) -> Downsamples by 2\n",
    "        self.conv_r_1 = layers.Conv2D(fout, 4, strides=2, padding='same', use_bias=False)\n",
    "        self.act_1 = layers.LeakyReLU(0.2)\n",
    "        \n",
    "        # 2. Conv 3x3, stride 1, pad 1\n",
    "        self.conv_r_2 = layers.Conv2D(fout, 3, strides=1, padding='same', use_bias=False)\n",
    "        self.act_2 = layers.LeakyReLU(0.2)\n",
    "        \n",
    "        # PyTorch: self.conv_s = nn.Conv2d(fin, fout, 1, stride=1, padding=0)\n",
    "        self.conv_s = layers.Conv2D(fout, 1, strides=1, padding='valid')\n",
    "        \n",
    "        # PyTorch: self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        # FIX: Initialize gamma to 0.1 to prevent \"dead\" discriminator at start.\n",
    "        # In TF, 0.0 initialization often causes the variable to get stuck, \n",
    "        # leading to the 4x4 block artifacts you observed.\n",
    "        self.gamma = tf.Variable(0.1, trainable=True, dtype=tf.float32)\n",
    "        \n",
    "        # PyTorch: F.avg_pool2d(x, 2)\n",
    "        self.avg_pool = layers.AveragePooling2D(pool_size=2, strides=2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # --- Residual Path (conv_r) ---\n",
    "        res = self.conv_r_1(x)\n",
    "        res = self.act_1(res)\n",
    "        res = self.conv_r_2(res)\n",
    "        res = self.act_2(res)\n",
    "        \n",
    "        # --- Shortcut Path ---\n",
    "        # PyTorch: if self.learned_shortcut: x = self.conv_s(x)\n",
    "        # PyTorch: if self.downsample: x = F.avg_pool2d(x, 2)\n",
    "        \n",
    "        if self.learned_shortcut:\n",
    "            x = self.conv_s(x)\n",
    "            \n",
    "        if self.downsample:\n",
    "            x = self.avg_pool(x)\n",
    "            \n",
    "        return x + self.gamma * res\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MAIN NETWORKS (NetG, NetD, NetC)\n",
    "# ==============================================================================\n",
    "\n",
    "class NetG(Model):\n",
    "    \"\"\"\n",
    "    PyTorch: class NetG(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, ngf=32, nz=100, cond_dim=256, imsize=64, ch_size=3):\n",
    "        super(NetG, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.nz = nz\n",
    "        \n",
    "        # PyTorch: self.fc = nn.Linear(nz, ngf*8*4*4)\n",
    "        self.fc = layers.Dense(ngf * 8 * 4 * 4)\n",
    "        \n",
    "        # PyTorch: get_G_in_out_chs(ngf, 64) -> [(8,8), (8,4), (4,2), (2,1)]\n",
    "        # Block 1: 4x4 -> 8x8 (8*ngf -> 8*ngf)\n",
    "        self.block1 = G_Block(cond_dim + nz, ngf * 8, ngf * 8, upsample=True)\n",
    "        # Block 2: 8x8 -> 16x16 (8*ngf -> 4*ngf)\n",
    "        self.block2 = G_Block(cond_dim + nz, ngf * 8, ngf * 4, upsample=True)\n",
    "        # Block 3: 16x16 -> 32x32 (4*ngf -> 2*ngf)\n",
    "        self.block3 = G_Block(cond_dim + nz, ngf * 4, ngf * 2, upsample=True)\n",
    "        # Block 4: 32x32 -> 64x64 (2*ngf -> 1*ngf)\n",
    "        self.block4 = G_Block(cond_dim + nz, ngf * 2, ngf * 1, upsample=True)\n",
    "        \n",
    "        # PyTorch: self.to_rgb = nn.Sequential(...)\n",
    "        self.to_rgb_act = layers.LeakyReLU(0.2)\n",
    "        self.to_rgb_conv = layers.Conv2D(ch_size, 3, strides=1, padding='same')\n",
    "        self.to_rgb_out = layers.Activation('tanh')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # noise: [B, nz], c: [B, cond_dim]\n",
    "        noise, c = inputs\n",
    "        \n",
    "        # PyTorch: out = self.fc(noise)\n",
    "        out = self.fc(noise)\n",
    "        \n",
    "        # PyTorch: out = out.view(noise.size(0), 8*self.ngf, 4, 4)\n",
    "        out = tf.reshape(out, [-1, 4, 4, self.ngf * 8])\n",
    "        \n",
    "        # PyTorch: cond = torch.cat((noise, c), dim=1)\n",
    "        cond = tf.concat([noise, c], axis=1)\n",
    "        \n",
    "        # PyTorch: loop over GBlocks\n",
    "        out = self.block1([out, cond])\n",
    "        out = self.block2([out, cond])\n",
    "        out = self.block3([out, cond])\n",
    "        out = self.block4([out, cond])\n",
    "        \n",
    "        # PyTorch: out = self.to_rgb(out)\n",
    "        out = self.to_rgb_act(out)\n",
    "        out = self.to_rgb_conv(out)\n",
    "        out = self.to_rgb_out(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class NetD(Model):\n",
    "    \"\"\"\n",
    "    PyTorch: class NetD(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, ndf=64, imsize=64, ch_size=3):\n",
    "        super(NetD, self).__init__()\n",
    "        \n",
    "        # PyTorch: self.conv_img = nn.Conv2d(ch_size, ndf, 3, 1, 1)\n",
    "        self.conv_img = layers.Conv2D(ndf, 3, strides=1, padding='same')\n",
    "        \n",
    "        # PyTorch: get_D_in_out_chs(ndf, 64) -> [(1,2), (2,4), (4,8), (8,8)]\n",
    "        # Block 1: 64x64 -> 32x32 (ndf -> 2*ndf)\n",
    "        self.block1 = D_Block(ndf, ndf * 2, downsample=True)\n",
    "        # Block 2: 32x32 -> 16x16 (2*ndf -> 4*ndf)\n",
    "        self.block2 = D_Block(ndf * 2, ndf * 4, downsample=True)\n",
    "        # Block 3: 16x16 -> 8x8 (4*ndf -> 8*ndf)\n",
    "        self.block3 = D_Block(ndf * 4, ndf * 8, downsample=True)\n",
    "        # Block 4: 8x8 -> 4x4 (8*ndf -> 8*ndf)\n",
    "        self.block4 = D_Block(ndf * 8, ndf * 8, downsample=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        out = self.conv_img(x)\n",
    "        \n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class NetC(Model):\n",
    "    \"\"\"\n",
    "    PyTorch: class NetC(nn.Module)\n",
    "    \"\"\"\n",
    "    def __init__(self, ndf=64, cond_dim=256):\n",
    "        super(NetC, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        \n",
    "        # PyTorch: nn.Conv2d(ndf*8+cond_dim, ndf*2, 3, 1, 1, bias=False)\n",
    "        # Input channels: 8*ndf (from NetD) + cond_dim\n",
    "        self.joint_conv_1 = layers.Conv2D(ndf * 2, 3, strides=1, padding='same', use_bias=False)\n",
    "        self.act = layers.LeakyReLU(0.2)\n",
    "        self.joint_conv_2 = layers.Conv2D(1, 4, strides=1, padding='valid', use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # out: [B, 4, 4, 8*ndf], y: [B, cond_dim]\n",
    "        out, y = inputs\n",
    "        \n",
    "        # PyTorch: y = y.view(-1, self.cond_dim, 1, 1)\n",
    "        # PyTorch: y = y.repeat(1, 1, 4, 4)\n",
    "        y = tf.reshape(y, [-1, 1, 1, self.cond_dim])\n",
    "        y = tf.tile(y, [1, 4, 4, 1])\n",
    "        \n",
    "        # PyTorch: h_c_code = torch.cat((out, y), 1)\n",
    "        h_c_code = tf.concat([out, y], axis=-1)\n",
    "        \n",
    "        # PyTorch: out = self.joint_conv(h_c_code)\n",
    "        out = self.joint_conv_1(h_c_code)\n",
    "        out = self.act(out)\n",
    "        out = self.joint_conv_2(out)\n",
    "        \n",
    "        # Output is [B, 1, 1, 1]\n",
    "        return tf.reshape(out, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def matching_aware_gradient_penalty(netD, netC, real_images, text_embeddings, p=6.0):\n",
    "    \"\"\"\n",
    "    Calculates the Matching Aware Gradient Penalty (MA-GP).\n",
    "    \n",
    "    Args:\n",
    "        netD: The Discriminator model (outputs features).\n",
    "        netC: The Compressor/Classifier model (takes features + text, outputs score).\n",
    "        real_images: Batch of real images [B, 64, 64, 3].\n",
    "        text_embeddings: Batch of matching text embeddings [B, Cond_Dim].\n",
    "        p: The power to raise the gradient norm to. Official DF-GAN uses p=6.\n",
    "        \n",
    "    Returns:\n",
    "        The gradient penalty scalar (averaged over batch).\n",
    "    \"\"\"\n",
    "    # 1. Watch BOTH real_images and text_embeddings\n",
    "    # Official DF-GAN penalizes gradients w.r.t both modalities\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(real_images)\n",
    "        tape.watch(text_embeddings)\n",
    "        \n",
    "        # 2. Forward pass through Discriminator and NetC\n",
    "        # Get features from image\n",
    "        features = netD(real_images, training=True)\n",
    "        \n",
    "        # Get scalar score from features + matching text\n",
    "        # NetC inputs are [features, text_embeddings]\n",
    "        pred_real = netC([features, text_embeddings], training=True)\n",
    "        \n",
    "    # 3. Calculate gradients of the prediction w.r.t BOTH inputs\n",
    "    grads = tape.gradient(pred_real, [real_images, text_embeddings])\n",
    "    grad_img = grads[0]\n",
    "    grad_text = grads[1]\n",
    "    \n",
    "    # 4. Flatten and Concatenate gradients\n",
    "    # Flatten: [B, -1]\n",
    "    grad_img_flat = tf.reshape(grad_img, [tf.shape(grad_img)[0], -1])\n",
    "    grad_text_flat = tf.reshape(grad_text, [tf.shape(grad_text)[0], -1])\n",
    "    \n",
    "    # Concatenate: [B, dim_img + dim_text]\n",
    "    grad_all = tf.concat([grad_img_flat, grad_text_flat], axis=1)\n",
    "    \n",
    "    # 5. Calculate L2 norm of the combined gradients\n",
    "    grad_norms = tf.norm(grad_all, axis=1)\n",
    "    \n",
    "    # 6. Calculate Penalty: 2.0 * E[||grad||^p]\n",
    "    # Official implementation includes a factor of 2.0\n",
    "    penalty = 2.0 * tf.reduce_mean(tf.pow(grad_norms, p))\n",
    "    \n",
    "    return penalty\n",
    "\n",
    "def discriminator_hinge_loss(real_score, fake_score, wrong_score=None):\n",
    "    \"\"\"\n",
    "    Hinge Loss for Discriminator.\n",
    "    L_D = E[max(0, 1 - D(real, text))] + E[max(0, 1 + D(fake, text))] \n",
    "          + (Optional) E[max(0, 1 + D(real, mismatch_text))]\n",
    "    \"\"\"\n",
    "    # Real Image + Matching Text: Should be > 1\n",
    "    real_loss = tf.reduce_mean(tf.nn.relu(1.0 - real_score))\n",
    "    \n",
    "    # Fake Image + Matching Text: Should be < -1\n",
    "    fake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake_score))\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    # (Optional) Real Image + Mismatched Text: Should be < -1\n",
    "    if wrong_score is not None:\n",
    "        wrong_loss = tf.reduce_mean(tf.nn.relu(1.0 + wrong_score))\n",
    "        total_loss += wrong_loss\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "def generator_hinge_loss(fake_score):\n",
    "    \"\"\"\n",
    "    Hinge Loss for Generator.\n",
    "    L_G = -E[D(fake, text)]\n",
    "    \"\"\"\n",
    "    # Generator wants D(fake) to be large (positive)\n",
    "    return -tf.reduce_mean(fake_score)\n",
    "\n",
    "def logit_loss(output, negative=False):\n",
    "    \"\"\"\n",
    "    BCE Loss (Logit Loss) for GANs.\n",
    "    Alternative to Hinge Loss, used in official DF-GAN implementation.\n",
    "    \n",
    "    Args:\n",
    "        output: Logits from the discriminator/compressor [B, 1]\n",
    "        negative: Boolean. False for real samples (label 1), True for fake samples (label 0).\n",
    "    \"\"\"\n",
    "    # PyTorch: output = nn.Sigmoid()(output); err = nn.BCELoss()(output, labels)\n",
    "    # TF: Use from_logits=True for numerical stability which combines Sigmoid + BCE\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    if not negative:\n",
    "        # Real labels: 1.0\n",
    "        labels = tf.ones_like(output)\n",
    "    else:\n",
    "        # Fake labels: 0.0\n",
    "        labels = tf.zeros_like(output)\n",
    "        \n",
    "    return bce(labels, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Helper for individual hinge loss components (local to training logic)\n",
    "def hinge_loss(output, negative=False):\n",
    "    if not negative:\n",
    "        return tf.reduce_mean(tf.nn.relu(1.0 - output))\n",
    "    else:\n",
    "        return tf.reduce_mean(tf.nn.relu(1.0 + output))\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    real_images, \n",
    "    input_ids, \n",
    "    attention_mask, \n",
    "    generator, \n",
    "    discriminator, \n",
    "    net_c, \n",
    "    text_encoder, \n",
    "    g_optimizer, \n",
    "    d_optimizer, \n",
    "    batch_size, \n",
    "    z_dim,\n",
    "    lambda_ma_gp=2.0,\n",
    "    diff_augment_fn=None  # Added argument\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes one training step for DF-GAN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Encode Text\n",
    "    # text_embeddings: [B, Cond_Dim]\n",
    "    text_embeddings = text_encoder(input_ids, attention_mask, training=False)\n",
    "\n",
    "    # 2. Train Discriminator (NetD + NetC)\n",
    "    with tf.GradientTape() as d_tape:\n",
    "        # --- Apply DiffAugment to Real Images ---\n",
    "        if diff_augment_fn is not None:\n",
    "            real_images = diff_augment_fn(real_images)\n",
    "        \n",
    "        # --- A. Real Image + Matching Text ---\n",
    "        real_features = discriminator(real_images, training=True)\n",
    "        real_score = net_c([real_features, text_embeddings], training=True)\n",
    "        errD_real = hinge_loss(real_score, negative=False)\n",
    "        \n",
    "        # --- B. Real Image + Mismatched Text ---\n",
    "        # Shift text to create mismatch\n",
    "        mismatched_text = tf.roll(text_embeddings, shift=1, axis=0)\n",
    "        # Note: PyTorch shifts features, we shift text. Result is equivalent (mismatched pairs).\n",
    "        wrong_score = net_c([real_features, mismatched_text], training=True)\n",
    "        errD_mis = hinge_loss(wrong_score, negative=True)\n",
    "        \n",
    "        # --- C. Fake Image + Matching Text ---\n",
    "        noise = tf.random.normal([batch_size, z_dim])\n",
    "        fake_images = generator([noise, text_embeddings], training=True)\n",
    "        \n",
    "        # --- Apply DiffAugment to Fake Images ---\n",
    "        if diff_augment_fn is not None:\n",
    "            fake_images = diff_augment_fn(fake_images)\n",
    "        \n",
    "        fake_features = discriminator(fake_images, training=True)\n",
    "        fake_score = net_c([fake_features, text_embeddings], training=True)\n",
    "        errD_fake = hinge_loss(fake_score, negative=True)\n",
    "        \n",
    "        # --- D. Matching Aware Gradient Penalty ---\n",
    "        # PyTorch: errD_MAGP = MA_GP(imgs, sent_emb, pred_real)\n",
    "        # Note: In TF we re-calculate pred_real inside this function to capture gradients correctly\n",
    "        errD_MAGP = matching_aware_gradient_penalty(\n",
    "            discriminator, net_c, real_images, text_embeddings, p=6.0\n",
    "        )\n",
    "        \n",
    "        # --- E. Total D Loss ---\n",
    "        # PyTorch: errD = errD_real + (errD_fake + errD_mis)/2.0 + errD_MAGP\n",
    "        # CRITICAL FIX: Added the / 2.0 weighting to match your repo\n",
    "        d_loss = errD_real + (errD_fake + errD_mis) / 2.0 + errD_MAGP\n",
    "\n",
    "    # Calculate and Apply Gradients for D\n",
    "    d_vars = discriminator.trainable_variables + net_c.trainable_variables\n",
    "    d_grads = d_tape.gradient(d_loss, d_vars)\n",
    "    d_optimizer.apply_gradients(zip(d_grads, d_vars))\n",
    "\n",
    "    # 3. Train Generator\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        # Re-generate noise/images for G update\n",
    "        noise = tf.random.normal([batch_size, z_dim])\n",
    "        fake_images = generator([noise, text_embeddings], training=True)\n",
    "        \n",
    "        # --- Apply DiffAugment to Fake Images (for G loss) ---\n",
    "        if diff_augment_fn is not None:\n",
    "            fake_images = diff_augment_fn(fake_images)\n",
    "        \n",
    "        fake_features = discriminator(fake_images, training=True)\n",
    "        fake_score = net_c([fake_features, text_embeddings], training=True)\n",
    "        \n",
    "        # G Loss: -mean(fake_score)\n",
    "        g_loss = generator_hinge_loss(fake_score)\n",
    "\n",
    "    # Calculate and Apply Gradients for G\n",
    "    g_vars = generator.trainable_variables\n",
    "    g_grads = g_tape.gradient(g_loss, g_vars)\n",
    "    g_optimizer.apply_gradients(zip(g_grads, g_vars))\n",
    "\n",
    "    return {\n",
    "        \"d_loss\": d_loss,\n",
    "        \"g_loss\": g_loss,\n",
    "        \"ma_gp\": errD_MAGP,\n",
    "        \"errD_real\": errD_real,\n",
    "        \"errD_fake\": errD_fake,\n",
    "        \"errD_mis\": errD_mis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from transformers import TFCLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def DiffAugment(x, policy='translation'):\n",
    "    \"\"\"\n",
    "    TensorFlow implementation of DiffAugment.\n",
    "    Supports 'color', 'translation', 'cutout'.\n",
    "    \"\"\"\n",
    "    if policy:\n",
    "        if 'color' in policy:\n",
    "            x = rand_brightness(x)\n",
    "            x = rand_saturation(x)\n",
    "            x = rand_contrast(x)\n",
    "        if 'translation' in policy:\n",
    "            x = rand_translation(x)\n",
    "        if 'cutout' in policy:\n",
    "            x = rand_cutout(x)\n",
    "    return x\n",
    "\n",
    "# --- Augmentation Primitives ---\n",
    "def rand_brightness(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=-0.5, maxval=0.5)\n",
    "    x = x + magnitude\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_saturation(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.0, maxval=2.0)\n",
    "    x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_contrast(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1], minval=0.5, maxval=1.5)\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return tf.clip_by_value(x, -1.0, 1.0)\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    shift = int(64 * ratio)\n",
    "    \n",
    "    # Pad the image with reflection\n",
    "    x_padded = tf.pad(x, [[0, 0], [shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "    \n",
    "    # Vectorized Random Crop using crop_and_resize\n",
    "    # We generate random top-left corners for the crop\n",
    "    padded_size = tf.cast(img_size + 2*shift, tf.float32)\n",
    "    max_offset = 2 * shift\n",
    "    \n",
    "    offsets_y = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    offsets_x = tf.random.uniform([batch_size], minval=0, maxval=max_offset + 1, dtype=tf.int32)\n",
    "    \n",
    "    offsets_y = tf.cast(offsets_y, tf.float32)\n",
    "    offsets_x = tf.cast(offsets_x, tf.float32)\n",
    "    \n",
    "    # Normalize coordinates to [0, 1] for crop_and_resize\n",
    "    # Box: [y1, x1, y2, x2]\n",
    "    y1 = offsets_y / padded_size\n",
    "    x1 = offsets_x / padded_size\n",
    "    y2 = (offsets_y + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    x2 = (offsets_x + tf.cast(img_size, tf.float32)) / padded_size\n",
    "    \n",
    "    boxes = tf.stack([y1, x1, y2, x2], axis=1) # [B, 4]\n",
    "    box_indices = tf.range(batch_size)\n",
    "    \n",
    "    # Perform crop and resize (which acts as crop here since size matches)\n",
    "    x_translated = tf.image.crop_and_resize(\n",
    "        x_padded, \n",
    "        boxes, \n",
    "        box_indices, \n",
    "        crop_size=[img_size, img_size]\n",
    "    )\n",
    "    \n",
    "    return x_translated\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    img_size = tf.shape(x)[1]\n",
    "    cutout_size = int(64 * ratio // 2) * 2\n",
    "    \n",
    "    # Vectorized mask generation\n",
    "    # Create grid [1, H, W]\n",
    "    iy, ix = tf.meshgrid(tf.range(img_size), tf.range(img_size), indexing='ij')\n",
    "    iy = tf.expand_dims(iy, 0) # [1, H, W]\n",
    "    ix = tf.expand_dims(ix, 0)\n",
    "    \n",
    "    # Random top-left corners for the cutout box [B, 1, 1]\n",
    "    offset_x = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    offset_y = tf.random.uniform([batch_size, 1, 1], minval=0, maxval=img_size + 1 - cutout_size, dtype=tf.int32)\n",
    "    \n",
    "    # Create boolean masks [B, H, W]\n",
    "    mask_x = tf.math.logical_and(ix >= offset_x, ix < offset_x + cutout_size)\n",
    "    mask_y = tf.math.logical_and(iy >= offset_y, iy < offset_y + cutout_size)\n",
    "    mask_box = tf.math.logical_and(mask_x, mask_y)\n",
    "    \n",
    "    # Invert mask (keep regions outside box) and cast to float\n",
    "    mask_keep = tf.cast(tf.math.logical_not(mask_box), x.dtype)\n",
    "    mask_keep = tf.expand_dims(mask_keep, -1) # [B, H, W, 1]\n",
    "    \n",
    "    return x * mask_keep\n",
    "\n",
    "def save_sample_images(generator, text_encoder, fixed_ids, fixed_mask, fixed_noise, epoch, save_dir):\n",
    "    \"\"\"\n",
    "    Generates and saves a grid of images using fixed noise/text for consistency.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Encode text\n",
    "    text_embeds = text_encoder(fixed_ids, fixed_mask, training=False)\n",
    "    \n",
    "    # Generate\n",
    "    fake_imgs = generator([fixed_noise, text_embeds], training=False)\n",
    "    \n",
    "    # Convert to [0, 1] for plotting\n",
    "    fake_imgs = (fake_imgs + 1.0) * 0.5\n",
    "    fake_imgs = tf.clip_by_value(fake_imgs, 0.0, 1.0).numpy()\n",
    "    \n",
    "    # Plot Grid (assuming batch size 8 or similar)\n",
    "    n = int(np.sqrt(len(fake_imgs)))\n",
    "    if n * n != len(fake_imgs): n = 8 # Fallback default\n",
    "    \n",
    "    # Save first 8 images or so\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(min(8, len(fake_imgs))):\n",
    "        plt.subplot(1, 8, i+1)\n",
    "        plt.imshow(fake_imgs[i])\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:03d}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import subprocess\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(dataset, args):\n",
    "    \"\"\"\n",
    "    Main training loop for DF-GAN with TensorBoard logging and LR decay.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # 1. Initialization & Logging Setup\n",
    "    # ==========================================================================\n",
    "    print(f\"--- Initializing Models (Image Size: {args['IMAGE_SIZE']}) ---\")\n",
    "    \n",
    "    # Models\n",
    "    generator = NetG(ngf=args['NGF'], nz=args['Z_DIM'], cond_dim=args['EMBED_DIM'])\n",
    "    discriminator = NetD(ndf=args['NDF'])\n",
    "    net_c = NetC(ndf=args['NDF'], cond_dim=args['EMBED_DIM'])\n",
    "    \n",
    "    print(\"--- Loading CLIP Text Encoder ---\")\n",
    "    text_encoder = ClipTextEncoder()\n",
    "\n",
    "    # Optimizers\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_G'], beta_1=0.0, beta_2=0.9)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=args['LR_D'], beta_1=0.0, beta_2=0.9)\n",
    "\n",
    "    # Checkpoints\n",
    "    checkpoint_dir = os.path.join(args['RUN_DIR'], 'checkpoints')\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        generator=generator, discriminator=discriminator, net_c=net_c,\n",
    "        g_optimizer=g_optimizer, d_optimizer=d_optimizer\n",
    "    )\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "    # TensorBoard Setup\n",
    "    log_dir = os.path.join(args['RUN_DIR'], 'logs')\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    try:\n",
    "        tensorboard_process = subprocess.Popen(\n",
    "            [sys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir]\n",
    "        )\n",
    "        print(f\"✓ TensorBoard launched (PID: {tensorboard_process.pid})\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not launch TensorBoard: {e}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 2. DiffAugment Setup\n",
    "    # ==========================================================================\n",
    "    diff_augment_fn = None\n",
    "    if args.get('USE_DIFFAUG', False):\n",
    "        print(f\"--- DiffAugment Enabled: {args['DIFFAUG_POLICY']} ---\")\n",
    "        def da_fn(imgs): return DiffAugment(imgs, policy=args['DIFFAUG_POLICY'])\n",
    "        diff_augment_fn = da_fn\n",
    "    else:\n",
    "        print(\"--- DiffAugment Disabled ---\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. Fixed Sample for Visualization\n",
    "    # ==========================================================================\n",
    "    for img_vis, ids_vis, mask_vis in dataset.take(1):\n",
    "        fixed_ids = ids_vis[:16]\n",
    "        fixed_mask = mask_vis[:16]\n",
    "        fixed_noise = tf.random.normal([16, args['Z_DIM']])\n",
    "        break\n",
    "        \n",
    "    # ==========================================================================\n",
    "    # 4. Training Loop\n",
    "    # ==========================================================================\n",
    "    print(f\"--- Starting Training for {args['N_EPOCH']} Epochs ---\")\n",
    "    steps_per_epoch = args['N_SAMPLE'] // args['BATCH_SIZE']\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(args['N_EPOCH']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- Learning Rate Decay ---\n",
    "        if epoch >= args.get('LR_DECAY_START', 50) and epoch % args.get('LR_DECAY_EVERY', 10) == 0:\n",
    "            decay = args.get('LR_DECAY_FACTOR', 0.95)\n",
    "            min_lr = args.get('LR_MIN', 1e-6)\n",
    "            \n",
    "            new_lr_g = max(g_optimizer.learning_rate.numpy() * decay, min_lr)\n",
    "            new_lr_d = max(d_optimizer.learning_rate.numpy() * decay, min_lr)\n",
    "            \n",
    "            g_optimizer.learning_rate.assign(new_lr_g)\n",
    "            d_optimizer.learning_rate.assign(new_lr_d)\n",
    "            print(f\"📉 LR Decay: G={new_lr_g:.2e}, D={new_lr_d:.2e}\")\n",
    "\n",
    "        # --- Epoch Loop ---\n",
    "        if epoch == 0: print(\"Note: First step takes longer due to XLA compilation...\")\n",
    "        \n",
    "        pbar = tqdm(enumerate(dataset), total=steps_per_epoch, desc=f\"Epoch {epoch+1}\")\n",
    "        epoch_metrics = {'g_loss': 0.0, 'd_loss': 0.0}\n",
    "        \n",
    "        for step, (real_images, input_ids, attention_mask) in pbar:\n",
    "            losses = train_step(\n",
    "                real_images, input_ids, attention_mask,\n",
    "                generator, discriminator, net_c, text_encoder,\n",
    "                g_optimizer, d_optimizer, args['BATCH_SIZE'], args['Z_DIM'],\n",
    "                lambda_ma_gp=args.get('LAMBDA_GP', 2.0),\n",
    "                diff_augment_fn=diff_augment_fn\n",
    "            )\n",
    "            \n",
    "            # Accumulate for epoch stats\n",
    "            epoch_metrics['g_loss'] += losses['g_loss']\n",
    "            epoch_metrics['d_loss'] += losses['d_loss']\n",
    "            \n",
    "            # TensorBoard Logging (Step-wise)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('Loss/D_Total', losses['d_loss'], step=global_step)\n",
    "                tf.summary.scalar('Loss/G_Total', losses['g_loss'], step=global_step)\n",
    "                tf.summary.scalar('Loss/MA_GP', losses['ma_gp'], step=global_step)\n",
    "                tf.summary.scalar('Loss/D_Real', losses['errD_real'], step=global_step)\n",
    "                tf.summary.scalar('Loss/D_Fake', losses['errD_fake'], step=global_step)\n",
    "                tf.summary.scalar('Loss/D_Mis', losses['errD_mis'], step=global_step)\n",
    "                \n",
    "                if global_step % 100 == 0:\n",
    "                    tf.summary.scalar('LR/Generator', g_optimizer.learning_rate, step=global_step)\n",
    "                    tf.summary.scalar('LR/Discriminator', d_optimizer.learning_rate, step=global_step)\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'D': f\"{losses['d_loss']:.3f}\", \n",
    "                    'G': f\"{losses['g_loss']:.3f}\",\n",
    "                    'GP': f\"{losses['ma_gp']:.3f}\"\n",
    "                })\n",
    "            global_step += 1\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        avg_g = epoch_metrics['g_loss'] / (step + 1)\n",
    "        avg_d = epoch_metrics['d_loss'] / (step + 1)\n",
    "        print(f\"Time: {time.time()-start_time:.1f}s | Avg G: {avg_g:.4f} | Avg D: {avg_d:.4f}\")\n",
    "        \n",
    "        # Log Epoch Averages\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('Epoch/G_Loss', avg_g, step=epoch)\n",
    "            tf.summary.scalar('Epoch/D_Loss', avg_d, step=epoch)\n",
    "\n",
    "        # Save Checkpoint\n",
    "        if (epoch + 1) % args['SAVE_FREQ'] == 0:\n",
    "            manager.save()\n",
    "            \n",
    "        # Save & Log Samples\n",
    "        if (epoch + 1) % args['SAMPLE_FREQ'] == 0:\n",
    "            # Generate samples\n",
    "            text_embeds = text_encoder(fixed_ids, fixed_mask, training=False)\n",
    "            fake_imgs = generator([fixed_noise, text_embeds], training=False)\n",
    "            \n",
    "            # Save to disk\n",
    "            save_sample_images(generator, text_encoder, fixed_ids, fixed_mask, fixed_noise, \n",
    "                             epoch+1, os.path.join(args['RUN_DIR'], 'samples'))\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            with summary_writer.as_default():\n",
    "                # Convert [-1, 1] -> [0, 1]\n",
    "                imgs_vis = (fake_imgs + 1.0) * 0.5\n",
    "                tf.summary.image('Generated Samples', imgs_vis, step=epoch, max_outputs=16)\n",
    "                \n",
    "    print(f\"\\n✓ Training Completed. Results in {args['RUN_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Run Directory: ./runs/20251125-002901\n",
      "Config: {\n",
      "  \"IMAGE_SIZE\": [\n",
      "    64,\n",
      "    64,\n",
      "    3\n",
      "  ],\n",
      "  \"NGF\": 32,\n",
      "  \"NDF\": 64,\n",
      "  \"Z_DIM\": 100,\n",
      "  \"EMBED_DIM\": 512,\n",
      "  \"LR_G\": 0.0001,\n",
      "  \"LR_D\": 0.0004,\n",
      "  \"BATCH_SIZE\": 128,\n",
      "  \"N_EPOCH\": 100,\n",
      "  \"LAMBDA_GP\": 2.0,\n",
      "  \"RUN_DIR\": \"./runs/20251125-002901\",\n",
      "  \"SAVE_FREQ\": 5,\n",
      "  \"SAMPLE_FREQ\": 1,\n",
      "  \"USE_DIFFAUG\": false,\n",
      "  \"DIFFAUG_POLICY\": \"translation\",\n",
      "  \"N_SAMPLE\": 7370\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Define configuration for training\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create a unique run directory\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_dir = f\"./runs/{run_id}\"\n",
    "if not os.path.exists(run_dir):\n",
    "    os.makedirs(run_dir)\n",
    "\n",
    "# User provided config\n",
    "config = {\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    'NGF': 64,\n",
    "    'NDF': 64,\n",
    "    'Z_DIM': 100,\n",
    "    'EMBED_DIM': 512,\n",
    "    'LR_G': 0.0001,\n",
    "    'LR_D': 0.0004,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'N_EPOCH': 600,          # Updated to 600 for good results\n",
    "    'LAMBDA_GP': 2.0,\n",
    "    'RUN_DIR': run_dir,\n",
    "    'SAVE_FREQ': 25,         # Save less frequently to save space\n",
    "    'SAMPLE_FREQ': 5,        # Sample every 5 epochs\n",
    "    'USE_DIFFAUG': True,     # ENABLED: Critical for Oxford-102\n",
    "    'DIFFAUG_POLICY': 'translation',\n",
    "    'N_SAMPLE': num_training_sample if 'num_training_sample' in locals() else 7370\n",
    "}\n",
    "\n",
    "# Save config for reproducibility\n",
    "with open(os.path.join(run_dir, 'config.json'), 'w') as f:\n",
    "    # Filter for JSON serializable values\n",
    "    json_config = {k: v for k, v in config.items() if isinstance(v, (int, float, str, list, bool))}\n",
    "    json.dump(json_config, f, indent=4)\n",
    "\n",
    "print(f\"Training Run Directory: {run_dir}\")\n",
    "print(f\"Config: {json.dumps(json_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Models (Image Size: [64, 64, 3]) ---\n",
      "--- Loading CLIP Text Encoder ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Models (Image Size: [64, 64, 3]) ---\n",
      "--- Loading CLIP Text Encoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Models (Image Size: [64, 64, 3]) ---\n",
      "--- Loading CLIP Text Encoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TensorBoard launched (PID: 19448)\n",
      "--- DiffAugment Disabled ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Models (Image Size: [64, 64, 3]) ---\n",
      "--- Loading CLIP Text Encoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TensorBoard launched (PID: 19448)\n",
      "--- DiffAugment Disabled ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Models (Image Size: [64, 64, 3]) ---\n",
      "--- Loading CLIP Text Encoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TensorBoard launched (PID: 19448)\n",
      "--- DiffAugment Disabled ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training for 100 Epochs ---\n",
      "Note: First step takes longer due to XLA compilation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/57 [00:00<?, ?it/s]\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.2 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.2 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "Epoch 1:   4%|▎         | 2/57 [00:53<23:43, 25.89s/it, D=2.919, G=0.502, GP=0.922]"
     ]
    }
   ],
   "source": [
    "# 'dataset' is the tf.data.Dataset object you created in the notebook\n",
    "train(dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "\t\th, w = images.shape[1], images.shape[2]\n",
    "\t\timg = np.zeros((h * size[0], w * size[1], 3))\n",
    "\t\tfor idx, image in enumerate(images):\n",
    "\t\t\t\ti = idx % size[1]\n",
    "\t\t\t\tj = idx // size[1]\n",
    "\t\t\t\timg[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\t\treturn img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "\t\t# getting the pixel values between [0, 1] to save it\n",
    "\t\treturn plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "\t\treturn imsave(images, size, image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for visualization during training\n",
    "# IMPORTANT: All three variables must have the same batch size!\n",
    "\n",
    "sample_size = BATCH_SIZE  # Current: 32\n",
    "ni = int(np.ceil(np.sqrt(sample_size)))  # Grid size for visualization\n",
    "\n",
    "# Create random noise seed\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "# Define 8 diverse sample sentences\n",
    "base_sentences = [\n",
    "\t\t\"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "\t\t\"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "\t\t\"the petals on this flower are white with a yellow center\",\n",
    "\t\t\"this flower has a lot of small round pink petals.\",\n",
    "\t\t\"this flower is orange in color, and has petals that are ruffled and rounded.\",\n",
    "\t\t\"the flower has yellow petals and the center of it is brown.\",\n",
    "\t\t\"this flower has petals that are blue and white.\",\n",
    "\t\t\"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "]\n",
    "\n",
    "# Repeat sentences to match sample_size (batch size)\n",
    "sample_sentences = []\n",
    "for i in range(sample_size):\n",
    "\t\tsample_sentences.append(base_sentences[i % len(base_sentences)])\n",
    "\n",
    "# Tokenize with CLIP\n",
    "sample_encoded = preprocess_text_clip(sample_sentences, max_length=77)\n",
    "sample_input_ids = sample_encoded['input_ids']\n",
    "sample_attention_mask = sample_encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string\n",
    "\t\t\t\tindex: Test sample ID\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tinput_ids, attention_mask, index\n",
    "\t\t\"\"\"\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing dataset generator - decodes IDs to raw text\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_pickle('./dataset/testData.pkl')\n",
    "\t\tcaptions_ids = data['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\tchosen_caption_ids = captions_ids[i]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\tindex = data['ID'].values\n",
    "\t\tindex = np.asarray(index)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.repeat().batch(batch_size)\n",
    "\t\t\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference directory inside the run directory\n",
    "inference_dir = os.path.join(config['RUN_DIR'], 'inference')\n",
    "if not os.path.exists(inference_dir):\n",
    "    os.makedirs(inference_dir)\n",
    "print(f\"Inference Directory: {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, config):\n",
    "    print(\"--- Starting Inference ---\")\n",
    "    \n",
    "    # 1. Re-initialize Models\n",
    "    # We need to re-create the models to load weights into them\n",
    "    print(\"Loading models...\")\n",
    "    generator = NetG(ngf=config['NGF'], nz=config['Z_DIM'], cond_dim=config['EMBED_DIM'])\n",
    "    text_encoder = ClipTextEncoder()\n",
    "    \n",
    "    # Dummy call to build the model (optional but good practice)\n",
    "    # generator.build((None, config['Z_DIM'])) \n",
    "    \n",
    "    # 2. Load Checkpoint\n",
    "    checkpoint_dir = os.path.join(config['RUN_DIR'], 'checkpoints')\n",
    "    \n",
    "    # We need to restore the generator. \n",
    "    # Note: We must define the checkpoint object exactly as it was saved to restore correctly,\n",
    "    # or use expect_partial() if we only care about specific parts (like generator).\n",
    "    checkpoint = tf.train.Checkpoint(generator=generator)\n",
    "    \n",
    "    latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_ckpt:\n",
    "        print(f\"Loading weights from: {latest_ckpt}\")\n",
    "        status = checkpoint.restore(latest_ckpt).expect_partial()\n",
    "        status.assert_existing_objects_matched()\n",
    "        print(\"✓ Weights loaded successfully\")\n",
    "    else:\n",
    "        print(\"⚠ NO CHECKPOINT FOUND! Generating with random weights (Garbage output).\")\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    total_images = 0\n",
    "    pbar = tqdm(total=NUM_TEST, desc='Generating images', unit='img')\n",
    "    \n",
    "    for input_ids, attention_mask, idx in dataset:\n",
    "        current_batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Generate Noise\n",
    "        noise = tf.random.normal([current_batch_size, config['Z_DIM']])\n",
    "        \n",
    "        # Encode Text\n",
    "        text_embeds = text_encoder(input_ids, attention_mask, training=False)\n",
    "        \n",
    "        # Generate Images\n",
    "        fake_imgs = generator([noise, text_embeds], training=False)\n",
    "        \n",
    "        # Save Images\n",
    "        for i in range(current_batch_size):\n",
    "            img_idx = idx[i].numpy()\n",
    "            img_path = os.path.join(inference_dir, f'inference_{img_idx:04d}.jpg')\n",
    "            \n",
    "            # Rescale [-1, 1] -> [0, 1]\n",
    "            img_to_save = (fake_imgs[i].numpy() + 1.0) * 0.5\n",
    "            \n",
    "            # Clip to ensure valid range\n",
    "            img_to_save = np.clip(img_to_save, 0.0, 1.0)\n",
    "            \n",
    "            plt.imsave(img_path, img_to_save)\n",
    "            \n",
    "            total_images += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "        if total_images >= NUM_TEST:\n",
    "            break\n",
    "            \n",
    "    pbar.close()\n",
    "    print(f\"✓ Generated {total_images} images to {inference_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "\t\tprint(f'⚠ No images found in {inference_dir}')\n",
    "\t\tprint('Please run the inference cell first!')\n",
    "else:\n",
    "\t\t# Randomly sample 20 images\n",
    "\t\tnp.random.seed(42)  # For reproducibility\n",
    "\t\tnum_samples = min(20, len(image_files))\n",
    "\t\tsample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "\t\tsample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "\t\t# Create 4x5 grid\n",
    "\t\tfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\t\taxes = axes.flatten()\n",
    "\n",
    "\t\tfor idx, img_path in enumerate(sample_files):\n",
    "\t\t\t\t# Extract image ID from filename\n",
    "\t\t\t\timg_id = int(Path(img_path).stem.split('_')[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Find caption\n",
    "\t\t\t\tcaption_idx = np.where(test_ids == img_id)[0][0]\n",
    "\t\t\t\tcaption_ids = test_captions[caption_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode caption\n",
    "\t\t\t\tcaption_text = ''\n",
    "\t\t\t\tfor word_id in caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':\n",
    "\t\t\t\t\t\t\t\tcaption_text += word + ' '\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Load and display image\n",
    "\t\t\t\timg = plt.imread(img_path)\n",
    "\t\t\t\taxes[idx].imshow(img)\n",
    "\t\t\t\taxes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\t# Hide unused subplots if less than 20 images\n",
    "\t\tfor idx in range(num_samples, 20):\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\tprint(f'\\nTotal generated images: {len(image_files)}')\n",
    "\t\tprint(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
