{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Text to Image</center></h1>\n",
    "\n",
    "<h2 id=\"Platform:-Kaggle\">Platform: <a href=\"https://www.kaggle.com/competitions/2025-datalab-cup-3-reverse-image-caption/overview\">Kaggle</a><a class=\"anchor-link\" href=\"#Platform:-Kaggle\">¶</a></h2>\n",
    "<h2 id=\"Overview\">Overview<a class=\"anchor-link\" href=\"#Overview\">¶</a></h2>\n",
    "<p>In this work, we are interested in translating text in the form of single-sentence human-written descriptions directly into image pixels. For example, \"<strong>this flower has petals that are yellow and has a ruffled stamen</strong>\" and \"<strong>this pink and yellow flower has a beautiful yellow center with many stamens</strong>\". You have to develop a novel deep architecture and GAN formulation to effectively translate visual concepts from characters to pixels.</p>\n",
    "\n",
    "<p>More specifically, given a set of texts, your task is to generate reasonable images with size 64x64x3 to illustrate the corresponding texts. Here we use <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford-102 flower dataset</a> and its <a href=\"https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view\">paired texts</a> as our training dataset.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/example.png\"/>\n",
    "\n",
    "<ul>\n",
    "<li>7370 images as training set, where each images is annotated with at most 10 texts.</li>\n",
    "<li>819 texts for testing. You must generate 1 64x64x3 image for each text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN\">Conditional GAN<a class=\"anchor-link\" href=\"#Conditional-GAN\">¶</a></h2>\n",
    "<p>Given a text, in order to generate the image which can illustrate it, our model must meet several requirements:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Our model should have ability to understand and extract the meaning of given texts.<ul>\n",
    "<li>Use RNN or other language model, such as BERT, ELMo or XLNet, to capture the meaning of text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Our model should be able to generate image.<ul>\n",
    "<li>Use GAN to generate high quality image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>GAN-generated image should illustrate the text.<ul>\n",
    "<li>Use conditional-GAN to generate image conditioned on given text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information $y$. We can perform the conditioning by feeding $y$ into both the discriminator and generator as additional input layer.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/cGAN.png\" width=\"500\"/>\n",
    "\n",
    "<p>There are two motivations for using some extra information in a GAN model:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Improve GAN.</li>\n",
    "<li>Generate targeted image.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Additional information that is correlated with the input images, such as class labels, can be used to improve the GAN. This improvement may come in the form of more stable training, faster training, and/or generated images that have better quality.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/GANCLS.jpg\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using DistilBERT tokenizer (sent2IdList removed)\n"
     ]
    }
   ],
   "source": [
    "# This cell previously contained sent2IdList() function\n",
    "# It has been removed as we now use DistilBERT tokenizer instead\n",
    "# The id2word_dict is still available from cell 6 for visualization purposes\n",
    "\n",
    "print(\"✓ Using DistilBERT tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Augmentation: ENABLED\n",
      "Active augmentations: random_flip_horizontal, random_rotation, random_brightness, random_contrast, random_saturation, random_hue\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation Configuration\n",
    "# Define this BEFORE training_data_generator to avoid reference issues\n",
    "aug_config = {\n",
    "    'enabled': True,                      # Master switch for augmentation\n",
    "    'random_flip_horizontal': True,       # Flowers can be mirrored\n",
    "    'random_flip_vertical': False,        # Flowers typically grow upward\n",
    "    'random_rotation': True,              # Any rotation is valid for flowers\n",
    "    'random_brightness': 0.15,            # Lighting variations (max delta)\n",
    "    'random_contrast': (0.9, 1.1),        # Subtle contrast changes (lower, upper)\n",
    "    'random_saturation': (0.9, 1.1),      # Color intensity (lower, upper)\n",
    "    'random_hue': 0.05,                   # Small color shifts (max delta)\n",
    "}\n",
    "\n",
    "print('Data Augmentation:', 'ENABLED' if aug_config['enabled'] else 'DISABLED')\n",
    "if aug_config['enabled']:\n",
    "    enabled_augs = [k for k, v in aug_config.items() if k != 'enabled' and v]\n",
    "    print(f'Active augmentations: {\", \".join(enabled_augs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def preprocess_text_distilbert(text, max_length=64):\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "    \"\"\"\n",
    "    Updated data generator using DistilBERT tokenization\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string (not IDs!)\n",
    "        image_path: Path to image file\n",
    "    \n",
    "    Returns:\n",
    "        img, input_ids, attention_mask\n",
    "    \"\"\"\n",
    "    # ============= IMAGE PROCESSING (same as before) =============\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    # Data augmentation (only applied during training)\n",
    "    if aug_config['enabled']:\n",
    "        if aug_config['random_flip_horizontal']:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "        \n",
    "        if aug_config['random_flip_vertical']:\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "        \n",
    "        if aug_config['random_rotation']:\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        if aug_config['random_brightness']:\n",
    "            img = tf.image.random_brightness(img, aug_config['random_brightness'])\n",
    "        \n",
    "        if aug_config['random_contrast']:\n",
    "            img = tf.image.random_contrast(img, \n",
    "                                          aug_config['random_contrast'][0], \n",
    "                                          aug_config['random_contrast'][1])\n",
    "        \n",
    "        if aug_config['random_saturation']:\n",
    "            img = tf.image.random_saturation(img, \n",
    "                                            aug_config['random_saturation'][0], \n",
    "                                            aug_config['random_saturation'][1])\n",
    "        \n",
    "        if aug_config['random_hue']:\n",
    "            img = tf.image.random_hue(img, aug_config['random_hue'])\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "    \n",
    "    # Normalize to [-1, 1] to match generator's tanh output\n",
    "    img = (img * 2.0) - 1.0\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    # ============= TEXT PROCESSING (NEW: Use DistilBERT tokenizer) =============\n",
    "    def tokenize_caption(text):\n",
    "        \"\"\"Python function to tokenize text using DistilBERT tokenizer\"\"\"\n",
    "        # Convert EagerTensor to bytes, then decode to string\n",
    "        text = text.numpy().decode('utf-8')\n",
    "        \n",
    "        # Tokenize using DistilBERT\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors='np'  # Use numpy arrays for TF compatibility\n",
    "        )\n",
    "        \n",
    "        return encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "    \n",
    "    # Use tf.py_function to call Python tokenizer\n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize_caption,\n",
    "        inp=[caption_text],\n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    # Set shapes explicitly\n",
    "    input_ids.set_shape([64])\n",
    "    attention_mask.set_shape([64])\n",
    "    \n",
    "    return img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    \"\"\"\n",
    "    Updated dataset generator to work with raw text (decoded from IDs)\n",
    "    \"\"\"\n",
    "    # Load the training data\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions_ids = df['Captions'].values\n",
    "    caption_texts = []\n",
    "    \n",
    "    # Decode pre-tokenized IDs back to raw text\n",
    "    for i in range(len(captions_ids)):\n",
    "        # Randomly choose one caption (list of ID lists)\n",
    "        chosen_caption_ids = random.choice(captions_ids[i])\n",
    "        \n",
    "        # Decode IDs back to text using id2word_dict\n",
    "        words = []\n",
    "        for word_id in chosen_caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':  # Skip padding tokens\n",
    "                words.append(word)\n",
    "        \n",
    "        caption_text = ' '.join(words)\n",
    "        caption_texts.append(caption_text)\n",
    "    \n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    # Verify same length\n",
    "    assert len(caption_texts) == len(image_paths)\n",
    "    \n",
    "    # Create dataset from raw text and image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption_texts, image_paths))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption_texts)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¶</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel\n",
    "\n",
    "class DistillBertEncoder(tf.keras.Model):\n",
    "    def __init__(self, output_dim=128, freeze_bert=True):\n",
    "        super(DistillBertEncoder, self).__init__()\n",
    "        \n",
    "        self.distilbert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        if(freeze_bert):\n",
    "            self.distilbert.trainable = False\n",
    "\n",
    "        self.projection = tf.keras.layers.Dense(output_dim, activation='relu')\n",
    "    \n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, input_ids, attention_mask, training=False):\n",
    "        outputs = self.distilbert(input_ids, attention_mask=attention_mask, training=training)\n",
    "\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        cls_embedding = self.dropout(cls_embedding, training=training)\n",
    "\n",
    "        text_features = self.projection(cls_embedding)\n",
    "\n",
    "        return text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Generator\">Generator<a class=\"anchor-link\" href=\"#Generator\">¶</a></h2>\n",
    "<p>A image generator which generates the target image illustrating the input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: hidden representation of input text and random noise z with random seed.</li>\n",
    "<li>Output: target image, which is conditioned on the given text, in size 64x64x3.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization as per DCGAN paper\n",
    "def dcgan_weight_init():\n",
    "    \"\"\"Returns weight initializer for DCGAN: Normal(mean=0, stddev=0.02)\"\"\"\n",
    "    return tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    DCGAN Generator for 64x64 images\n",
    "    Uses transposed convolutions to progressively upsample from noise+text\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # Weight initializer\n",
    "        init = dcgan_weight_init()\n",
    "        \n",
    "        # Project and reshape\n",
    "        # Input: [batch, z_dim + text_embed_dim] (e.g., 512 + 128 = 640)\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            4 * 4 * 1024,  # Will reshape to [batch, 4, 4, 1024]\n",
    "            use_bias=False,\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        self.bn0 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Transposed convolutions for upsampling\n",
    "        # 4x4 -> 8x8\n",
    "        self.conv1 = tf.keras.layers.Conv2DTranspose(\n",
    "            512, kernel_size=4, strides=2, padding='same',\n",
    "            use_bias=False, kernel_initializer=init\n",
    "        )\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 8x8 -> 16x16\n",
    "        self.conv2 = tf.keras.layers.Conv2DTranspose(\n",
    "            256, kernel_size=4, strides=2, padding='same',\n",
    "            use_bias=False, kernel_initializer=init\n",
    "        )\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 16x16 -> 32x32\n",
    "        self.conv3 = tf.keras.layers.Conv2DTranspose(\n",
    "            128, kernel_size=4, strides=2, padding='same',\n",
    "            use_bias=False, kernel_initializer=init\n",
    "        )\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 32x32 -> 64x64 (final output)\n",
    "        self.conv4 = tf.keras.layers.Conv2DTranspose(\n",
    "            3, kernel_size=4, strides=2, padding='same',\n",
    "            use_bias=False, kernel_initializer=init\n",
    "        )\n",
    "        # No batch norm on output layer\n",
    "        \n",
    "    def call(self, text, noise_z, training=True):\n",
    "        # Concatenate noise and text embeddings\n",
    "        # text shape: [batch, text_embed_dim] (e.g., [16, 128])\n",
    "        # noise_z shape: [batch, z_dim] (e.g., [16, 512])\n",
    "        x = tf.concat([noise_z, text], axis=1)  # [batch, 640]\n",
    "        \n",
    "        # Project and reshape\n",
    "        x = self.dense(x)  # [batch, 4*4*1024]\n",
    "        x = self.bn0(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.reshape(x, [-1, 4, 4, 1024])  # [batch, 4, 4, 1024]\n",
    "        \n",
    "        # Upsample: 4x4 -> 8x8\n",
    "        x = self.conv1(x)  # [batch, 8, 8, 512]\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Upsample: 8x8 -> 16x16\n",
    "        x = self.conv2(x)  # [batch, 16, 16, 256]\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Upsample: 16x16 -> 32x32\n",
    "        x = self.conv3(x)  # [batch, 32, 32, 128]\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Upsample: 32x32 -> 64x64 (final)\n",
    "        x = self.conv4(x)  # [batch, 64, 64, 3]\n",
    "        output = tf.nn.tanh(x)  # Output in range [-1, 1]\n",
    "        \n",
    "        # Return both for compatibility with existing training code\n",
    "        return x, output  # logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Discriminator\">Discriminator<a class=\"anchor-link\" href=\"#Discriminator\">¶</a></h2>\n",
    "<p>A binary classifier which can discriminate the real and fake image:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Real image<ul>\n",
    "<li>Input: real image and the paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 1.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Fake Image<ul>\n",
    "<li>Input: generated image and paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 0.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    WGAN-GP Critic for 64x64 images\n",
    "    Key differences from DCGAN Discriminator:\n",
    "    1. NO batch normalization (causes issues with gradient penalty)\n",
    "    2. NO sigmoid activation (outputs raw scores)\n",
    "    3. Uses LeakyReLU throughout\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # Weight initializer\n",
    "        init = dcgan_weight_init()\n",
    "        \n",
    "        # Strided convolutions for downsampling\n",
    "        # 64x64 -> 32x32 (NO batch norm on first layer)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            64, kernel_size=4, strides=2, padding='same',\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        \n",
    "        # 32x32 -> 16x16 (NO batch norm!)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            128, kernel_size=4, strides=2, padding='same',\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        \n",
    "        # 16x16 -> 8x8 (NO batch norm!)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            256, kernel_size=4, strides=2, padding='same',\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        \n",
    "        # 8x8 -> 4x4 (NO batch norm!)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(\n",
    "            512, kernel_size=4, strides=2, padding='same',\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        \n",
    "        # Text conditioning layers\n",
    "        self.text_dense = tf.keras.layers.Dense(\n",
    "            512, kernel_initializer=init\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.final = tf.keras.layers.Dense(1, kernel_initializer=init)\n",
    "        \n",
    "    def call(self, img, text, training=True):\n",
    "        # Image path: 64x64x3 -> 4x4x512\n",
    "        x = self.conv1(img)  # [batch, 32, 32, 64]\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv2(x)  # [batch, 16, 16, 128]\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv3(x)  # [batch, 8, 8, 256]\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv4(x)  # [batch, 4, 4, 512]\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        # Flatten image features\n",
    "        x = self.flatten(x)  # [batch, 8192]\n",
    "        \n",
    "        # Process text\n",
    "        text_features = self.text_dense(text)  # [batch, 512]\n",
    "        text_features = tf.nn.leaky_relu(text_features, alpha=0.2)\n",
    "        \n",
    "        # Concatenate image and text features\n",
    "        combined = tf.concat([x, text_features], axis=1)  # [batch, 8704]\n",
    "        \n",
    "        # Final output - RAW SCORES (no sigmoid!)\n",
    "        output = self.final(combined)  # [batch, 1]\n",
    "        \n",
    "        return output  # Return only scores, not probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,\n",
    "    'EMBED_DIM': 256,\n",
    "    'VOCAB_SIZE': len(word2Id_dict),\n",
    "    'RNN_HIDDEN_SIZE': 128,\n",
    "    'Z_DIM': 512,\n",
    "    'DENSE_DIM': 128,\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': 2e-4,\n",
    "    'BETA_1': 0.0,        # WGAN-GP: use 0.0 instead of 0.5\n",
    "    'BETA_2': 0.9,        # WGAN-GP: use 0.9\n",
    "    'N_CRITIC': 5,        # NEW: critic iterations per generator iteration\n",
    "    'LAMBDA_GP': 10.0,    # NEW: gradient penalty weight\n",
    "    'N_EPOCH': 100,\n",
    "    'N_SAMPLE': num_training_sample,\n",
    "    'PRINT_FREQ': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "text_encoder = DistillBertEncoder(output_dim=hparas['RNN_HIDDEN_SIZE'], freeze_bert=True)\n",
    "generator = Generator(hparas)\n",
    "critic = Critic(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Loss-Function-and-Optimization\">Loss Function and Optimization<a class=\"anchor-link\" href=\"#Loss-Function-and-Optimization\">¶</a></h2>\n",
    "<p>Although the conditional GAN model is quite complex, the loss function used to optimize the network is relatively simple. Actually, it is simply a binary classification task, thus we use cross entropy as our loss.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss_critic(real_scores, fake_scores):\n",
    "    \"\"\"\n",
    "    Wasserstein loss for critic\n",
    "    Critic wants to maximize: E[critic(real)] - E[critic(fake)]\n",
    "    So we minimize: E[critic(fake)] - E[critic(real)]\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(fake_scores) - tf.reduce_mean(real_scores)\n",
    "\n",
    "def wasserstein_loss_generator(fake_scores):\n",
    "    \"\"\"\n",
    "    Wasserstein loss for generator\n",
    "    Generator wants to maximize: E[critic(fake)]\n",
    "    So we minimize: -E[critic(fake)]\n",
    "    \"\"\"\n",
    "    return -tf.reduce_mean(fake_scores)\n",
    "\n",
    "def gradient_penalty(critic, real_images, fake_images, text_embed, batch_size):\n",
    "    \"\"\"\n",
    "    Gradient penalty for WGAN-GP\n",
    "    \n",
    "    Computes ||∇_x critic(x)||₂ for interpolated images x\n",
    "    Penalty = λ * mean((||gradient|| - 1)²)\n",
    "    \"\"\"\n",
    "    # Random weight for interpolation\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    \n",
    "    # Interpolated images: x_hat = alpha * real + (1 - alpha) * fake\n",
    "    interpolated = alpha * real_images + (1.0 - alpha) * fake_images\n",
    "    \n",
    "    # Compute critic scores on interpolated images\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        interpolated_scores = critic(interpolated, text_embed, training=True)\n",
    "    \n",
    "    # Compute gradients of scores w.r.t. interpolated images\n",
    "    gradients = gp_tape.gradient(interpolated_scores, [interpolated])[0]\n",
    "    \n",
    "    # Compute L2 norm of gradients for each sample\n",
    "    # gradients shape: [batch, 64, 64, 3]\n",
    "    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "    \n",
    "    # Gradient penalty: mean((||gradient|| - 1)²)\n",
    "    gradient_penalty = tf.reduce_mean(tf.square(gradients_norm - 1.0))\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# WGAN-GP: Use Adam with beta_1=0.0, beta_2=0.9\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=hparas['LR'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=hparas['LR'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    critic_optimizer=critic_optimizer,\n",
    "    text_encoder=text_encoder,\n",
    "    generator=generator,\n",
    "    critic=critic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wasserstein_distance(real_scores, fake_scores):\n",
    "    \"\"\"\n",
    "    Approximation of Wasserstein distance\n",
    "    Higher is better (critic getting better at distinguishing)\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(real_scores) - tf.reduce_mean(fake_scores)\n",
    "\n",
    "def calculate_gradient_norm(gradients):\n",
    "    \"\"\"Calculate L2 norm of gradients\"\"\"\n",
    "    squared_norms = [tf.reduce_sum(tf.square(g)) for g in gradients if g is not None]\n",
    "    total_norm = tf.sqrt(tf.reduce_sum(squared_norms))\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    WGAN-GP training step with n_critic iterations\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(real_image)[0]\n",
    "    \n",
    "    # Encode text once (used for both critic and generator)\n",
    "    text_embed = text_encoder(input_ids, attention_mask, training=True)\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Critic (multiple iterations)\n",
    "    # ============================================================\n",
    "    for _ in range(hparas['N_CRITIC']):\n",
    "        noise = tf.random.normal([batch_size, hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "        \n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # Generate fake images\n",
    "            _, fake_image = generator(text_embed, noise, training=True)\n",
    "            \n",
    "            # Get critic scores\n",
    "            real_scores = critic(real_image, text_embed, training=True)\n",
    "            fake_scores = critic(fake_image, text_embed, training=True)\n",
    "            \n",
    "            # Wasserstein loss\n",
    "            c_loss_wasserstein = wasserstein_loss_critic(real_scores, fake_scores)\n",
    "            \n",
    "            # Gradient penalty\n",
    "            gp = gradient_penalty(critic, real_image, fake_image, text_embed, batch_size)\n",
    "            \n",
    "            # Total critic loss\n",
    "            c_loss = c_loss_wasserstein + hparas['LAMBDA_GP'] * gp\n",
    "        \n",
    "        # Update critic\n",
    "        grad_c = critic_tape.gradient(c_loss, critic.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(grad_c, critic.trainable_variables))\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Generator (once per n_critic iterations)\n",
    "    # ============================================================\n",
    "    noise = tf.random.normal([batch_size, hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        _, fake_image = generator(text_embed, noise, training=True)\n",
    "        fake_scores = critic(fake_image, text_embed, training=True)\n",
    "        g_loss = wasserstein_loss_generator(fake_scores)\n",
    "    \n",
    "    # Update generator\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    wasserstein_dist = calculate_wasserstein_distance(real_scores, fake_scores)\n",
    "    grad_norm_g = calculate_gradient_norm(grad_g)\n",
    "    grad_norm_c = calculate_gradient_norm(grad_c)\n",
    "    \n",
    "    return {\n",
    "        'g_loss': g_loss,\n",
    "        'c_loss': c_loss,\n",
    "        'c_loss_wasserstein': c_loss_wasserstein,\n",
    "        'gp': gp,\n",
    "        'wasserstein_dist': wasserstein_dist,\n",
    "        'grad_norm_g': grad_norm_g,\n",
    "        'grad_norm_c': grad_norm_c\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(input_ids, attention_mask, noise):\n",
    "    # Encode text with DistilBERT (no hidden state)\n",
    "    text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "    _, fake_image = generator(text_embed, noise, training=False)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentences tokenized: 16 sentences\n",
      "Input IDs shape: (16, 64)\n",
      "Attention mask shape: (16, 64)\n"
     ]
    }
   ],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "# Fix: 8 sentences × 2 repetitions = 16 total (matching sample_size and sample_seed batch dimension)\n",
    "sample_sentences = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/(2*ni)) + \\\n",
    "                   [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/(2*ni)) + \\\n",
    "                   [\"the petals on this flower are white with a yellow center\"] * int(sample_size/(2*ni)) + \\\n",
    "                   [\"this flower has a lot of small round pink petals.\"] * int(sample_size/(2*ni)) + \\\n",
    "                   [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/(2*ni)) + \\\n",
    "                   [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/(2*ni)) + \\\n",
    "                   [\"this flower has petals that are blue and white.\"] * int(sample_size/(2*ni)) +\\\n",
    "                   [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/(2*ni))\n",
    "\n",
    "# Tokenize with DistilBERT (no more sent2IdList!)\n",
    "sample_encoded = preprocess_text_distilbert(sample_sentences, max_length=64)\n",
    "sample_input_ids = sample_encoded['input_ids']\n",
    "sample_attention_mask = sample_encoded['attention_mask']\n",
    "\n",
    "print(f\"Sample sentences tokenized: {len(sample_sentences)} sentences\")\n",
    "print(f\"Input IDs shape: {sample_input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {sample_attention_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing training runs...\n",
      "================================================================================\n",
      "Available Training Runs:\n",
      "================================================================================\n",
      "20251116-204432  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251116-204819  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251116-225453  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251116-230026  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251116-230343  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251117-012045  |  Checkpoint: ✗  |  Config: ✗  |  Samples: 0\n",
      "20251117-013543  |  Checkpoint: ✗  |  Config: ✗  |  Samples: 0\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['20251116-204432',\n",
       " '20251116-204819',\n",
       " '20251116-225453',\n",
       " '20251116-230026',\n",
       " '20251116-230343',\n",
       " '20251117-012045',\n",
       " '20251117-013543']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper functions for managing training runs\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def list_available_runs():\n",
    "    \"\"\"List all available training runs with their details\"\"\"\n",
    "    run_dirs = sorted(glob.glob('runs/*/'))\n",
    "    \n",
    "    if not run_dirs:\n",
    "        print('No training runs found in runs/ directory')\n",
    "        return []\n",
    "    \n",
    "    print('=' * 80)\n",
    "    print('Available Training Runs:')\n",
    "    print('=' * 80)\n",
    "    \n",
    "    available_runs = []\n",
    "    for run_dir in run_dirs:\n",
    "        timestamp = run_dir.split('/')[-2]\n",
    "        available_runs.append(timestamp)\n",
    "        \n",
    "        # Check for checkpoints\n",
    "        checkpoint_dir = f'{run_dir}checkpoints'\n",
    "        latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        has_checkpoint = '✓' if latest_ckpt else '✗'\n",
    "        \n",
    "        # Check for config\n",
    "        config_path = f'{run_dir}config.json'\n",
    "        has_config = '✓' if os.path.exists(config_path) else '✗'\n",
    "        \n",
    "        # Count sample images\n",
    "        sample_count = len(glob.glob(f'{run_dir}samples/*.jpg'))\n",
    "        \n",
    "        print(f'{timestamp}  |  Checkpoint: {has_checkpoint}  |  Config: {has_config}  |  Samples: {sample_count}')\n",
    "        \n",
    "        if latest_ckpt:\n",
    "            print(f'  └─ Latest checkpoint: {latest_ckpt}')\n",
    "    \n",
    "    print('=' * 80)\n",
    "    return available_runs\n",
    "\n",
    "def load_run_config(run_timestamp):\n",
    "    \"\"\"Load configuration from a previous run\"\"\"\n",
    "    config_path = f'runs/{run_timestamp}/config.json'\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f'Config not found: {config_path}')\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f'✓ Loaded config from: {config_path}')\n",
    "    return config\n",
    "\n",
    "# List available runs\n",
    "print('Checking for existing training runs...')\n",
    "list_available_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created NEW run directory: runs/20251117-134622\n",
      "\n",
      "Run directory structure:\n",
      "  runs/20251117-134622/\n",
      "  ├── checkpoints/ : runs/20251117-134622/checkpoints\n",
      "  ├── samples/     : runs/20251117-134622/samples\n",
      "  └── inference/   : runs/20251117-134622/inference\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# RESUME TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "# Set to None for new run, or specify run timestamp to resume\n",
    "# Example: RESUME_RUN = '20251116-225453'\n",
    "RESUME_RUN = None  # ← Change this to resume from specific run\n",
    "\n",
    "# ============================================================\n",
    "# RUN DIRECTORY SETUP\n",
    "# ============================================================\n",
    "if RESUME_RUN:\n",
    "    # Resume from existing run\n",
    "    run_dir = f'runs/{RESUME_RUN}'\n",
    "    \n",
    "    # Verify directory exists\n",
    "    if not os.path.exists(run_dir):\n",
    "        raise FileNotFoundError(f'Run directory not found: {run_dir}')\n",
    "    \n",
    "    # Load existing config\n",
    "    try:\n",
    "        prev_config = load_run_config(RESUME_RUN)\n",
    "        run_timestamp = prev_config.get('run_timestamp', RESUME_RUN)\n",
    "        print(f'\\n⟳ RESUMING training from: {run_dir}')\n",
    "        print(f'  Original start: {run_timestamp}')\n",
    "        \n",
    "        # Warn if hyperparameters might be different\n",
    "        if 'hyperparameters' in prev_config:\n",
    "            prev_hparas = prev_config['hyperparameters']\n",
    "            if prev_hparas.get('BATCH_SIZE') != hparas['BATCH_SIZE']:\n",
    "                print(f'  ⚠ WARNING: Batch size changed ({prev_hparas.get(\"BATCH_SIZE\")} → {hparas[\"BATCH_SIZE\"]})')\n",
    "            if prev_hparas.get('LR') != hparas['LR']:\n",
    "                print(f'  ⚠ WARNING: Learning rate changed ({prev_hparas.get(\"LR\")} → {hparas[\"LR\"]})')\n",
    "    except Exception as e:\n",
    "        print(f'⚠ Could not load previous config: {e}')\n",
    "        run_timestamp = RESUME_RUN\n",
    "    \n",
    "    # Use existing subdirectories\n",
    "    checkpoint_dir = f'{run_dir}/checkpoints'\n",
    "    samples_dir = f'{run_dir}/samples'\n",
    "    inference_dir = f'{run_dir}/inference'\n",
    "    \n",
    "    # Create directories if they don't exist (shouldn't happen, but safety check)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    os.makedirs(inference_dir, exist_ok=True)\n",
    "    \n",
    "else:\n",
    "    # Create new run\n",
    "    run_timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    run_dir = f'runs/{run_timestamp}'\n",
    "    \n",
    "    # All outputs for this run go in subdirectories\n",
    "    checkpoint_dir = f'{run_dir}/checkpoints'\n",
    "    samples_dir = f'{run_dir}/samples'\n",
    "    inference_dir = f'{run_dir}/inference'\n",
    "    \n",
    "    # Create all directories\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    os.makedirs(inference_dir, exist_ok=True)\n",
    "    \n",
    "    print(f'✓ Created NEW run directory: {run_dir}')\n",
    "\n",
    "# Display directory structure\n",
    "print(f'\\nRun directory structure:')\n",
    "print(f'  {run_dir}/')\n",
    "print(f'  ├── checkpoints/ : {checkpoint_dir}')\n",
    "print(f'  ├── samples/     : {samples_dir}')\n",
    "print(f'  └── inference/   : {inference_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    global run_dir, checkpoint_dir, samples_dir, inference_dir\n",
    "    \n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    \n",
    "    log_dir = f'{run_dir}/logs'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    print(f'Training run: {run_dir}')\n",
    "    print(f'TensorBoard logs: {log_dir}')\n",
    "    print(f'Model: WGAN-GP with {hparas[\"N_CRITIC\"]} critic iterations')\n",
    "    \n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        c_total_loss = 0\n",
    "        c_total_loss_wasserstein = 0\n",
    "        gp_total = 0\n",
    "        wd_total = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        pbar = tqdm(dataset, desc=f'Epoch {epoch+1}/{hparas[\"N_EPOCH\"]}', \n",
    "                   total=steps_per_epoch, unit='batch')\n",
    "        \n",
    "        for batch_idx, (image, input_ids, attention_mask) in enumerate(pbar):\n",
    "            metrics = train_step(image, input_ids, attention_mask)\n",
    "            \n",
    "            # Accumulate losses\n",
    "            g_total_loss += metrics['g_loss']\n",
    "            c_total_loss += metrics['c_loss']\n",
    "            c_total_loss_wasserstein += metrics['c_loss_wasserstein']\n",
    "            gp_total += metrics['gp']\n",
    "            wd_total += metrics['wasserstein_dist']\n",
    "            \n",
    "            # Log to TensorBoard every batch\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('Losses/generator_loss', metrics['g_loss'], step=global_step)\n",
    "                tf.summary.scalar('Losses/critic_loss_total', metrics['c_loss'], step=global_step)\n",
    "                tf.summary.scalar('Losses/critic_loss_wasserstein', metrics['c_loss_wasserstein'], step=global_step)\n",
    "                tf.summary.scalar('Losses/gradient_penalty', metrics['gp'], step=global_step)\n",
    "                tf.summary.scalar('Metrics/wasserstein_distance', metrics['wasserstein_dist'], step=global_step)\n",
    "                \n",
    "                if global_step % 50 == 0:\n",
    "                    tf.summary.scalar('Gradients/generator_gradient_norm', metrics['grad_norm_g'], step=global_step)\n",
    "                    tf.summary.scalar('Gradients/critic_gradient_norm', metrics['grad_norm_c'], step=global_step)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'G_loss': f'{metrics[\"g_loss\"]:.4f}',\n",
    "                'C_loss': f'{metrics[\"c_loss\"]:.4f}',\n",
    "                'W_dist': f'{metrics[\"wasserstein_dist\"]:.4f}'\n",
    "            })\n",
    "            \n",
    "            global_step += 1\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_g_loss = g_total_loss / steps_per_epoch\n",
    "        avg_c_loss = c_total_loss / steps_per_epoch\n",
    "        avg_c_loss_w = c_total_loss_wasserstein / steps_per_epoch\n",
    "        avg_gp = gp_total / steps_per_epoch\n",
    "        avg_wd = wd_total / steps_per_epoch\n",
    "        epoch_time = time.time() - start\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: G_loss={avg_g_loss:.4f}, C_loss={avg_c_loss:.4f} ' +\n",
    "              f'(W={avg_c_loss_w:.4f}, GP={avg_gp:.4f}), W_dist={avg_wd:.4f}, Time={epoch_time:.2f}s')\n",
    "        \n",
    "        # Log epoch averages\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('Epoch/generator_loss_avg', avg_g_loss, step=epoch)\n",
    "            tf.summary.scalar('Epoch/critic_loss_avg', avg_c_loss, step=epoch)\n",
    "            tf.summary.scalar('Epoch/wasserstein_distance_avg', avg_wd, step=epoch)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            saved_path = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "            print(f'  ✓ Checkpoint saved: {saved_path}')\n",
    "        \n",
    "        # Visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            fake_image = test_step(sample_input_ids, sample_attention_mask, sample_seed)\n",
    "            save_images(fake_image, [ni, ni], f'{samples_dir}/train_{epoch+1:03d}.jpg')\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                display_images = (fake_image + 1.0) / 2.0\n",
    "                tf.summary.image('Generated_Samples', display_images, step=epoch, max_outputs=16)\n",
    "            \n",
    "            print(f'  ✓ Sample image saved and logged to TensorBoard')\n",
    "    \n",
    "    print('\\n✓ Training completed!')\n",
    "    print(f'All outputs saved to: {run_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run: runs/20251117-134622\n",
      "TensorBoard logs: runs/20251117-134622/logs\n",
      "Model: WGAN-GP with 5 critic iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/460 [00:00<?, ?batch/s]/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Epoch 1/100:   1%|          | 5/460 [00:19<29:32,  3.90s/batch, G_loss=-16.4033, C_loss=-87.5314, W_dist=79.2971]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN_EPOCH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhparas[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN_EPOCH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     26\u001b[0m            total\u001b[38;5;241m=\u001b[39msteps_per_epoch, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (image, input_ids, attention_mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m---> 29\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     g_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Evaluation</center></h1>\n",
    "\n",
    "<p><code>dataset/testData.pkl</code> is a pandas dataframe containing testing text with attributes 'ID' and 'Captions'.</p>\n",
    "\n",
    "<ul>\n",
    "<li>'ID': text ID used to name generated image.</li>\n",
    "<li>'Captions': text used as condition to generate image.</li>\n",
    "</ul>\n",
    "\n",
    "<p>For each captions, you need to generate <strong>inference_ID.png</strong> to evaluate quality of generated image. You must name the generated image in this format, otherwise we cannot evaluate your images.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "    \"\"\"\n",
    "    Updated testing data generator using DistilBERT tokenization\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string\n",
    "        index: Test sample ID\n",
    "    \n",
    "    Returns:\n",
    "        input_ids, attention_mask, index\n",
    "    \"\"\"\n",
    "    def tokenize_caption(text):\n",
    "        \"\"\"Python function to tokenize text using DistilBERT tokenizer\"\"\"\n",
    "        # Convert EagerTensor to bytes, then decode to string\n",
    "        text = text.numpy().decode('utf-8')\n",
    "        \n",
    "        # Tokenize using DistilBERT\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors='np'\n",
    "        )\n",
    "        \n",
    "        return encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "    \n",
    "    # Use tf.py_function to call Python tokenizer\n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize_caption,\n",
    "        inp=[caption_text],\n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    # Set shapes explicitly\n",
    "    input_ids.set_shape([64])\n",
    "    attention_mask.set_shape([64])\n",
    "    \n",
    "    return input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    \"\"\"\n",
    "    Updated testing dataset generator - decodes IDs to raw text\n",
    "    \"\"\"\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    captions_ids = data['Captions'].values\n",
    "    caption_texts = []\n",
    "    \n",
    "    # Decode pre-tokenized IDs back to text\n",
    "    for i in range(len(captions_ids)):\n",
    "        chosen_caption_ids = captions_ids[i]\n",
    "        \n",
    "        # Decode IDs back to text using id2word_dict\n",
    "        words = []\n",
    "        for word_id in chosen_caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':  # Skip padding tokens\n",
    "                words.append(word)\n",
    "        \n",
    "        caption_text = ' '.join(words)\n",
    "        caption_texts.append(caption_text)\n",
    "    \n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    # Create dataset from raw text\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference directory is already created by the train() function\n",
    "# No need to create it again here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore checkpoint from the current run directory\n",
    "print(f'Looking for checkpoints in: {checkpoint_dir}')\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    checkpoint.restore(latest_checkpoint)\n",
    "    # Extract checkpoint number from path (e.g., 'ckpt-2' -> 2)\n",
    "    ckpt_num = latest_checkpoint.split('-')[-1]\n",
    "    print(f'✓ Restored checkpoint: {latest_checkpoint}')\n",
    "    print(f'  Checkpoint number: {ckpt_num}')\n",
    "    \n",
    "    # Try to infer which epoch this is (checkpoints saved every 50 epochs by default)\n",
    "    # This is an estimate based on the training code\n",
    "    estimated_epoch = int(ckpt_num) * 50\n",
    "    print(f'  Estimated epoch: {estimated_epoch}')\n",
    "else:\n",
    "    print('⚠ No checkpoint found, using fresh/untrained model')\n",
    "    print('  Training will start from epoch 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    \"\"\"\n",
    "    Updated inference function for DistilBERT\n",
    "    \"\"\"\n",
    "    # No hidden state needed for DistilBERT\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    total_images = 0\n",
    "    \n",
    "    # Progress bar for inference\n",
    "    pbar = tqdm(total=NUM_TEST, desc='Generating images', unit='img')\n",
    "    \n",
    "    # Unpack 3 values: input_ids, attention_mask, idx\n",
    "    for input_ids, attention_mask, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(input_ids, attention_mask, sample_seed)\n",
    "        step += 1\n",
    "        \n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave(f'{inference_dir}/inference_{idx[i]:04d}.jpg', fake_image[i].numpy()*0.5 + 0.5)\n",
    "            total_images += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f'\\n✓ Generated {total_images} images in {time.time()-start:.4f} sec')\n",
    "    print(f'✓ Images saved to: {inference_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(f'⚠ No images found in {inference_dir}')\n",
    "    print('Please run the inference cell first!')\n",
    "else:\n",
    "    # Randomly sample 20 images\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    num_samples = min(20, len(image_files))\n",
    "    sample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "    sample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "    # Create 4x5 grid\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, img_path in enumerate(sample_files):\n",
    "        # Extract image ID from filename\n",
    "        img_id = int(Path(img_path).stem.split('_')[1])\n",
    "        \n",
    "        # Find caption\n",
    "        caption_idx = np.where(test_ids == img_id)[0][0]\n",
    "        caption_ids = test_captions[caption_idx]\n",
    "        \n",
    "        # Decode caption\n",
    "        caption_text = ''\n",
    "        for word_id in caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':\n",
    "                caption_text += word + ' '\n",
    "        \n",
    "        # Load and display image\n",
    "        img = plt.imread(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    # Hide unused subplots if less than 20 images\n",
    "    for idx in range(num_samples, 20):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nTotal generated images: {len(image_files)}')\n",
    "    print(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
