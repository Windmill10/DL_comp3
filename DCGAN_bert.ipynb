{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Text to Image</center></h1>\n",
    "\n",
    "<h2 id=\"Platform:-Kaggle\">Platform: <a href=\"https://www.kaggle.com/competitions/2025-datalab-cup-3-reverse-image-caption/overview\">Kaggle</a><a class=\"anchor-link\" href=\"#Platform:-Kaggle\">¶</a></h2>\n",
    "<h2 id=\"Overview\">Overview<a class=\"anchor-link\" href=\"#Overview\">¶</a></h2>\n",
    "<p>In this work, we are interested in translating text in the form of single-sentence human-written descriptions directly into image pixels. For example, \"<strong>this flower has petals that are yellow and has a ruffled stamen</strong>\" and \"<strong>this pink and yellow flower has a beautiful yellow center with many stamens</strong>\". You have to develop a novel deep architecture and GAN formulation to effectively translate visual concepts from characters to pixels.</p>\n",
    "\n",
    "<p>More specifically, given a set of texts, your task is to generate reasonable images with size 64x64x3 to illustrate the corresponding texts. Here we use <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford-102 flower dataset</a> and its <a href=\"https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view\">paired texts</a> as our training dataset.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/example.png\"/>\n",
    "\n",
    "<ul>\n",
    "<li>7370 images as training set, where each images is annotated with at most 10 texts.</li>\n",
    "<li>819 texts for testing. You must generate 1 64x64x3 image for each text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN\">Conditional GAN<a class=\"anchor-link\" href=\"#Conditional-GAN\">¶</a></h2>\n",
    "<p>Given a text, in order to generate the image which can illustrate it, our model must meet several requirements:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Our model should have ability to understand and extract the meaning of given texts.<ul>\n",
    "<li>Use RNN or other language model, such as BERT, ELMo or XLNet, to capture the meaning of text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Our model should be able to generate image.<ul>\n",
    "<li>Use GAN to generate high quality image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>GAN-generated image should illustrate the text.<ul>\n",
    "<li>Use conditional-GAN to generate image conditioned on given text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information $y$. We can perform the conditioning by feeding $y$ into both the discriminator and generator as additional input layer.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/cGAN.png\" width=\"500\"/>\n",
    "\n",
    "<p>There are two motivations for using some extra information in a GAN model:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Improve GAN.</li>\n",
    "<li>Generate targeted image.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Additional information that is correlated with the input images, such as class labels, can be used to improve the GAN. This improvement may come in the form of more stable training, faster training, and/or generated images that have better quality.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/GANCLS.jpg\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttry:\n",
    "\t\t# Restrict TensorFlow to only use the first GPU\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\t\t# Currently, memory growth needs to be the same across GPUs\n",
    "\t\tfor gpu in gpus:\n",
    "\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\n",
    "\t\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\t\tprint(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\texcept RuntimeError as e:\n",
    "\t\t# Memory growth must be set before GPUs have been initialized\n",
    "\t\tprint(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using CLIP tokenizer (sent2IdList removed)\n"
     ]
    }
   ],
   "source": [
    "# This cell previously contained sent2IdList() function\n",
    "# It has been removed as we now use CLIP tokenizer instead\n",
    "# The id2word_dict is still available from cell 6 for visualization purposes\n",
    "\n",
    "print(\"✓ Using CLIP tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Load CLIP Tokenizer\n",
    "# \"openai/clip-vit-base-patch32\" is a standard, powerful model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def preprocess_text_clip(text, max_length=77):\n",
    "\t\tencoded = tokenizer(\n",
    "\t\t\t\ttext,\n",
    "\t\t\t\tpadding='max_length',\n",
    "\t\t\t\ttruncation=True,\n",
    "\t\t\t\tmax_length=max_length,\n",
    "\t\t\t\treturn_tensors='tf'\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t\t'input_ids': encoded['input_ids'],\n",
    "\t\t\t\t'attention_mask': encoded['attention_mask']\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DiffAugment functions loaded\n",
      "  Policies available: color, translation, cutout\n"
     ]
    }
   ],
   "source": [
    "def DiffAugment(x, policy='color,translation,cutout', channels_first=False, params=None):\n",
    "\t\t\"\"\"\n",
    "\t\tDifferentiable augmentation for GANs\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tx: Input images [batch, H, W, C] \n",
    "\t\t\t\tpolicy: Comma-separated augmentation policies\n",
    "\t\t\t\tchannels_first: If True, expects [batch, C, H, W]\n",
    "\t\t\t\tparams: Optional dict of pre-generated augmentation parameters for consistency\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tAugmented images\n",
    "\t\t\"\"\"\n",
    "\t\tif policy:\n",
    "\t\t\t\tif not channels_first:\n",
    "\t\t\t\t\t\t# TensorFlow format: [batch, H, W, C]\n",
    "\t\t\t\t\t\tfor p in policy.split(','):\n",
    "\t\t\t\t\t\t\t\tfor f in AUGMENT_FNS[p]:\n",
    "\t\t\t\t\t\t\t\t\t\tx = f(x, params)  # ← Pass params!\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def rand_brightness(x, params=None):\n",
    "\t\t\"\"\"Random brightness adjustment\"\"\"\n",
    "\t\tif params is not None and 'brightness' in params:\n",
    "\t\t\t\tmagnitude = params['brightness']\n",
    "\t\telse:\n",
    "\t\t\t\tmagnitude = tf.random.uniform([], -0.5, 0.5)\n",
    "\t\tx = x + magnitude\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def rand_saturation(x, params=None):\n",
    "\t\t\"\"\"Random saturation adjustment\"\"\"\n",
    "\t\tif params is not None and 'saturation' in params:\n",
    "\t\t\t\tmagnitude = params['saturation']\n",
    "\t\telse:\n",
    "\t\t\t\tmagnitude = tf.random.uniform([], 0.0, 2.0)\n",
    "\t\tx_mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "\t\tx = (x - x_mean) * magnitude + x_mean\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def rand_contrast(x, params=None):\n",
    "\t\t\"\"\"Random contrast adjustment\"\"\"\n",
    "\t\tif params is not None and 'contrast' in params:\n",
    "\t\t\t\tmagnitude = params['contrast']\n",
    "\t\telse:\n",
    "\t\t\t\tmagnitude = tf.random.uniform([], 0.5, 1.5)\n",
    "\t\tx_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "\t\tx = (x - x_mean) * magnitude + x_mean\n",
    "\t\treturn x\n",
    "\n",
    "def rand_translation(x, params=None, ratio=0.125):\n",
    "\t\t\"\"\"Random translation (shift) - Fully vectorized for @tf.function\"\"\"\n",
    "\t\tbatch_size = tf.shape(x)[0]\n",
    "\t\timage_size = tf.shape(x)[1]\n",
    "\t\tshift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "\t\t\n",
    "\t\t# Random translation amounts for entire batch\n",
    "\t\tif params is not None and 'translation_x' in params:\n",
    "\t\t\t\ttranslation_x = params['translation_x']\n",
    "\t\t\t\ttranslation_y = params['translation_y']\n",
    "\t\telse:\n",
    "\t\t\t\ttranslation_x = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "\t\t\t\ttranslation_y = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "\t\t\n",
    "\t\tdef translate_single_image(args):\n",
    "\t\t\t\t\"\"\"Translate a single image\"\"\"\n",
    "\t\t\t\timg, tx, ty = args\n",
    "\t\t\t\timg = tf.pad(img, [[shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "\t\t\t\timg = tf.image.crop_to_bounding_box(img, shift + ty, shift + tx, image_size, image_size)\n",
    "\t\t\t\treturn img\n",
    "\t\t\n",
    "\t\t# Use tf.map_fn (graph-mode compatible)\n",
    "\t\tx_translated = tf.map_fn(\n",
    "\t\t\t\ttranslate_single_image,\n",
    "\t\t\t\t(x, translation_x, translation_y),\n",
    "\t\t\t\tfn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "\t\t\t\tparallel_iterations=BATCH_SIZE\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\treturn x_translated\n",
    "\n",
    "\n",
    "def rand_cutout(x, params=None, ratio=0.2):\n",
    "\t\t\"\"\"\n",
    "\t\tRandom cutout - SIMPLIFIED vectorized version\n",
    "\t\t\n",
    "\t\tInstead of complex per-pixel masking, we create rectangular masks\n",
    "\t\tusing broadcasting and boolean operations\n",
    "\t\t\"\"\"\n",
    "\t\tbatch_size = tf.shape(x)[0]\n",
    "\t\timage_size = tf.shape(x)[1]\n",
    "\t\tchannels = tf.shape(x)[3]\n",
    "\t\t\n",
    "\t\t# Cutout size\n",
    "\t\tcutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "\t\t\n",
    "\t\t# Random offset for cutout location\n",
    "\t\tif params is not None and 'cutout_x' in params:\n",
    "\t\t\t\toffset_x = params['cutout_x']\n",
    "\t\t\t\toffset_y = params['cutout_y']\n",
    "\t\telse:\n",
    "\t\t\t\toffset_x = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "\t\t\t\toffset_y = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "\t\t\n",
    "\t\tdef cutout_single_image(args):\n",
    "\t\t\t\t\"\"\"Apply cutout to single image using simple slicing\"\"\"\n",
    "\t\t\t\timg, ox, oy = args\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Create coordinate grids\n",
    "\t\t\t\theight_range = tf.range(image_size)\n",
    "\t\t\t\twidth_range = tf.range(image_size)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Create 2D grids\n",
    "\t\t\t\tyy, xx = tf.meshgrid(height_range, width_range, indexing='ij')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Create mask: True where we want to KEEP pixels\n",
    "\t\t\t\tmask_y = tf.logical_or(yy < oy, yy >= oy + cutout_size)\n",
    "\t\t\t\tmask_x = tf.logical_or(xx < ox, xx >= ox + cutout_size)\n",
    "\t\t\t\tmask = tf.logical_or(mask_y, mask_x)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Expand mask to all channels\n",
    "\t\t\t\tmask = tf.expand_dims(mask, axis=-1)  # [H, W, 1]\n",
    "\t\t\t\tmask = tf.tile(mask, [1, 1, channels])  # [H, W, C]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Apply mask (convert bool to float)\n",
    "\t\t\t\tmask = tf.cast(mask, tf.float32)\n",
    "\t\t\t\treturn img * mask\n",
    "\t\t\n",
    "\t\t# Use tf.map_fn\n",
    "\t\tx_cutout = tf.map_fn(\n",
    "\t\t\t\tcutout_single_image,\n",
    "\t\t\t\t(x, offset_x, offset_y),\n",
    "\t\t\t\tfn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "\t\t\t\tparallel_iterations=BATCH_SIZE\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\treturn x_cutout\n",
    "\n",
    "\n",
    "# Augmentation function registry\n",
    "AUGMENT_FNS = {\n",
    "\t\t'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "\t\t'translation': [rand_translation],\n",
    "\t\t'cutout': [rand_cutout],\n",
    "}\n",
    "\n",
    "\n",
    "print(\"✓ DiffAugment functions loaded\")\n",
    "print(\"  Policies available: color, translation, cutout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string (not IDs!)\n",
    "\t\t\t\timage_path: Path to image file\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\timg, input_ids, attention_mask\n",
    "\t\t\"\"\"\n",
    "\t\t# ============= IMAGE PROCESSING (same as before) =============\n",
    "\t\timg = tf.io.read_file(image_path)\n",
    "\t\timg = tf.image.decode_image(img, channels=3)\n",
    "\t\timg = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "\t\timg.set_shape([None, None, 3])\n",
    "\t\timg = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "\t\t\n",
    "\t\t# Normalize to [-1, 1] to match generator's tanh output\n",
    "\t\timg = (img * 2.0) - 1.0\n",
    "\t\timg.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "\t\t\n",
    "\t\t# ============= TEXT PROCESSING (NEW: Use CLIP tokenizer) =============\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'  # Use numpy arrays for TF compatibility\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly for CLIP\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated dataset generator to work with raw text (decoded from IDs)\n",
    "\t\t\"\"\"\n",
    "\t\t# Load the training data\n",
    "\t\tdf = pd.read_pickle(filenames)\n",
    "\t\tcaptions_ids = df['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to raw text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\t# Randomly choose one caption (list of ID lists)\n",
    "\t\t\t\tchosen_caption_ids = random.choice(captions_ids[i])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\timage_paths = df['ImagePath'].values\n",
    "\t\t\n",
    "\t\t# Verify same length\n",
    "\t\tassert len(caption_texts) == len(image_paths)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text and image paths\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, image_paths))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.cache()\n",
    "\t\tdataset = dataset.shuffle(len(caption_texts)).batch(batch_size, drop_remainder=True)\n",
    "\t\tdataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¶</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import TFCLIPTextModel, TFCLIPModel\n",
    "\n",
    "class ClipTextEncoder(tf.keras.Model):\n",
    "\t\tdef __init__(self, output_dim=512, freeze_clip=True):\n",
    "\t\t\t\tsuper(ClipTextEncoder, self).__init__()\n",
    "\t\t\t\t# Load Pre-trained CLIP Text Model\n",
    "\t\t\t\tself.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\t\t\t\t\n",
    "\t\t\t\tif freeze_clip:\n",
    "\t\t\t\t\t\tself.clip.trainable = False\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t# REMOVED: Projection, LayerNorm, Dropout to ensure RAW embeddings\n",
    "\t\n",
    "\t\tdef call(self, input_ids, attention_mask, training=False):\n",
    "\t\t\t# 1. Get the projected features (Aligned with images, e.g., 512-dim)\n",
    "\t\t\ttext_embeds = self.clip.get_text_features(\n",
    "\t\t\t\tinput_ids=input_ids, \n",
    "\t\t\t\tattention_mask=attention_mask\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\t# 2. CRITICAL FIX: Manually normalize to get the actual CLIP embedding\n",
    "\t\t\t# CLIP uses cosine similarity, so vectors must be unit length.\n",
    "\t\t\ttext_embeds = tf.math.l2_normalize(text_embeds, axis=1)\n",
    "\t\t\t\n",
    "\t\t\treturn text_embeds\n",
    "\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Generator\">Generator<a class=\"anchor-link\" href=\"#Generator\">¶</a></h2>\n",
    "<p>A image generator which generates the target image illustrating the input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: hidden representation of input text and random noise z with random seed.</li>\n",
    "<li>Output: target image, which is conditioned on the given text, in size 64x64x3.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization as per DCGAN paper\n",
    "def dcgan_weight_init():\n",
    "\t\treturn tf.keras.initializers.HeNormal()\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # 1. Initialize Weights (He Normal is better for WGAN-GP)\n",
    "        init = tf.keras.initializers.HeNormal()\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # [NEW] Text Conditioning Projection (\"The Translator\")\n",
    "        # Maps the 512-dim unit-vector from CLIP to a learned 128-dim space\n",
    "        # that allows the Generator to \"understand\" the instruction.\n",
    "        # ---------------------------------------------------------\n",
    "        self.text_projection = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, kernel_initializer=init),\n",
    "            tf.keras.layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        \n",
    "        # 2. Main Input Projection\n",
    "        # Input dim = Noise (100) + Projected Text (128) = 228\n",
    "        self.dense = tf.keras.layers.Dense(4 * 4 * 512, use_bias=False, kernel_initializer=init)\n",
    "        self.bn0 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 3. Upsample Blocks\n",
    "        self.up1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv1 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.up2 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.up3 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 4. Final Layer\n",
    "        self.up4 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        # Note: use_bias=True here is critical (as discussed before)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer=init)\n",
    "        \n",
    "    def call(self, text_embedding, noise_z, training=True):\n",
    "        # ---------------------------------------------------------\n",
    "        # Step 1: Process the Text\n",
    "        # text_embedding shape: [Batch, 512] (Normalized)\n",
    "        # text_feat shape:      [Batch, 128] (Unbounded, Learnable)\n",
    "        # ---------------------------------------------------------\n",
    "        text_feat = self.text_projection(text_embedding)\n",
    "        \n",
    "        # Step 2: Concatenate with Noise\n",
    "        # noise_z shape: [Batch, 100]\n",
    "        # x shape:       [Batch, 228]\n",
    "        x = tf.concat([noise_z, text_feat], axis=1)\n",
    "        \n",
    "        # Step 3: Project to 4x4x512\n",
    "        x = self.dense(x)\n",
    "        x = self.bn0(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.reshape(x, [-1, 4, 4, 512])\n",
    "        \n",
    "        # Step 4: Upsampling\n",
    "        x = self.up1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Step 5: Final Generation\n",
    "        x = self.up4(x)\n",
    "        output = self.conv4(x)\n",
    "        output = tf.nn.tanh(output)\n",
    "        \n",
    "        return x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Discriminator\">Discriminator<a class=\"anchor-link\" href=\"#Discriminator\">¶</a></h2>\n",
    "<p>A binary classifier which can discriminate the real and fake image:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Real image<ul>\n",
    "<li>Input: real image and the paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 1.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Fake Image<ul>\n",
    "<li>Input: generated image and paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 0.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "\t\t\"\"\"\n",
    "\t\tProjection Discriminator with Stability Fixes\n",
    "\t\t\"\"\"\n",
    "\t\tdef __init__(self, hparas):\n",
    "\t\t\t\tsuper(Critic, self).__init__()\n",
    "\t\t\t\tself.hparas = hparas\n",
    "\t\t\t\tinit = tf.keras.initializers.HeNormal()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# --- IMAGE PATH ---\n",
    "\t\t\t\t# 64 -> 32\n",
    "\t\t\t\tself.conv1 = tf.keras.layers.Conv2D(64, 4, 2, padding='same', kernel_initializer=init)\n",
    "\t\t\t\tself.ln1 = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 32 -> 16\n",
    "\t\t\t\tself.conv2 = tf.keras.layers.Conv2D(128, 4, 2, padding='same', kernel_initializer=init)\n",
    "\t\t\t\tself.ln2 = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 16 -> 8\n",
    "\t\t\t\tself.conv3 = tf.keras.layers.Conv2D(256, 4, 2, padding='same', kernel_initializer=init)\n",
    "\t\t\t\tself.ln3 = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 8 -> 4\n",
    "\t\t\t\tself.conv4 = tf.keras.layers.Conv2D(512, 4, 2, padding='same', kernel_initializer=init)\n",
    "\t\t\t\tself.ln4 = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# --- TEXT PATH ---\n",
    "\t\t\t\t# Project text to match the depth of image features (512 channels)\n",
    "\t\t\t\tself.text_dense = tf.keras.layers.Dense(512, kernel_initializer=tf.keras.initializers.Orthogonal())\n",
    "\t\t\t\tself.text_dense_ln = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# --- SCORING ---\n",
    "\t\t\t\t# \"Realism\" score (f(x))\n",
    "\t\t\t\tself.flatten = tf.keras.layers.Flatten()\n",
    "\t\t\t\tself.disc_realism = tf.keras.layers.Dense(1, kernel_initializer=init)\n",
    "\n",
    "\t\tdef call(self, img, text, training=True):\n",
    "\t\t\t\t# 1. Extract Image Features\n",
    "\t\t\t\tx = tf.nn.leaky_relu(self.ln1(self.conv1(img), training=training), alpha=0.2)\n",
    "\t\t\t\tx = tf.nn.leaky_relu(self.ln2(self.conv2(x), training=training), alpha=0.2)\n",
    "\t\t\t\tx = tf.nn.leaky_relu(self.ln3(self.conv3(x), training=training), alpha=0.2)\n",
    "\t\t\t\tx = tf.nn.leaky_relu(self.ln4(self.conv4(x), training=training), alpha=0.2)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# x shape: [Batch, 4, 4, 512]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 2. Process Text Features (psi(y))\n",
    "\t\t\t\tpsi_y = self.text_dense(text) \n",
    "\t\t\t\tpsi_y = self.text_dense_ln(psi_y, training=training)\n",
    "\t\t\t\tpsi_y = tf.nn.leaky_relu(psi_y, alpha=0.2)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 3. PROJECTION SCORE (Alignment)\n",
    "\t\t\t\t# Reshape text to [Batch, 1, 1, 512]\n",
    "\t\t\t\t# psi_y_reshaped = tf.reshape(psi_y, [-1, 1, 1, 512])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Global Sum Pooling of image features for projection dot product\n",
    "\t\t\t\t# Note: Original used sum([1,2]), optimized uses reduce_sum on phi_x * psi_y.\n",
    "\t\t\t\t# Both are mathematically similar, but let's stick to the stable explicit dot product:\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Get global image vector\n",
    "\t\t\t\timg_vec = tf.reduce_sum(x, axis=[1, 2]) # [Batch, 512]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Dot product\n",
    "\t\t\t\talignment_score = tf.reduce_sum(img_vec * psi_y, axis=1, keepdims=True)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 4. REALISM SCORE\n",
    "\t\t\t\tflat_img = self.flatten(x)\n",
    "\t\t\t\trealism_score = self.disc_realism(flat_img)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 5. TOTAL SCORE\n",
    "\t\t\t\treturn realism_score + alignment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters updated:\n",
      "  Batch size: 128\n",
      "  Learning rate: 0.0002\n",
      "  N_Critic: 5\n",
      "  Lambda_GP: 10.0\n",
      "  DiffAugment: True (translation)\n"
     ]
    }
   ],
   "source": [
    "hparas = {\n",
    "\t\t'MAX_SEQ_LENGTH': 77,          # CLIP Standard\n",
    "\t\t'EMBED_DIM': 512,              # CLIP Output\n",
    "\t\t'VOCAB_SIZE': 49408,           # CLIP Vocab Size (approx)\n",
    "\t\t'RNN_HIDDEN_SIZE': 512,        # Target projection size (Updated to 512 for Raw CLIP)\n",
    "\t\t'Z_DIM': 512,\n",
    "\t\t'DENSE_DIM': 128,\n",
    "\t\t'IMAGE_SIZE': [64, 64, 3],\n",
    "\t\t\n",
    "\t\t'BATCH_SIZE': BATCH_SIZE,      # Fix: Use Global Variable\n",
    "\t\t'LR': 2e-4,\n",
    "\t\t'BETA_1': 0.0,\n",
    "\t\t'BETA_2': 0.9,\n",
    "\t\t'N_CRITIC': 5,\n",
    "\t\t'LAMBDA_GP': 10.0,\n",
    "\t\t'LAMBDA_MISMATCH': 1.0,\n",
    "\t\t\n",
    "\t\t'LR_DECAY_START': 50,\n",
    "\t\t'LR_DECAY_EVERY': 10,\n",
    "\t\t'LR_DECAY_FACTOR': 0.95,\n",
    "\t\t'LR_MIN': 1e-5,\n",
    "\t\t\n",
    "\t\t'USE_DIFFAUG': True,\n",
    "\t\t'DIFFAUG_POLICY': 'translation', # Conservative augmentation\n",
    "\t\t\n",
    "\t\t'N_EPOCH': 1000,\n",
    "\t\t'N_SAMPLE': num_training_sample,\n",
    "\t\t'PRINT_FREQ': 5\n",
    "}\n",
    "\n",
    "print(f\"✓ Hyperparameters updated:\")\n",
    "print(f\"  Batch size: {hparas['BATCH_SIZE']}\")\n",
    "print(f\"  Learning rate: {hparas['LR']}\")\n",
    "print(f\"  N_Critic: {hparas['N_CRITIC']}\")\n",
    "print(f\"  Lambda_GP: {hparas['LAMBDA_GP']}\")\n",
    "print(f\"  DiffAugment: {hparas['USE_DIFFAUG']} ({hparas['DIFFAUG_POLICY']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCLIPModel.\n",
      "\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n",
      "All the layers of TFCLIPModel were initialized from the model checkpoint at openai/clip-vit-base-patch32.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "text_encoder = ClipTextEncoder(output_dim=hparas['RNN_HIDDEN_SIZE'], freeze_clip=True)\n",
    "generator = Generator(hparas)\n",
    "critic = Critic(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Loss-Function-and-Optimization\">Loss Function and Optimization<a class=\"anchor-link\" href=\"#Loss-Function-and-Optimization\">¶</a></h2>\n",
    "<p>Although the conditional GAN model is quite complex, the loss function used to optimize the network is relatively simple. Actually, it is simply a binary classification task, thus we use cross entropy as our loss.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss_critic(real_scores, fake_scores):\n",
    "\t\t\"\"\"\n",
    "\t\tWasserstein loss for critic\n",
    "\t\tCritic wants to maximize: E[critic(real)] - E[critic(fake)]\n",
    "\t\tSo we minimize: E[critic(fake)] - E[critic(real)]\n",
    "\t\t\"\"\"\n",
    "\t\treturn tf.reduce_mean(fake_scores) - tf.reduce_mean(real_scores)\n",
    "\n",
    "def mismatch_loss_critic(real_scores, wrong_scores, margin=1.0):\n",
    "\t\t\"\"\"\n",
    "\t\tHinge loss for GAN-CLS (mismatched pairs).\n",
    "\t\tWants: real_scores > wrong_scores + margin\n",
    "\t\tMinimizes: max(0, margin + wrong_scores - real_scores)\n",
    "\t\t\"\"\"\n",
    "\t\tloss = tf.nn.relu(margin + wrong_scores - real_scores)\n",
    "\t\treturn tf.reduce_mean(loss)\n",
    "\n",
    "def wasserstein_loss_generator(fake_scores):\n",
    "\t\t\"\"\"\n",
    "\t\tWasserstein loss for generator\n",
    "\t\tGenerator wants to maximize: E[critic(fake)]\n",
    "\t\tSo we minimize: -E[critic(fake)]\n",
    "\t\t\"\"\"\n",
    "\t\treturn -tf.reduce_mean(fake_scores)\n",
    "\n",
    "def gradient_penalty(critic, real_images, fake_images, text_embed, batch_size, \n",
    "                     diffaug_policy=None, aug_params=None):\n",
    "    \"\"\"\n",
    "    Computes Gradient Penalty with correct DiffAugment application.\n",
    "    \n",
    "    CRITICAL: \n",
    "    1. Input must be RAW images (real_images, fake_images).\n",
    "    2. Interpolation happens on RAW images.\n",
    "    3. Augmentation is applied INSIDE the tape on the interpolated result.\n",
    "    \"\"\"\n",
    "    # 1. Interpolate RAW images\n",
    "    # This creates a valid image on the manifold between Real and Fake\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    interpolated = alpha * real_images + (1.0 - alpha) * fake_images\n",
    "    \n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        # We must watch the 'interpolated' tensor (the RAW one)\n",
    "        gp_tape.watch(interpolated)\n",
    "        \n",
    "        # 2. Apply DiffAugment to the INTERPOLATED image\n",
    "        # This allows gradients to backpropagate through the augmentation logic\n",
    "        if diffaug_policy is not None and aug_params is not None:\n",
    "            interpolated_aug = DiffAugment(interpolated, policy=diffaug_policy, params=aug_params)\n",
    "        else:\n",
    "            interpolated_aug = interpolated\n",
    "            \n",
    "        # 3. Critic Pass (on Augmented Interpolation)\n",
    "        interpolated_scores = critic(interpolated_aug, text_embed, training=True)\n",
    "    \n",
    "    # 4. Compute Gradients w.r.t INTERPOLATED (Pre-Augmentation)\n",
    "    # The chain rule will naturally include the gradient of the augmentation function\n",
    "    gradients = gp_tape.gradient(interpolated_scores, [interpolated])[0]\n",
    "    \n",
    "    # 5. Compute Norm (with Epsilon for stability)\n",
    "    gradients_sqr_sum = tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3])\n",
    "    gradients_norm = tf.sqrt(gradients_sqr_sum + 1e-12)\n",
    "    \n",
    "    # 6. Penalty\n",
    "    return tf.reduce_mean(tf.square(gradients_norm - 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# WGAN-GP: Use Adam with beta_1=0.0, beta_2=0.9\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "\t\tlearning_rate=hparas['LR'],\n",
    "\t\tbeta_1=hparas['BETA_1'],\n",
    "\t\tbeta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(\n",
    "\t\tlearning_rate=hparas['LR'],\n",
    "\t\tbeta_1=hparas['BETA_1'],\n",
    "\t\tbeta_2=hparas['BETA_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(\n",
    "\t\tgenerator_optimizer=generator_optimizer,\n",
    "\t\tcritic_optimizer=critic_optimizer,\n",
    "\t\ttext_encoder=text_encoder,\n",
    "\t\tgenerator=generator,\n",
    "\t\tcritic=critic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wasserstein_distance(real_scores, fake_scores):\n",
    "\t\t\"\"\"\n",
    "\t\tApproximation of Wasserstein distance\n",
    "\t\tHigher is better (critic getting better at distinguishing)\n",
    "\t\t\"\"\"\n",
    "\t\treturn tf.reduce_mean(real_scores) - tf.reduce_mean(fake_scores)\n",
    "\n",
    "def calculate_gradient_norm(gradients):\n",
    "\t\t\"\"\"Calculate L2 norm of gradients\"\"\"\n",
    "\t\tsquared_norms = [tf.reduce_sum(tf.square(g)) for g in gradients if g is not None]\n",
    "\t\ttotal_norm = tf.sqrt(tf.reduce_sum(squared_norms))\n",
    "\t\treturn total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, input_ids, attention_mask):\n",
    "    batch_size = tf.shape(real_image)[0]\n",
    "    \n",
    "    # --- Helper to generate consistent Augment Params ---\n",
    "    def get_aug_params(bs):\n",
    "        if not hparas['USE_DIFFAUG']: return None\n",
    "        image_size = 64\n",
    "        shift = tf.cast(image_size * 0.125 + 0.5, tf.int32)\n",
    "        cutout_size = tf.cast(image_size * 0.5 + 0.5, tf.int32)\n",
    "        \n",
    "        return {\n",
    "            'brightness': tf.random.uniform([], -0.5, 0.5),\n",
    "            'saturation': tf.random.uniform([], 0.0, 2.0),\n",
    "            'contrast': tf.random.uniform([], 0.5, 1.5),\n",
    "            'translation_x': tf.random.uniform([bs], -shift, shift + 1, dtype=tf.int32),\n",
    "            'translation_y': tf.random.uniform([bs], -shift, shift + 1, dtype=tf.int32),\n",
    "            'cutout_x': tf.random.uniform([bs], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "            'cutout_y': tf.random.uniform([bs], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "        }\n",
    "\n",
    "    # ============================================================\n",
    "    # 1. Train Critic\n",
    "    # ============================================================\n",
    "    # Initialize variables to ensure they exist after the loop\n",
    "    c_loss = 0.0\n",
    "    c_loss_w = 0.0\n",
    "    c_loss_mismatch = 0.0\n",
    "    gp = 0.0\n",
    "    \n",
    "    for _ in range(hparas['N_CRITIC']):\n",
    "        noise = tf.random.normal([batch_size, hparas['Z_DIM']])\n",
    "        aug_params = get_aug_params(batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # --- Text Embeddings ---\n",
    "            text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "            text_embed = tf.math.l2_normalize(text_embed, axis=1)\n",
    "            text_embed = tf.stop_gradient(text_embed)\n",
    "            \n",
    "            wrong_text_embed = tf.roll(text_embed, shift=1, axis=0)\n",
    "            \n",
    "            # --- Generate Fake (RAW) ---\n",
    "            _, fake_image = generator(text_embed, noise, training=True)\n",
    "            \n",
    "            # --- Augment ---\n",
    "            if hparas['USE_DIFFAUG']:\n",
    "                real_image_aug = DiffAugment(real_image, policy=hparas['DIFFAUG_POLICY'], params=aug_params)\n",
    "                fake_image_aug = DiffAugment(fake_image, policy=hparas['DIFFAUG_POLICY'], params=aug_params)\n",
    "            else:\n",
    "                real_image_aug = real_image\n",
    "                fake_image_aug = fake_image\n",
    "            \n",
    "            # --- Scores ---\n",
    "            real_scores = critic(real_image_aug, text_embed, training=True)\n",
    "            fake_scores = critic(fake_image_aug, text_embed, training=True)\n",
    "            wrong_scores = critic(real_image_aug, wrong_text_embed, training=True)\n",
    "            \n",
    "            # --- Losses ---\n",
    "            c_loss_w = wasserstein_loss_critic(real_scores, fake_scores)\n",
    "            c_loss_mismatch = mismatch_loss_critic(real_scores, wrong_scores)\n",
    "            \n",
    "            # --- Gradient Penalty (Pass RAW images) ---\n",
    "            gp = gradient_penalty(\n",
    "                critic, \n",
    "                real_image, \n",
    "                fake_image, \n",
    "                text_embed, \n",
    "                batch_size,\n",
    "                diffaug_policy=hparas['DIFFAUG_POLICY'] if hparas['USE_DIFFAUG'] else None,\n",
    "                aug_params=aug_params\n",
    "            )\n",
    "            \n",
    "            c_loss = c_loss_w + hparas['LAMBDA_GP'] * gp + hparas['LAMBDA_MISMATCH'] * c_loss_mismatch\n",
    "        \n",
    "        # Apply Gradients\n",
    "        grad_c = critic_tape.gradient(c_loss, critic.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(grad_c, critic.trainable_variables))\n",
    "    \n",
    "    # ============================================================\n",
    "    # 2. Train Generator\n",
    "    # ============================================================\n",
    "    noise = tf.random.normal([batch_size, hparas['Z_DIM']])\n",
    "    gen_aug_params = get_aug_params(batch_size)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "        text_embed = tf.math.l2_normalize(text_embed, axis=1)\n",
    "        text_embed = tf.stop_gradient(text_embed)\n",
    "        \n",
    "        _, fake_image = generator(text_embed, noise, training=True)\n",
    "        \n",
    "        if hparas['USE_DIFFAUG']:\n",
    "            fake_image_aug = DiffAugment(fake_image, policy=hparas['DIFFAUG_POLICY'], params=gen_aug_params)\n",
    "        else:\n",
    "            fake_image_aug = fake_image\n",
    "            \n",
    "        fake_scores = critic(fake_image_aug, text_embed, training=True)\n",
    "        g_loss = wasserstein_loss_generator(fake_scores)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    \n",
    "    # ============================================================\n",
    "    # 3. Calculate Metrics\n",
    "    # ============================================================\n",
    "    wasserstein_dist = calculate_wasserstein_distance(real_scores, fake_scores)\n",
    "    \n",
    "    # Calculate Gradient Norms for monitoring stability\n",
    "    # Filter out None gradients (frozen layers)\n",
    "    valid_grads_c = [g for g in grad_c if g is not None]\n",
    "    valid_grads_g = [g for g in grad_g if g is not None]\n",
    "    \n",
    "    grad_norm_c = tf.linalg.global_norm(valid_grads_c)\n",
    "    grad_norm_g = tf.linalg.global_norm(valid_grads_g)\n",
    "    \n",
    "    # Return EVERYTHING needed for the train loop logging\n",
    "    return {\n",
    "        'g_loss': g_loss,\n",
    "        'c_loss': c_loss,\n",
    "        'c_loss_wasserstein': c_loss_w,\n",
    "        'c_loss_mismatch': c_loss_mismatch,\n",
    "        'gp': gp,\n",
    "        'wasserstein_dist': wasserstein_dist,\n",
    "        'grad_norm_g': grad_norm_g,\n",
    "        'grad_norm_c': grad_norm_c,\n",
    "        'lr_g': generator_optimizer.learning_rate,\n",
    "        'lr_c': critic_optimizer.learning_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(input_ids, attention_mask, noise):\n",
    "\t\t# Encode text with DistilBERT (no hidden state)\n",
    "\t\ttext_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "\t\t_, fake_image = generator(text_embed, noise, training=False)\n",
    "\t\treturn fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "\t\th, w = images.shape[1], images.shape[2]\n",
    "\t\timg = np.zeros((h * size[0], w * size[1], 3))\n",
    "\t\tfor idx, image in enumerate(images):\n",
    "\t\t\t\ti = idx % size[1]\n",
    "\t\t\t\tj = idx // size[1]\n",
    "\t\t\t\timg[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\t\treturn img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "\t\t# getting the pixel values between [0, 1] to save it\n",
    "\t\treturn plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "\t\treturn imsave(images, size, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample data created:\n",
      "  Batch size: 128\n",
      "  Grid size (ni): 12 × 12 = 144\n",
      "  Sample sentences: 128 sentences\n",
      "  sample_seed shape: (128, 512)\n",
      "  sample_input_ids shape: (128, 77)\n",
      "  sample_attention_mask shape: (128, 77)\n",
      "✓ All dimensions match!\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for visualization during training\n",
    "# IMPORTANT: All three variables must have the same batch size!\n",
    "\n",
    "sample_size = BATCH_SIZE  # Current: 32\n",
    "ni = int(np.ceil(np.sqrt(sample_size)))  # Grid size for visualization\n",
    "\n",
    "# Create random noise seed\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "# Define 8 diverse sample sentences\n",
    "base_sentences = [\n",
    "\t\t\"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "\t\t\"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "\t\t\"the petals on this flower are white with a yellow center\",\n",
    "\t\t\"this flower has a lot of small round pink petals.\",\n",
    "\t\t\"this flower is orange in color, and has petals that are ruffled and rounded.\",\n",
    "\t\t\"the flower has yellow petals and the center of it is brown.\",\n",
    "\t\t\"this flower has petals that are blue and white.\",\n",
    "\t\t\"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "]\n",
    "\n",
    "# Repeat sentences to match sample_size (batch size)\n",
    "sample_sentences = []\n",
    "for i in range(sample_size):\n",
    "\t\tsample_sentences.append(base_sentences[i % len(base_sentences)])\n",
    "\n",
    "# Tokenize with CLIP\n",
    "sample_encoded = preprocess_text_clip(sample_sentences, max_length=77)\n",
    "sample_input_ids = sample_encoded['input_ids']\n",
    "sample_attention_mask = sample_encoded['attention_mask']\n",
    "\n",
    "# Verify all dimensions match!\n",
    "print(f\"✓ Sample data created:\")\n",
    "print(f\"  Batch size: {sample_size}\")\n",
    "print(f\"  Grid size (ni): {ni} × {ni} = {ni*ni}\")\n",
    "print(f\"  Sample sentences: {len(sample_sentences)} sentences\")\n",
    "print(f\"  sample_seed shape: {sample_seed.shape}\")\n",
    "print(f\"  sample_input_ids shape: {sample_input_ids.shape}\")\n",
    "print(f\"  sample_attention_mask shape: {sample_attention_mask.shape}\")\n",
    "\n",
    "# Check for dimension mismatches\n",
    "assert len(sample_sentences) == sample_size, f\"Mismatch: {len(sample_sentences)} != {sample_size}\"\n",
    "assert sample_seed.shape[0] == sample_size, f\"Mismatch: {sample_seed.shape[0]} != {sample_size}\"\n",
    "assert sample_input_ids.shape[0] == sample_size, f\"Mismatch: {sample_input_ids.shape[0]} != {sample_size}\"\n",
    "assert sample_attention_mask.shape[0] == sample_size, f\"Mismatch: {sample_attention_mask.shape[0]} != {sample_size}\"\n",
    "print(\"✓ All dimensions match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing training runs...\n",
      "================================================================================\n",
      "Available Training Runs:\n",
      "================================================================================\n",
      "20251118-133643  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251119-010342  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251119-145917  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-155708  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-160146  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-160215  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-160332  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-160506  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-161954  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-162611  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-162922  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-163047  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-163128  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-163152  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-163337  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-164139  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-164339  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-165758  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-171205  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-224708  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-230349  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-231036  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251120-231318  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251122-015004  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251123-235207  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251124-012742  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251124-012954  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251124-015147  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "20251124-015649  |  Checkpoint: ✗  |  Config: ✓  |  Samples: 0\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['20251118-133643',\n",
       " '20251119-010342',\n",
       " '20251119-145917',\n",
       " '20251120-155708',\n",
       " '20251120-160146',\n",
       " '20251120-160215',\n",
       " '20251120-160332',\n",
       " '20251120-160506',\n",
       " '20251120-161954',\n",
       " '20251120-162611',\n",
       " '20251120-162922',\n",
       " '20251120-163047',\n",
       " '20251120-163128',\n",
       " '20251120-163152',\n",
       " '20251120-163337',\n",
       " '20251120-164139',\n",
       " '20251120-164339',\n",
       " '20251120-165758',\n",
       " '20251120-171205',\n",
       " '20251120-224708',\n",
       " '20251120-230349',\n",
       " '20251120-231036',\n",
       " '20251120-231318',\n",
       " '20251122-015004',\n",
       " '20251123-235207',\n",
       " '20251124-012742',\n",
       " '20251124-012954',\n",
       " '20251124-015147',\n",
       " '20251124-015649']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper functions for managing training runs\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def list_available_runs():\n",
    "\t\t\"\"\"List all available training runs with their details\"\"\"\n",
    "\t\trun_dirs = sorted(glob.glob('runs/*/'))\n",
    "\t\t\n",
    "\t\tif not run_dirs:\n",
    "\t\t\t\tprint('No training runs found in runs/ directory')\n",
    "\t\t\t\treturn []\n",
    "\t\t\n",
    "\t\tprint('=' * 80)\n",
    "\t\tprint('Available Training Runs:')\n",
    "\t\tprint('=' * 80)\n",
    "\t\t\n",
    "\t\tavailable_runs = []\n",
    "\t\tfor run_dir in run_dirs:\n",
    "\t\t\t\ttimestamp = run_dir.split('/')[-2]\n",
    "\t\t\t\tavailable_runs.append(timestamp)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Check for checkpoints\n",
    "\t\t\t\tcheckpoint_dir = f'{run_dir}checkpoints'\n",
    "\t\t\t\tlatest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\t\t\t\thas_checkpoint = '✓' if latest_ckpt else '✗'\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Check for config\n",
    "\t\t\t\tconfig_path = f'{run_dir}config.json'\n",
    "\t\t\t\thas_config = '✓' if os.path.exists(config_path) else '✗'\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Count sample images\n",
    "\t\t\t\tsample_count = len(glob.glob(f'{run_dir}samples/*.jpg'))\n",
    "\t\t\t\t\n",
    "\t\t\t\tprint(f'{timestamp}  |  Checkpoint: {has_checkpoint}  |  Config: {has_config}  |  Samples: {sample_count}')\n",
    "\t\t\t\t\n",
    "\t\t\t\tif latest_ckpt:\n",
    "\t\t\t\t\t\tprint(f'  └─ Latest checkpoint: {latest_ckpt}')\n",
    "\t\t\n",
    "\t\tprint('=' * 80)\n",
    "\t\treturn available_runs\n",
    "\n",
    "def load_run_config(run_timestamp):\n",
    "\t\t\"\"\"Load configuration from a previous run\"\"\"\n",
    "\t\tconfig_path = f'runs/{run_timestamp}/config.json'\n",
    "\t\t\n",
    "\t\tif not os.path.exists(config_path):\n",
    "\t\t\t\traise FileNotFoundError(f'Config not found: {config_path}')\n",
    "\t\t\n",
    "\t\twith open(config_path, 'r') as f:\n",
    "\t\t\t\tconfig = json.load(f)\n",
    "\t\t\n",
    "\t\tprint(f'✓ Loaded config from: {config_path}')\n",
    "\t\treturn config\n",
    "\n",
    "# List available runs\n",
    "print('Checking for existing training runs...')\n",
    "list_available_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created NEW run directory: runs/20251124-021320\n",
      "✓ Saved configuration to: runs/20251124-021320/config.json\n",
      "\n",
      "Run directory structure:\n",
      "  runs/20251124-021320/\n",
      "  ├── checkpoints/  : runs/20251124-021320/checkpoints\n",
      "  ├── best_models/  : runs/20251124-021320/best_models\n",
      "  ├── samples/      : runs/20251124-021320/samples\n",
      "  └── inference/    : runs/20251124-021320/inference\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# RESUME TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "# Set to None for new run, or specify run timestamp to resume\n",
    "# Example: RESUME_RUN = '20251116-225453'\n",
    "RESUME_RUN = None # ← Change this to resume from specific run\n",
    "\n",
    "# ============================================================\n",
    "# RUN DIRECTORY SETUP\n",
    "# ============================================================\n",
    "if RESUME_RUN:\n",
    "\t\t# Resume from existing run\n",
    "\t\trun_dir = f'runs/{RESUME_RUN}'\n",
    "\t\t\n",
    "\t\t# Verify directory exists\n",
    "\t\tif not os.path.exists(run_dir):\n",
    "\t\t\t\traise FileNotFoundError(f'Run directory not found: {run_dir}')\n",
    "\t\t\n",
    "\t\t# Load existing config\n",
    "\t\ttry:\n",
    "\t\t\t\tprev_config = load_run_config(RESUME_RUN)\n",
    "\t\t\t\trun_timestamp = prev_config.get('run_timestamp', RESUME_RUN)\n",
    "\t\t\t\tprint(f'\\n⟳ RESUMING training from: {run_dir}')\n",
    "\t\t\t\tprint(f'  Original start: {run_timestamp}')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Warn if hyperparameters might be different\n",
    "\t\t\t\tif 'hyperparameters' in prev_config:\n",
    "\t\t\t\t\t\tprev_hparas = prev_config['hyperparameters']\n",
    "\t\t\t\t\t\tif prev_hparas.get('BATCH_SIZE') != BATCH_SIZE:\n",
    "\t\t\t\t\t\t\t\tprint(f'  ⚠ WARNING: Batch size changed ({prev_hparas.get(\"BATCH_SIZE\")} → {hparas[\"BATCH_SIZE\"]})')\n",
    "\t\t\t\t\t\tif prev_hparas.get('LR') != hparas['LR']:\n",
    "\t\t\t\t\t\t\t\tprint(f'  ⚠ WARNING: Learning rate changed ({prev_hparas.get(\"LR\")} → {hparas[\"LR\"]})')\n",
    "\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f'⚠ Could not load previous config: {e}')\n",
    "\t\t\t\trun_timestamp = RESUME_RUN\n",
    "\t\t\n",
    "\t\t# Use existing subdirectories\n",
    "\t\tcheckpoint_dir = f'{run_dir}/checkpoints'\n",
    "\t\tbest_models_dir = f'{run_dir}/best_models'\n",
    "\t\tsamples_dir = f'{run_dir}/samples'\n",
    "\t\tinference_dir = f'{run_dir}/inference'\n",
    "\t\t\n",
    "\t\t# Create directories if they don't exist (shouldn't happen, but safety check)\n",
    "\t\tos.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\t\tos.makedirs(best_models_dir, exist_ok=True)\n",
    "\t\tos.makedirs(samples_dir, exist_ok=True)\n",
    "\t\tos.makedirs(inference_dir, exist_ok=True)\n",
    "\t\t\n",
    "else:\n",
    "\t\t# Create new run\n",
    "\t\trun_timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\t\trun_dir = f'runs/{run_timestamp}'\n",
    "\t\t\n",
    "\t\t# All outputs for this run go in subdirectories\n",
    "\t\tcheckpoint_dir = f'{run_dir}/checkpoints'\n",
    "\t\tbest_models_dir = f'{run_dir}/best_models'\n",
    "\t\tsamples_dir = f'{run_dir}/samples'\n",
    "\t\tinference_dir = f'{run_dir}/inference'\n",
    "\t\t\n",
    "\t\t# Create all directories\n",
    "\t\tos.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\t\tos.makedirs(best_models_dir, exist_ok=True)\n",
    "\t\tos.makedirs(samples_dir, exist_ok=True)\n",
    "\t\tos.makedirs(inference_dir, exist_ok=True)\n",
    "\t\t\n",
    "\t\tprint(f'✓ Created NEW run directory: {run_dir}')\n",
    "\t\t\n",
    "\t\t# Save hyperparameters\n",
    "\t\tconfig_to_save = {\n",
    "\t\t\t\t'run_timestamp': run_timestamp,\n",
    "\t\t\t\t'hyperparameters': hparas,\n",
    "\t\t}\n",
    "\t\twith open(f'{run_dir}/config.json', 'w') as f:\n",
    "\t\t\t\tjson.dump(config_to_save, f, indent=4)\n",
    "\t\tprint(f'✓ Saved configuration to: {run_dir}/config.json')\n",
    "\n",
    "\n",
    "# Display directory structure\n",
    "print(f'\\nRun directory structure:')\n",
    "print(f'  {run_dir}/')\n",
    "print(f'  ├── checkpoints/  : {checkpoint_dir}')\n",
    "print(f'  ├── best_models/  : {best_models_dir}')\n",
    "print(f'  ├── samples/      : {samples_dir}')\n",
    "print(f'  └── inference/    : {inference_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Starting NEW training run - no checkpoint restoration needed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RESTORE CHECKPOINT FOR RESUMING TRAINING\n",
    "# ============================================================\n",
    "if RESUME_RUN:\n",
    "\t\t# When resuming, restore the LATEST regular checkpoint (not best model)\n",
    "\t\t# This ensures training continues from where it left off\n",
    "\t\tprint(f'\\nRestoring checkpoint for resuming training...')\n",
    "\t\tprint(f'Looking for latest checkpoint in: {checkpoint_dir}')\n",
    "\t\t\n",
    "\t\tlatest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\t\tif latest_checkpoint:\n",
    "\t\t\t\tcheckpoint.restore(latest_checkpoint).expect_partial()\n",
    "\t\t\t\tckpt_num = latest_checkpoint.split('-')[-1]\n",
    "\t\t\t\tprint(f'✓ Restored latest checkpoint: {latest_checkpoint}')\n",
    "\t\t\t\tprint(f'  Checkpoint number: {ckpt_num}')\n",
    "\t\t\t\tprint(f'  Training will continue from this point')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Also check if best model exists\n",
    "\t\t\t\tbest_checkpoint = tf.train.latest_checkpoint(best_models_dir)\n",
    "\t\t\t\tif best_checkpoint:\n",
    "\t\t\t\t\t\tprint(f'\\n✓ Best model also available at: {best_checkpoint}')\n",
    "\t\telse:\n",
    "\t\t\t\tprint('⚠ No checkpoint found in the run directory')\n",
    "\t\t\t\tprint('  Training will start from epoch 1 (this is unusual for RESUME mode)')\n",
    "else:\n",
    "\t\tprint('\\n✓ Starting NEW training run - no checkpoint restoration needed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "def train(dataset, epochs):\n",
    "\t\tglobal run_dir, checkpoint_dir, best_models_dir, samples_dir, inference_dir\n",
    "\t\t\n",
    "\t\tcheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\t\tbest_model_prefix = os.path.join(best_models_dir, \"best_ckpt\")\n",
    "\t\t\n",
    "\t\tlog_dir = f'{run_dir}/logs'\n",
    "\t\tos.makedirs(log_dir, exist_ok=True)\n",
    "\t\tsummary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\t\n",
    "\t\tprint(f\"Run tensorboard --logdir {log_dir}\")\n",
    "\t\tprint(f'Model: WGAN-GP with {hparas[\"N_CRITIC\"]} critic iterations')\n",
    "\t\tprint(f'DiffAugment: {hparas[\"USE_DIFFAUG\"]} ({hparas.get(\"DIFFAUG_POLICY\", \"N/A\")})')\n",
    "\t\t\n",
    "\t\tsteps_per_epoch = int(hparas['N_SAMPLE']/BATCH_SIZE)\n",
    "\t\tglobal_step = 0\n",
    "\t\t\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\t\t# Using sys.executable ensures we use tensorboard from the correct python env\n",
    "\t\t\t\ttensorboard_process = subprocess.Popen([\n",
    "\t\t\t\t\t\tsys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir\n",
    "\t\t\t\t])\n",
    "\t\t\t\tprint(f\"✓ TensorBoard launched as a background process (PID: {tensorboard_process.pid}).\")\n",
    "\t\t\t\tprint(\"  It might take a few seconds to become available in your browser.\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"⚠ Could not start TensorBoard automatically: {e}\")\n",
    "\t\t\t\tprint(f\"  You can start it manually by running: tensorboard --logdir {log_dir}\")\n",
    "\n",
    "\t\t# ========== EARLY STOPPING SETUP ==========\n",
    "\t\tbest_wasserstein_dist = float('inf')\n",
    "\t\tpatience = 300\n",
    "\t\tpatience_counter = 0\n",
    "\t\t# ==========================================\n",
    "\t\t\n",
    "\t\tfor epoch in range(hparas['N_EPOCH']):\n",
    "\t\t\t\t# ========== LEARNING RATE DECAY ==========\n",
    "\t\t\t\tif epoch >= hparas['LR_DECAY_START'] and epoch % hparas['LR_DECAY_EVERY'] == 0:\n",
    "\t\t\t\t\t\tcurrent_lr_g = generator_optimizer.learning_rate.numpy()\n",
    "\t\t\t\t\t\tcurrent_lr_c = critic_optimizer.learning_rate.numpy()\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tnew_lr_g = max(current_lr_g * hparas['LR_DECAY_FACTOR'], hparas['LR_MIN'])\n",
    "\t\t\t\t\t\tnew_lr_c = max(current_lr_c * hparas['LR_DECAY_FACTOR'], hparas['LR_MIN'])\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tgenerator_optimizer.learning_rate.assign(new_lr_g)\n",
    "\t\t\t\t\t\tcritic_optimizer.learning_rate.assign(new_lr_c)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tprint(f'  📉 LR Decay: G={new_lr_g:.2e}, C={new_lr_c:.2e}')\n",
    "\t\t\t\t# ==========================================\n",
    "\t\t\t\t\n",
    "\t\t\t\tg_total_loss = 0\n",
    "\t\t\t\tc_total_loss = 0\n",
    "\t\t\t\tc_total_loss_wasserstein = 0\n",
    "\t\t\t\tgp_total = 0\n",
    "\t\t\t\twd_total = 0\n",
    "\t\t\t\tstart = time.time()\n",
    "\t\t\t\t\n",
    "\t\t\t\tpbar = tqdm(dataset, desc=f'Epoch {epoch+1}/{hparas[\"N_EPOCH\"]}', \n",
    "\t\t\t\t\t\t\t\t\t total=steps_per_epoch, unit='batch')\n",
    "\t\t\t\t\n",
    "\t\t\t\tfor batch_idx, (image, input_ids, attention_mask) in enumerate(pbar):\n",
    "\t\t\t\t\t\tmetrics = train_step(image, input_ids, attention_mask)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Accumulate losses\n",
    "\t\t\t\t\t\tg_total_loss += metrics['g_loss']\n",
    "\t\t\t\t\t\tc_total_loss += metrics['c_loss']\n",
    "\t\t\t\t\t\tc_total_loss_wasserstein += metrics['c_loss_wasserstein']\n",
    "\t\t\t\t\t\tgp_total += metrics['gp']\n",
    "\t\t\t\t\t\twd_total += metrics['wasserstein_dist']\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Log to TensorBoard\n",
    "\t\t\t\t\t\twith summary_writer.as_default():\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/generator_loss', metrics['g_loss'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/critic_loss_total', metrics['c_loss'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/critic_loss_wasserstein', metrics['c_loss_wasserstein'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/critic_loss_mismatch', metrics['c_loss_mismatch'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/gradient_penalty', metrics['gp'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Metrics/wasserstein_distance', metrics['wasserstein_dist'], step=global_step)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tif global_step % 50 == 0:\n",
    "\t\t\t\t\t\t\t\t\t\ttf.summary.scalar('Gradients/generator_gradient_norm', metrics['grad_norm_g'], step=global_step)\n",
    "\t\t\t\t\t\t\t\t\t\ttf.summary.scalar('Gradients/critic_gradient_norm', metrics['grad_norm_c'], step=global_step)\n",
    "\t\t\t\t\t\t\t\t\t\t# ========== LOG LEARNING RATES ==========\n",
    "\t\t\t\t\t\t\t\t\t\ttf.summary.scalar('Training/learning_rate_generator', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tgenerator_optimizer.learning_rate.numpy(), step=global_step)\n",
    "\t\t\t\t\t\t\t\t\t\ttf.summary.scalar('Training/learning_rate_critic', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcritic_optimizer.learning_rate.numpy(), step=global_step)\n",
    "\t\t\t\t\t\t\t\t\t\t# ========================================\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Update progress bar\n",
    "\t\t\t\t\t\tpbar.set_postfix({\n",
    "\t\t\t\t\t\t\t\t'G_loss': f'{metrics[\"g_loss\"]:.4f}',\n",
    "\t\t\t\t\t\t\t\t'C_loss': f'{metrics[\"c_loss\"]:.4f}',\n",
    "\t\t\t\t\t\t\t\t'W_dist': f'{metrics[\"wasserstein_dist\"]:.4f}'\n",
    "\t\t\t\t\t\t})\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tglobal_step += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\tpbar.close()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Print epoch summary\n",
    "\t\t\t\tavg_g_loss = g_total_loss / steps_per_epoch\n",
    "\t\t\t\tavg_c_loss = c_total_loss / steps_per_epoch\n",
    "\t\t\t\tavg_c_loss_w = c_total_loss_wasserstein / steps_per_epoch\n",
    "\t\t\t\tavg_gp = gp_total / steps_per_epoch\n",
    "\t\t\t\tavg_wd = wd_total / steps_per_epoch\n",
    "\t\t\t\tepoch_time = time.time() - start\n",
    "\t\t\t\t\n",
    "\t\t\t\tprint(f'Epoch {epoch+1}: G_loss={avg_g_loss:.4f}, C_loss={avg_c_loss:.4f} ' +\n",
    "\t\t\t\t\t\t\tf'(W={avg_c_loss_w:.4f}, GP={avg_gp:.4f}), W_dist={avg_wd:.4f}, Time={epoch_time:.2f}s')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Log epoch averages\n",
    "\t\t\t\twith summary_writer.as_default():\n",
    "\t\t\t\t\t\ttf.summary.scalar('Epoch/generator_loss_avg', avg_g_loss, step=epoch)\n",
    "\t\t\t\t\t\ttf.summary.scalar('Epoch/critic_loss_avg', avg_c_loss, step=epoch)\n",
    "\t\t\t\t\t\ttf.summary.scalar('Epoch/wasserstein_distance_avg', avg_wd, step=epoch)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# ========== EARLY STOPPING CHECK ==========\n",
    "\t\t\t\tif avg_wd < best_wasserstein_dist:  # Avoid near-zero\n",
    "\t\t\t\t\t\tbest_wasserstein_dist = avg_wd\n",
    "\t\t\t\t\t\tpatience_counter = 0\n",
    "\t\t\t\t\t\t# Save best model in separate directory\n",
    "\t\t\t\t\t\tbest_path = checkpoint.save(file_prefix=best_model_prefix)\n",
    "\t\t\t\t\t\tprint(f'  ⭐ Best model saved! W_dist={avg_wd:.4f} → {best_path}')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\tpatience_counter += 1\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tif patience_counter >= patience:\n",
    "\t\t\t\t\t\tprint(f'\\n⚠️ Early stopping triggered at epoch {epoch+1}')\n",
    "\t\t\t\t\t\tprint(f'   Best Wasserstein distance: {best_wasserstein_dist:.4f}')\n",
    "\t\t\t\t\t\tprint(f'   No improvement for {patience} epochs')\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t# ==========================================\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Save checkpoint (more frequently now)\n",
    "\t\t\t\tif (epoch + 1) % 5 == 0:  # Changed from 50\n",
    "\t\t\t\t\t\tsaved_path = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\t\t\t\t\t\tprint(f'  ✓ Checkpoint saved: {saved_path}')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Visualization\n",
    "\t\t\t\tif (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "\t\t\t\t\t\tfake_image = test_step(sample_input_ids, sample_attention_mask, sample_seed)\n",
    "\t\t\t\t\t\tsave_images(fake_image, [ni, ni], f'{samples_dir}/train_{epoch+1:03d}.jpg')\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\twith summary_writer.as_default():\n",
    "\t\t\t\t\t\t\t\tdisplay_images = (fake_image + 1.0) / 2.0\n",
    "\t\t\t\t\t\t\t\ttf.summary.image('Generated_Samples', display_images, step=epoch, max_outputs=16)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tprint(f'  ✓ Sample image saved and logged to TensorBoard')\n",
    "\t\t\n",
    "\t\tprint('\\n✓ Training completed!')\n",
    "\t\tprint(f'All outputs saved to: {run_dir}')\n",
    "\t\tprint(f'Best Wasserstein distance achieved: {best_wasserstein_dist:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run tensorboard --logdir runs/20251124-021320/logs\n",
      "Model: WGAN-GP with 5 critic iterations\n",
      "DiffAugment: True (translation)\n",
      "✓ TensorBoard launched as a background process (PID: 13105).\n",
      "  It might take a few seconds to become available in your browser.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/57 [00:00<?, ?batch/s]/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.2 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.2 at http://localhost:6007/ (Press CTRL+C to quit)\n",
      "/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Epoch 1/1000:   0%|          | 0/57 [00:20<?, ?batch/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN_EPOCH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhparas[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN_EPOCH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     62\u001b[0m \t\t\t\t\t total\u001b[38;5;241m=\u001b[39msteps_per_epoch, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (image, input_ids, attention_mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m---> 65\u001b[0m \t\tmetrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \t\t\u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \t\tg_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:905\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    910\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[1;32m    911\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:598\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 598\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/5h/n64mcyts207dxrlc16rx_5cw0000gn/T/__autograph_generated_filex4449fza.py:114\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(real_image, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    112\u001b[0m grad_c \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_c\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m wrong_text_embed \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrong_text_embed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN_CRITIC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc_loss_mismatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc_loss_w\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrad_c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreal_scores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m noise \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal, ([ag__\u001b[38;5;241m.\u001b[39mld(batch_size), ag__\u001b[38;5;241m.\u001b[39mld(hparas)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ_DIM\u001b[39m\u001b[38;5;124m'\u001b[39m]],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m    116\u001b[0m gen_aug_params \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(get_aug_params), (ag__\u001b[38;5;241m.\u001b[39mld(batch_size),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:449\u001b[0m, in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    445\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(iter_, distribute\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# TODO(b/162250181): Use _tf_iterator_for_stmt(iter(iter_)...\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     for_fn \u001b[38;5;241m=\u001b[39m _tf_distributed_iterable_for_stmt\n\u001b[0;32m--> 449\u001b[0m \u001b[43mfor_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:500\u001b[0m, in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m iter_:\n\u001b[0;32m--> 500\u001b[0m     \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:466\u001b[0m, in \u001b[0;36m_py_for_stmt.<locals>.protected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprotected_body\u001b[39m(protected_iter):\n\u001b[0;32m--> 466\u001b[0m   \u001b[43moriginal_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotected_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m   after_iteration()\n\u001b[1;32m    468\u001b[0m   before_iteration()\n",
      "File \u001b[0;32m/var/folders/5h/n64mcyts207dxrlc16rx_5cw0000gn/T/__autograph_generated_filex4449fza.py:99\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     97\u001b[0m     gp \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(gradient_penalty), (ag__\u001b[38;5;241m.\u001b[39mld(critic), ag__\u001b[38;5;241m.\u001b[39mld(real_image), ag__\u001b[38;5;241m.\u001b[39mld(fake_image), ag__\u001b[38;5;241m.\u001b[39mld(text_embed), ag__\u001b[38;5;241m.\u001b[39mld(batch_size)), \u001b[38;5;28mdict\u001b[39m(diffaug_policy\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mif_exp(ag__\u001b[38;5;241m.\u001b[39mld(hparas)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSE_DIFFAUG\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(hparas)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDIFFAUG_POLICY\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparas[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSE_DIFFAUG\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m), aug_params\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(aug_params)), fscope)\n\u001b[1;32m     98\u001b[0m     c_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(c_loss_w) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(hparas)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAMBDA_GP\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(gp) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(hparas)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAMBDA_MISMATCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(c_loss_mismatch)\n\u001b[0;32m---> 99\u001b[0m grad_c \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritic_tape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(critic_optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(grad_c), ag__\u001b[38;5;241m.\u001b[39mld(critic)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:460\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:148\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    146\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/ops/array_grad.py:809\u001b[0m, in \u001b[0;36m_ReshapeGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;129m@ops\u001b[39m\u001b[38;5;241m.\u001b[39mRegisterGradient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ReshapeGrad\u001b[39m(op: ops\u001b[38;5;241m.\u001b[39mOperation, grad):\n\u001b[1;32m    807\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    808\u001b[0m       array_ops\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m--> 809\u001b[0m           _IndexedSlicesToTensorNoWarning(grad), \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m    810\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    811\u001b[0m   ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:688\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    686\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     out_type \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m--> 688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:726\u001b[0m, in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_type:\n\u001b[1;32m    724\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m constant_op\u001b[38;5;241m.\u001b[39m_tensor_shape_tensor_conversion_function(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    725\u001b[0m           input_shape)\n\u001b[0;32m--> 726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_type:\n\u001b[1;32m    728\u001b[0m   out_type \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mint32\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    174\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:286\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 286\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_broadcast\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:268\u001b[0m, in \u001b[0;36m_create_graph_constant\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    266\u001b[0m dtype_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtensor_value\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    267\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensor_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value}\n\u001b[0;32m--> 268\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[1;32m    272\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[1;32m    273\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m   callback_outputs \u001b[38;5;241m=\u001b[39m op_callbacks\u001b[38;5;241m.\u001b[39minvoke_op_callbacks(\n\u001b[1;32m    275\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[38;5;241m=\u001b[39mname, graph\u001b[38;5;241m=\u001b[39mg)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[1;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:2651\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   2648\u001b[0m control_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_control_dependencies_for_inputs(input_ops)\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m-> 2651\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m   2652\u001b[0m   ret \u001b[38;5;241m=\u001b[39m Operation\u001b[38;5;241m.\u001b[39mfrom_node_def(\n\u001b[1;32m   2653\u001b[0m       node_def,\n\u001b[1;32m   2654\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2660\u001b[0m       op_def\u001b[38;5;241m=\u001b[39mop_def,\n\u001b[1;32m   2661\u001b[0m   )\n\u001b[1;32m   2662\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/util/lock_util.py:130\u001b[0m, in \u001b[0;36mGroupLock._Context.__exit__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, type_arg, value_arg, traceback_arg):\n\u001b[1;32m    129\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m type_arg, value_arg, traceback_arg\n\u001b[0;32m--> 130\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelease\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_group_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorflow/python/util/lock_util.py:102\u001b[0m, in \u001b[0;36mGroupLock.release\u001b[0;34m(self, group_id)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_member_counts[group_id] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_member_counts[group_id] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotify_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ready\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/threading.py:396\u001b[0m, in \u001b[0;36mCondition.notify_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnotify_all\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    390\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wake up all threads waiting on this condition.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    If the calling thread has not acquired the lock when this method\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m    is called, a RuntimeError is raised.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_waiters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/threading.py:369\u001b[0m, in \u001b[0;36mCondition.notify\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnotify\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wake up one or more threads waiting on this condition, if any.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    If the calling thread has not acquired the lock when this method is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m \n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_owned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot notify on un-acquired lock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    371\u001b[0m     waiters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\n",
      "File \u001b[0;32m/opt/anaconda3/envs/comp3/lib/python3.10/threading.py:282\u001b[0m, in \u001b[0;36mCondition._is_owned\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_is_owned\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# Return True if lock is owned by current_thread.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# This method is called only if _lock doesn't have _is_owned().\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Evaluation</center></h1>\n",
    "\n",
    "<p><code>dataset/testData.pkl</code> is a pandas dataframe containing testing text with attributes 'ID' and 'Captions'.</p>\n",
    "\n",
    "<ul>\n",
    "<li>'ID': text ID used to name generated image.</li>\n",
    "<li>'Captions': text used as condition to generate image.</li>\n",
    "</ul>\n",
    "\n",
    "<p>For each captions, you need to generate <strong>inference_ID.png</strong> to evaluate quality of generated image. You must name the generated image in this format, otherwise we cannot evaluate your images.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string\n",
    "\t\t\t\tindex: Test sample ID\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tinput_ids, attention_mask, index\n",
    "\t\t\"\"\"\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing dataset generator - decodes IDs to raw text\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_pickle('./dataset/testData.pkl')\n",
    "\t\tcaptions_ids = data['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\tchosen_caption_ids = captions_ids[i]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\tindex = data['ID'].values\n",
    "\t\tindex = np.asarray(index)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.repeat().batch(batch_size)\n",
    "\t\t\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference directory is already created by the train() function\n",
    "# No need to create it again here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore BEST MODEL for inference\n",
    "print(f'Looking for BEST model in: {best_models_dir}')\n",
    "\n",
    "best_checkpoint = tf.train.latest_checkpoint(best_models_dir)\n",
    "if best_checkpoint:\n",
    "\t\tcheckpoint.restore(best_checkpoint)\n",
    "\t\tprint(f'✓ Restored BEST model: {best_checkpoint}')\n",
    "\t\tprint(f'  This is the model with the lowest Wasserstein distance during training')\n",
    "else:\n",
    "\t\tprint('⚠ No best model found, trying regular checkpoints...')\n",
    "\t\tlatest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\t\tif latest_checkpoint:\n",
    "\t\t\t\tcheckpoint.restore(latest_checkpoint)\n",
    "\t\t\t\tprint(f'✓ Restored latest checkpoint: {latest_checkpoint}')\n",
    "\t\t\t\tprint('  ⚠ WARNING: Using latest checkpoint, not best model')\n",
    "\t\telse:\n",
    "\t\t\t\tprint('⚠ No checkpoint found at all, using fresh/untrained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated inference function for CLIP\n",
    "\t\tFIXED: Generate fresh random noise for each batch!\n",
    "\t\t\"\"\"\n",
    "\t\tsample_size = BATCH_SIZE\n",
    "\t\t\n",
    "\t\tstep = 0\n",
    "\t\tstart = time.time()\n",
    "\t\ttotal_images = 0\n",
    "\t\t\n",
    "\t\t# Progress bar for inference\n",
    "\t\tpbar = tqdm(total=NUM_TEST, desc='Generating images', unit='img')\n",
    "\t\t\n",
    "\t\t# Unpack 3 values: input_ids, attention_mask, idx\n",
    "\t\tfor input_ids, attention_mask, idx in dataset:\n",
    "\t\t\t\tif step > EPOCH_TEST:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\t# CRITICAL FIX: Generate FRESH random noise for each batch\n",
    "\t\t\t\t# This ensures diversity across all 819 test images\n",
    "\t\t\t\tsample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "\t\t\t\t\n",
    "\t\t\t\tfake_image = test_step(input_ids, attention_mask, sample_seed)\n",
    "\t\t\t\tstep += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\tfor i in range(BATCH_SIZE):\n",
    "\t\t\t\t\t\tplt.imsave(f'{inference_dir}/inference_{idx[i]:04d}.jpg', fake_image[i].numpy()*0.5 + 0.5)\n",
    "\t\t\t\t\t\ttotal_images += 1\n",
    "\t\t\t\t\t\tpbar.update(1)\n",
    "\t\t\n",
    "\t\tpbar.close()\n",
    "\t\tprint(f'\\n✓ Generated {total_images} images in {time.time()-start:.4f} sec')\n",
    "\t\tprint(f'✓ Images saved to: {inference_dir}')\n",
    "\t\tprint(f'✓ Each image generated with unique random noise for better diversity!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "\t\tprint(f'⚠ No images found in {inference_dir}')\n",
    "\t\tprint('Please run the inference cell first!')\n",
    "else:\n",
    "\t\t# Randomly sample 20 images\n",
    "\t\tnp.random.seed(42)  # For reproducibility\n",
    "\t\tnum_samples = min(20, len(image_files))\n",
    "\t\tsample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "\t\tsample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "\t\t# Create 4x5 grid\n",
    "\t\tfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\t\taxes = axes.flatten()\n",
    "\n",
    "\t\tfor idx, img_path in enumerate(sample_files):\n",
    "\t\t\t\t# Extract image ID from filename\n",
    "\t\t\t\timg_id = int(Path(img_path).stem.split('_')[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Find caption\n",
    "\t\t\t\tcaption_idx = np.where(test_ids == img_id)[0][0]\n",
    "\t\t\t\tcaption_ids = test_captions[caption_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode caption\n",
    "\t\t\t\tcaption_text = ''\n",
    "\t\t\t\tfor word_id in caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':\n",
    "\t\t\t\t\t\t\t\tcaption_text += word + ' '\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Load and display image\n",
    "\t\t\t\timg = plt.imread(img_path)\n",
    "\t\t\t\taxes[idx].imshow(img)\n",
    "\t\t\t\taxes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\t# Hide unused subplots if less than 20 images\n",
    "\t\tfor idx in range(num_samples, 20):\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\tprint(f'\\nTotal generated images: {len(image_files)}')\n",
    "\t\tprint(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
