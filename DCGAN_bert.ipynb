{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Text to Image</center></h1>\n",
    "\n",
    "<h2 id=\"Platform:-Kaggle\">Platform: <a href=\"https://www.kaggle.com/competitions/2025-datalab-cup-3-reverse-image-caption/overview\">Kaggle</a><a class=\"anchor-link\" href=\"#Platform:-Kaggle\">¶</a></h2>\n",
    "<h2 id=\"Overview\">Overview<a class=\"anchor-link\" href=\"#Overview\">¶</a></h2>\n",
    "<p>In this work, we are interested in translating text in the form of single-sentence human-written descriptions directly into image pixels. For example, \"<strong>this flower has petals that are yellow and has a ruffled stamen</strong>\" and \"<strong>this pink and yellow flower has a beautiful yellow center with many stamens</strong>\". You have to develop a novel deep architecture and GAN formulation to effectively translate visual concepts from characters to pixels.</p>\n",
    "\n",
    "<p>More specifically, given a set of texts, your task is to generate reasonable images with size 64x64x3 to illustrate the corresponding texts. Here we use <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford-102 flower dataset</a> and its <a href=\"https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view\">paired texts</a> as our training dataset.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/example.png\"/>\n",
    "\n",
    "<ul>\n",
    "<li>7370 images as training set, where each images is annotated with at most 10 texts.</li>\n",
    "<li>819 texts for testing. You must generate 1 64x64x3 image for each text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN\">Conditional GAN<a class=\"anchor-link\" href=\"#Conditional-GAN\">¶</a></h2>\n",
    "<p>Given a text, in order to generate the image which can illustrate it, our model must meet several requirements:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Our model should have ability to understand and extract the meaning of given texts.<ul>\n",
    "<li>Use RNN or other language model, such as BERT, ELMo or XLNet, to capture the meaning of text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Our model should be able to generate image.<ul>\n",
    "<li>Use GAN to generate high quality image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>GAN-generated image should illustrate the text.<ul>\n",
    "<li>Use conditional-GAN to generate image conditioned on given text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information $y$. We can perform the conditioning by feeding $y$ into both the discriminator and generator as additional input layer.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/cGAN.png\" width=\"500\"/>\n",
    "\n",
    "<p>There are two motivations for using some extra information in a GAN model:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Improve GAN.</li>\n",
    "<li>Generate targeted image.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Additional information that is correlated with the input images, such as class labels, can be used to improve the GAN. This improvement may come in the form of more stable training, faster training, and/or generated images that have better quality.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/GANCLS.jpg\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "\ttry:\n",
    "\t\t# Restrict TensorFlow to only use the first GPU\n",
    "\t\ttf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "\t\t# Currently, memory growth needs to be the same across GPUs\n",
    "\t\tfor gpu in gpus:\n",
    "\t\t\ttf.config.experimental.set_memory_growth(gpu, True)\n",
    "\t\tlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\t\tprint(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\texcept RuntimeError as e:\n",
    "\t\t# Memory growth must be set before GPUs have been initialized\n",
    "\t\tprint(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell previously contained sent2IdList() function\n",
    "# It has been removed as we now use CLIP tokenizer instead\n",
    "# The id2word_dict is still available from cell 6 for visualization purposes\n",
    "\n",
    "print(\"✓ Using CLIP tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# Load CLIP Tokenizer\n",
    "# \"openai/clip-vit-base-patch32\" is a standard, powerful model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def preprocess_text_clip(text, max_length=77):\n",
    "\t\tencoded = tokenizer(\n",
    "\t\t\t\ttext,\n",
    "\t\t\t\tpadding='max_length',\n",
    "\t\t\t\ttruncation=True,\n",
    "\t\t\t\tmax_length=max_length,\n",
    "\t\t\t\treturn_tensors='tf'\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t\t'input_ids': encoded['input_ids'],\n",
    "\t\t\t\t'attention_mask': encoded['attention_mask']\n",
    "\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiffAugment(x, policy='color,translation,cutout', channels_first=False, params=None):\n",
    "\t\t\"\"\"\n",
    "\t\tDifferentiable augmentation for GANs\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tx: Input images [batch, H, W, C] \n",
    "\t\t\t\tpolicy: Comma-separated augmentation policies\n",
    "\t\t\t\tchannels_first: If True, expects [batch, C, H, W]\n",
    "\t\t\t\tparams: Optional dict of pre-generated augmentation parameters for consistency\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tAugmented images\n",
    "\t\t\"\"\"\n",
    "\t\tif policy:\n",
    "\t\t\t\tif not channels_first:\n",
    "\t\t\t\t\t\t# TensorFlow format: [batch, H, W, C]\n",
    "\t\t\t\t\t\tfor p in policy.split(','):\n",
    "\t\t\t\t\t\t\t\tfor f in AUGMENT_FNS[p]:\n",
    "\t\t\t\t\t\t\t\t\t\tx = f(x, params)  # ← Pass params!\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def rand_brightness(x, params=None):\n",
    "\t\t\"\"\"Random brightness adjustment\"\"\"\n",
    "\t\tif params is not None and 'brightness' in params:\n",
    "\t\t\t\tmagnitude = params['brightness']\n",
    "\t\telse:\n",
    "\t\t\t\tmagnitude = tf.random.uniform([], -0.5, 0.5)\n",
    "\t\tx = x + magnitude\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def rand_saturation(x, params=None):\n",
    "\t\t\"\"\"Random saturation adjustment\"\"\"\n",
    "\t\tif params is not None and 'saturation' in params:\n",
    "\t\t\t\tmagnitude = params['saturation']\n",
    "\t\telse:\n",
    "\t\t\t\tmagnitude = tf.random.uniform([], 0.0, 2.0)\n",
    "\t\tx_mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "\t\tx = (x - x_mean) * magnitude + x_mean\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def rand_contrast(x, params=None):\n",
    "\t\t\"\"\"Random contrast adjustment\"\"\"\n",
    "\t\tif params is not None and 'contrast' in params:\n",
    "\t\t\t\tmagnitude = params['contrast']\n",
    "\t\telse:\n",
    "\t\t\t\tmagnitude = tf.random.uniform([], 0.5, 1.5)\n",
    "\t\tx_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "\t\tx = (x - x_mean) * magnitude + x_mean\n",
    "\t\treturn x\n",
    "\n",
    "def rand_translation(x, params=None, ratio=0.125):\n",
    "\t\t\"\"\"Random translation (shift) - Fully vectorized for @tf.function\"\"\"\n",
    "\t\tbatch_size = tf.shape(x)[0]\n",
    "\t\timage_size = tf.shape(x)[1]\n",
    "\t\tshift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "\t\t\n",
    "\t\t# Random translation amounts for entire batch\n",
    "\t\tif params is not None and 'translation_x' in params:\n",
    "\t\t\t\ttranslation_x = params['translation_x']\n",
    "\t\t\t\ttranslation_y = params['translation_y']\n",
    "\t\telse:\n",
    "\t\t\t\ttranslation_x = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "\t\t\t\ttranslation_y = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "\t\t\n",
    "\t\tdef translate_single_image(args):\n",
    "\t\t\t\t\"\"\"Translate a single image\"\"\"\n",
    "\t\t\t\timg, tx, ty = args\n",
    "\t\t\t\timg = tf.pad(img, [[shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "\t\t\t\timg = tf.image.crop_to_bounding_box(img, shift + ty, shift + tx, image_size, image_size)\n",
    "\t\t\t\treturn img\n",
    "\t\t\n",
    "\t\t# Use tf.map_fn (graph-mode compatible)\n",
    "\t\tx_translated = tf.map_fn(\n",
    "\t\t\t\ttranslate_single_image,\n",
    "\t\t\t\t(x, translation_x, translation_y),\n",
    "\t\t\t\tfn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "\t\t\t\tparallel_iterations=BATCH_SIZE\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\treturn x_translated\n",
    "\n",
    "\n",
    "def rand_cutout(x, params=None, ratio=0.2):\n",
    "\t\t\"\"\"\n",
    "\t\tRandom cutout - SIMPLIFIED vectorized version\n",
    "\t\t\n",
    "\t\tInstead of complex per-pixel masking, we create rectangular masks\n",
    "\t\tusing broadcasting and boolean operations\n",
    "\t\t\"\"\"\n",
    "\t\tbatch_size = tf.shape(x)[0]\n",
    "\t\timage_size = tf.shape(x)[1]\n",
    "\t\tchannels = tf.shape(x)[3]\n",
    "\t\t\n",
    "\t\t# Cutout size\n",
    "\t\tcutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "\t\t\n",
    "\t\t# Random offset for cutout location\n",
    "\t\tif params is not None and 'cutout_x' in params:\n",
    "\t\t\t\toffset_x = params['cutout_x']\n",
    "\t\t\t\toffset_y = params['cutout_y']\n",
    "\t\telse:\n",
    "\t\t\t\toffset_x = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "\t\t\t\toffset_y = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "\t\t\n",
    "\t\tdef cutout_single_image(args):\n",
    "\t\t\t\t\"\"\"Apply cutout to single image using simple slicing\"\"\"\n",
    "\t\t\t\timg, ox, oy = args\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Create coordinate grids\n",
    "\t\t\t\theight_range = tf.range(image_size)\n",
    "\t\t\t\twidth_range = tf.range(image_size)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Create 2D grids\n",
    "\t\t\t\tyy, xx = tf.meshgrid(height_range, width_range, indexing='ij')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Create mask: True where we want to KEEP pixels\n",
    "\t\t\t\tmask_y = tf.logical_or(yy < oy, yy >= oy + cutout_size)\n",
    "\t\t\t\tmask_x = tf.logical_or(xx < ox, xx >= ox + cutout_size)\n",
    "\t\t\t\tmask = tf.logical_or(mask_y, mask_x)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Expand mask to all channels\n",
    "\t\t\t\tmask = tf.expand_dims(mask, axis=-1)  # [H, W, 1]\n",
    "\t\t\t\tmask = tf.tile(mask, [1, 1, channels])  # [H, W, C]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Apply mask (convert bool to float)\n",
    "\t\t\t\tmask = tf.cast(mask, tf.float32)\n",
    "\t\t\t\treturn img * mask\n",
    "\t\t\n",
    "\t\t# Use tf.map_fn\n",
    "\t\tx_cutout = tf.map_fn(\n",
    "\t\t\t\tcutout_single_image,\n",
    "\t\t\t\t(x, offset_x, offset_y),\n",
    "\t\t\t\tfn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "\t\t\t\tparallel_iterations=BATCH_SIZE\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\treturn x_cutout\n",
    "\n",
    "\n",
    "# Augmentation function registry\n",
    "AUGMENT_FNS = {\n",
    "\t\t'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "\t\t'translation': [rand_translation],\n",
    "\t\t'cutout': [rand_cutout],\n",
    "}\n",
    "\n",
    "\n",
    "print(\"✓ DiffAugment functions loaded\")\n",
    "print(\"  Policies available: color, translation, cutout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string (not IDs!)\n",
    "\t\t\t\timage_path: Path to image file\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\timg, input_ids, attention_mask\n",
    "\t\t\"\"\"\n",
    "\t\t# ============= IMAGE PROCESSING (same as before) =============\n",
    "\t\timg = tf.io.read_file(image_path)\n",
    "\t\timg = tf.image.decode_image(img, channels=3)\n",
    "\t\timg = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "\t\timg.set_shape([None, None, 3])\n",
    "\t\timg = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "\t\t\n",
    "\t\t# Normalize to [-1, 1] to match generator's tanh output\n",
    "\t\timg = (img * 2.0) - 1.0\n",
    "\t\timg.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "\t\t\n",
    "\t\t# ============= TEXT PROCESSING (NEW: Use CLIP tokenizer) =============\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'  # Use numpy arrays for TF compatibility\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly for CLIP\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated dataset generator to work with raw text (decoded from IDs)\n",
    "\t\t\"\"\"\n",
    "\t\t# Load the training data\n",
    "\t\tdf = pd.read_pickle(filenames)\n",
    "\t\tcaptions_ids = df['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to raw text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\t# Randomly choose one caption (list of ID lists)\n",
    "\t\t\t\tchosen_caption_ids = random.choice(captions_ids[i])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\timage_paths = df['ImagePath'].values\n",
    "\t\t\n",
    "\t\t# Verify same length\n",
    "\t\tassert len(caption_texts) == len(image_paths)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text and image paths\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, image_paths))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.cache()\n",
    "\t\tdataset = dataset.shuffle(len(caption_texts)).batch(batch_size, drop_remainder=True)\n",
    "\t\tdataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¶</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import TFCLIPTextModel, TFCLIPModel\n",
    "\n",
    "class ClipTextEncoder(tf.keras.Model):\n",
    "\t\tdef __init__(self, output_dim=512, freeze_clip=True):\n",
    "\t\t\t\tsuper(ClipTextEncoder, self).__init__()\n",
    "\t\t\t\t# Load Pre-trained CLIP Text Model\n",
    "\t\t\t\tself.clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\t\t\t\t\n",
    "\t\t\t\tif freeze_clip:\n",
    "\t\t\t\t\t\tself.clip.trainable = False\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t# REMOVED: Projection, LayerNorm, Dropout to ensure RAW embeddings\n",
    "\t\n",
    "\t\tdef call(self, input_ids, attention_mask, training=False):\n",
    "\t\t\t# 1. Get the projected features (Aligned with images, e.g., 512-dim)\n",
    "\t\t\ttext_embeds = self.clip.get_text_features(\n",
    "\t\t\t\tinput_ids=input_ids, \n",
    "\t\t\t\tattention_mask=attention_mask\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\t# 2. CRITICAL FIX: Manually normalize to get the actual CLIP embedding\n",
    "\t\t\t# CLIP uses cosine similarity, so vectors must be unit length.\n",
    "\t\t\ttext_embeds = tf.math.l2_normalize(text_embeds, axis=1)\n",
    "\t\t\t\n",
    "\t\t\treturn text_embeds\n",
    "\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Generator\">Generator<a class=\"anchor-link\" href=\"#Generator\">¶</a></h2>\n",
    "<p>A image generator which generates the target image illustrating the input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: hidden representation of input text and random noise z with random seed.</li>\n",
    "<li>Output: target image, which is conditioned on the given text, in size 64x64x3.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization as per DCGAN paper\n",
    "def dcgan_weight_init():\n",
    "\t\treturn tf.keras.initializers.HeNormal()\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # 1. Initialize Weights (He Normal is better for WGAN-GP)\n",
    "        init = tf.keras.initializers.HeNormal()\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # [NEW] Text Conditioning Projection (\"The Translator\")\n",
    "        # Maps the 512-dim unit-vector from CLIP to a learned 128-dim space\n",
    "        # that allows the Generator to \"understand\" the instruction.\n",
    "        # ---------------------------------------------------------\n",
    "        self.text_projection = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, kernel_initializer=init),\n",
    "            tf.keras.layers.LeakyReLU(0.2)\n",
    "        ])\n",
    "        \n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(4 * 4 * 512, use_bias=False, kernel_initializer=init)\n",
    "        self.bn0 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 3. Upsample Blocks\n",
    "        self.up1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv1 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.up2 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.up3 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 4. Final Layer\n",
    "        self.up4 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        # Note: use_bias=True here is critical (as discussed before)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', use_bias=True, kernel_initializer=init)\n",
    "        \n",
    "    def call(self, text_embedding, noise_z, training=True):\n",
    "        # ---------------------------------------------------------\n",
    "        # Step 1: Process the Text\n",
    "        # text_embedding shape: [Batch, 512] (Normalized)\n",
    "        # text_feat shape:      [Batch, 128] (Unbounded, Learnable)\n",
    "        # ---------------------------------------------------------\n",
    "        text_feat = self.text_projection(text_embedding)\n",
    "        \n",
    "        # Step 2: Concatenate with Noise\n",
    "        # noise_z shape: [Batch, 100]\n",
    "        # x shape:       [Batch, 228]\n",
    "        x = tf.concat([noise_z, text_feat], axis=1)\n",
    "        \n",
    "        # Step 3: Project to 4x4x512\n",
    "        x = self.dense(x)\n",
    "        x = self.bn0(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.reshape(x, [-1, 4, 4, 512])\n",
    "        \n",
    "        # Step 4: Upsampling\n",
    "        x = self.up1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Step 5: Final Generation\n",
    "        x = self.up4(x)\n",
    "        output = self.conv4(x)\n",
    "        output = tf.nn.tanh(output)\n",
    "        \n",
    "        return x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Discriminator\">Discriminator<a class=\"anchor-link\" href=\"#Discriminator\">¶</a></h2>\n",
    "<p>A binary classifier which can discriminate the real and fake image:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Real image<ul>\n",
    "<li>Input: real image and the paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 1.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Fake Image<ul>\n",
    "<li>Input: generated image and paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 0.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "\t\t\"\"\"\n",
    "\t\tProjection Discriminator with Stability Fixes\n",
    "\t\t\"\"\"\n",
    "\t\tdef __init__(self, hparas):\n",
    "\t\t\t\tsuper(Critic, self).__init__()\n",
    "\t\t\t\tself.hparas = hparas\n",
    "\t\t\t\tinit = tf.keras.initializers.HeNormal()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# --- IMAGE PATH ---\n",
    "\t\t\t\t# 64 -> 32\n",
    "\t\t\t\tself.conv1 = tf.keras.layers.Conv2D(64, 4, 2, padding='same', kernel_initializer=init)\n",
    "\t\t\t\t#self.ln1 = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 32 -> 16\n",
    "\t\t\t\tself.conv2 = tf.keras.layers.Conv2D(128, 4, 2, padding='same', kernel_initializer=init)\n",
    "\t\t\t\tself.ln2 = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 16 -> 8\n",
    "\t\t\t\tself.conv3 = tf.keras.layers.Conv2D(256, 4, 2, padding='same', kernel_initializer=init)\n",
    "\t\t\t\tself.ln3 = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 8 -> 4\n",
    "\t\t\t\tself.conv4 = tf.keras.layers.Conv2D(512, 4, 2, padding='same', kernel_initializer=init)\n",
    "\t\t\t\tself.ln4 = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# --- TEXT PATH ---\n",
    "\t\t\t\t# Project text to match the depth of image features (512 channels)\n",
    "\t\t\t\tself.text_dense = tf.keras.layers.Dense(512, kernel_initializer=tf.keras.initializers.Orthogonal())\n",
    "\t\t\t\tself.text_dense_ln = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# --- SCORING ---\n",
    "\t\t\t\t# \"Realism\" score (f(x))\n",
    "\t\t\t\tself.flatten = tf.keras.layers.Flatten()\n",
    "\t\t\t\tself.disc_realism = tf.keras.layers.Dense(1, kernel_initializer=init)\n",
    "\n",
    "\t\tdef call(self, img, text, training=True):\n",
    "\t\t\t\t# 1. Extract Image Features\n",
    "\t\t\t\tx = tf.nn.leaky_relu(self.conv1(img), alpha=0.2)\n",
    "\t\t\t\tx = tf.nn.leaky_relu(self.ln2(self.conv2(x), training=training), alpha=0.2)\n",
    "\t\t\t\tx = tf.nn.leaky_relu(self.ln3(self.conv3(x), training=training), alpha=0.2)\n",
    "\t\t\t\tx = tf.nn.leaky_relu(self.ln4(self.conv4(x), training=training), alpha=0.2)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# x shape: [Batch, 4, 4, 512]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 2. Process Text Features (psi(y))\n",
    "\t\t\t\tpsi_y = self.text_dense(text) \n",
    "\t\t\t\tpsi_y = self.text_dense_ln(psi_y, training=training)\n",
    "\t\t\t\tpsi_y = tf.nn.leaky_relu(psi_y, alpha=0.2)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 3. PROJECTION SCORE (Alignment)\n",
    "\t\t\t\t# Reshape text to [Batch, 1, 1, 512]\n",
    "\t\t\t\t# psi_y_reshaped = tf.reshape(psi_y, [-1, 1, 1, 512])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Global Sum Pooling of image features for projection dot product\n",
    "\t\t\t\t# Note: Original used sum([1,2]), optimized uses reduce_sum on phi_x * psi_y.\n",
    "\t\t\t\t# Both are mathematically similar, but let's stick to the stable explicit dot product:\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Get global image vector\n",
    "\t\t\t\timg_vec = tf.reduce_mean(x, axis=[1, 2]) # [Batch, 512]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Dot product\n",
    "\t\t\t\talignment_score = tf.reduce_sum(img_vec * psi_y, axis=1, keepdims=True)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 4. REALISM SCORE\n",
    "\t\t\t\tflat_img = self.flatten(x)\n",
    "\t\t\t\trealism_score = self.disc_realism(flat_img)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# 5. TOTAL SCORE\n",
    "\t\t\t\treturn realism_score + alignment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "\t\t'MAX_SEQ_LENGTH': 77,          # CLIP Standard\n",
    "\t\t'EMBED_DIM': 512,              # CLIP Output\n",
    "\t\t'VOCAB_SIZE': 49408,           # CLIP Vocab Size (approx)\n",
    "\t\t'RNN_HIDDEN_SIZE': 512,        # Target projection size (Updated to 512 for Raw CLIP)\n",
    "\t\t'Z_DIM': 512,\n",
    "\t\t'DENSE_DIM': 128,\n",
    "\t\t'IMAGE_SIZE': [64, 64, 3],\n",
    "\t\t\n",
    "\t\t'BATCH_SIZE': BATCH_SIZE,      # Fix: Use Global Variable\n",
    "\t\t'LR': 2e-4,\n",
    "\t\t'BETA_1': 0.0,\n",
    "\t\t'BETA_2': 0.9,\n",
    "\t\t'N_CRITIC': 5,\n",
    "\t\t'LAMBDA_GP': 10.0,\n",
    "\t\t'LAMBDA_MISMATCH': 1.0,\n",
    "\t\t\n",
    "\t\t'LR_DECAY_START': 50,\n",
    "\t\t'LR_DECAY_EVERY': 10,\n",
    "\t\t'LR_DECAY_FACTOR': 0.95,\n",
    "\t\t'LR_MIN': 1e-5,\n",
    "\t\t\n",
    "\t\t'USE_DIFFAUG': True,\n",
    "\t\t'DIFFAUG_POLICY': 'translation', # Conservative augmentation\n",
    "\t\t\n",
    "\t\t'N_EPOCH': 1000,\n",
    "\t\t'N_SAMPLE': num_training_sample,\n",
    "\t\t'PRINT_FREQ': 5\n",
    "}\n",
    "\n",
    "print(f\"✓ Hyperparameters updated:\")\n",
    "print(f\"  Batch size: {hparas['BATCH_SIZE']}\")\n",
    "print(f\"  Learning rate: {hparas['LR']}\")\n",
    "print(f\"  N_Critic: {hparas['N_CRITIC']}\")\n",
    "print(f\"  Lambda_GP: {hparas['LAMBDA_GP']}\")\n",
    "print(f\"  DiffAugment: {hparas['USE_DIFFAUG']} ({hparas['DIFFAUG_POLICY']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = ClipTextEncoder(output_dim=hparas['RNN_HIDDEN_SIZE'], freeze_clip=True)\n",
    "generator = Generator(hparas)\n",
    "critic = Critic(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Loss-Function-and-Optimization\">Loss Function and Optimization<a class=\"anchor-link\" href=\"#Loss-Function-and-Optimization\">¶</a></h2>\n",
    "<p>Although the conditional GAN model is quite complex, the loss function used to optimize the network is relatively simple. Actually, it is simply a binary classification task, thus we use cross entropy as our loss.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss_critic(real_scores, fake_scores):\n",
    "\t\t\"\"\"\n",
    "\t\tWasserstein loss for critic\n",
    "\t\tCritic wants to maximize: E[critic(real)] - E[critic(fake)]\n",
    "\t\tSo we minimize: E[critic(fake)] - E[critic(real)]\n",
    "\t\t\"\"\"\n",
    "\t\treturn tf.reduce_mean(fake_scores) - tf.reduce_mean(real_scores)\n",
    "\n",
    "def mismatch_loss_critic(real_scores, wrong_scores, margin=1.0):\n",
    "\t\t\"\"\"\n",
    "\t\tHinge loss for GAN-CLS (mismatched pairs).\n",
    "\t\tWants: real_scores > wrong_scores + margin\n",
    "\t\tMinimizes: max(0, margin + wrong_scores - real_scores)\n",
    "\t\t\"\"\"\n",
    "\t\tloss = tf.nn.relu(margin + wrong_scores - real_scores)\n",
    "\t\treturn tf.reduce_mean(loss)\n",
    "\n",
    "def wasserstein_loss_generator(fake_scores):\n",
    "\t\t\"\"\"\n",
    "\t\tWasserstein loss for generator\n",
    "\t\tGenerator wants to maximize: E[critic(fake)]\n",
    "\t\tSo we minimize: -E[critic(fake)]\n",
    "\t\t\"\"\"\n",
    "\t\treturn -tf.reduce_mean(fake_scores)\n",
    "\n",
    "def gradient_penalty(critic, real_images, fake_images, text_embed, batch_size, \n",
    "                     diffaug_policy=None, aug_params=None):\n",
    "    \"\"\"\n",
    "    Computes Gradient Penalty with CORRECT DiffAugment application.\n",
    "    \n",
    "    CORRECTED LOGIC:\n",
    "    1. Augment Real and Fake images FIRST.\n",
    "    2. Interpolate between the AUGMENTED images.\n",
    "    3. Compute gradients w.r.t the INTERPOLATION.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Apply DiffAugment to RAW images FIRST\n",
    "    # We essentially treat the augmented images as our \"training data\" for this step.\n",
    "    if diffaug_policy is not None and aug_params is not None:\n",
    "        real_images_used = DiffAugment(real_images, policy=diffaug_policy, params=aug_params)\n",
    "        fake_images_used = DiffAugment(fake_images, policy=diffaug_policy, params=aug_params)\n",
    "    else:\n",
    "        real_images_used = real_images\n",
    "        fake_images_used = fake_images\n",
    "\n",
    "    # 2. Interpolate between the AUGMENTED images\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    interpolated = alpha * real_images_used + (1.0 - alpha) * fake_images_used\n",
    "    \n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        # 3. Watch the INTERPOLATED (Augmented) image\n",
    "        gp_tape.watch(interpolated)\n",
    "            \n",
    "        # 4. Critic Pass (No further augmentation needed)\n",
    "        interpolated_scores = critic(interpolated, text_embed, training=True)\n",
    "    \n",
    "    # 5. Compute Gradients w.r.t INTERPOLATED input\n",
    "    # Now we are checking the slope of the Critic on the augmented manifold.\n",
    "    gradients = gp_tape.gradient(interpolated_scores, [interpolated])[0]\n",
    "    \n",
    "    # 6. Compute Norm\n",
    "    gradients_sqr_sum = tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3])\n",
    "    gradients_norm = tf.sqrt(gradients_sqr_sum + 1e-12)\n",
    "    \n",
    "    # 7. Penalty\n",
    "    return tf.reduce_mean(tf.square(gradients_norm - 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN-GP: Use Adam with beta_1=0.0, beta_2=0.9\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "\t\tlearning_rate=hparas['LR'],\n",
    "\t\tbeta_1=hparas['BETA_1'],\n",
    "\t\tbeta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(\n",
    "\t\tlearning_rate=hparas['LR'],\n",
    "\t\tbeta_1=hparas['BETA_1'],\n",
    "\t\tbeta_2=hparas['BETA_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(\n",
    "\t\tgenerator_optimizer=generator_optimizer,\n",
    "\t\tcritic_optimizer=critic_optimizer,\n",
    "\t\ttext_encoder=text_encoder,\n",
    "\t\tgenerator=generator,\n",
    "\t\tcritic=critic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wasserstein_distance(real_scores, fake_scores):\n",
    "\t\t\"\"\"\n",
    "\t\tApproximation of Wasserstein distance\n",
    "\t\tHigher is better (critic getting better at distinguishing)\n",
    "\t\t\"\"\"\n",
    "\t\treturn tf.reduce_mean(real_scores) - tf.reduce_mean(fake_scores)\n",
    "\n",
    "def calculate_gradient_norm(gradients):\n",
    "\t\t\"\"\"Calculate L2 norm of gradients\"\"\"\n",
    "\t\tsquared_norms = [tf.reduce_sum(tf.square(g)) for g in gradients if g is not None]\n",
    "\t\ttotal_norm = tf.sqrt(tf.reduce_sum(squared_norms))\n",
    "\t\treturn total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, input_ids, attention_mask):\n",
    "    batch_size = tf.shape(real_image)[0]\n",
    "    \n",
    "    # --- Helper to generate consistent Augment Params ---\n",
    "    def get_aug_params(bs):\n",
    "        if not hparas['USE_DIFFAUG']: return None\n",
    "        image_size = 64\n",
    "        shift = tf.cast(image_size * 0.125 + 0.5, tf.int32)\n",
    "        cutout_size = tf.cast(image_size * 0.5 + 0.5, tf.int32)\n",
    "        \n",
    "        return {\n",
    "            'brightness': tf.random.uniform([], -0.5, 0.5),\n",
    "            'saturation': tf.random.uniform([], 0.0, 2.0),\n",
    "            'contrast': tf.random.uniform([], 0.5, 1.5),\n",
    "            'translation_x': tf.random.uniform([bs], -shift, shift + 1, dtype=tf.int32),\n",
    "            'translation_y': tf.random.uniform([bs], -shift, shift + 1, dtype=tf.int32),\n",
    "            'cutout_x': tf.random.uniform([bs], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "            'cutout_y': tf.random.uniform([bs], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "        }\n",
    "\n",
    "    # ============================================================\n",
    "    # 1. Train Critic\n",
    "    # ============================================================\n",
    "    # Initialize variables to ensure they exist after the loop\n",
    "    c_loss = 0.0\n",
    "    c_loss_w = 0.0\n",
    "    c_loss_mismatch = 0.0\n",
    "    gp = 0.0\n",
    "    \n",
    "    for _ in range(hparas['N_CRITIC']):\n",
    "        noise = tf.random.normal([batch_size, hparas['Z_DIM']])\n",
    "        aug_params = get_aug_params(batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # --- Text Embeddings ---\n",
    "            text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "            text_embed = tf.math.l2_normalize(text_embed, axis=1)\n",
    "            text_embed = tf.stop_gradient(text_embed)\n",
    "            \n",
    "            wrong_text_embed = tf.roll(text_embed, shift=1, axis=0)\n",
    "            \n",
    "            # --- Generate Fake (RAW) ---\n",
    "            _, fake_image = generator(text_embed, noise, training=True)\n",
    "            \n",
    "            # --- Augment ---\n",
    "            if hparas['USE_DIFFAUG']:\n",
    "                real_image_aug = DiffAugment(real_image, policy=hparas['DIFFAUG_POLICY'], params=aug_params)\n",
    "                fake_image_aug = DiffAugment(fake_image, policy=hparas['DIFFAUG_POLICY'], params=aug_params)\n",
    "            else:\n",
    "                real_image_aug = real_image\n",
    "                fake_image_aug = fake_image\n",
    "            \n",
    "            # --- Scores ---\n",
    "            real_scores = critic(real_image_aug, text_embed, training=True)\n",
    "            fake_scores = critic(fake_image_aug, text_embed, training=True)\n",
    "            wrong_scores = critic(real_image_aug, wrong_text_embed, training=True)\n",
    "            \n",
    "            # --- Losses ---\n",
    "            c_loss_w = wasserstein_loss_critic(real_scores, fake_scores)\n",
    "            c_loss_mismatch = mismatch_loss_critic(real_scores, wrong_scores)\n",
    "            \n",
    "            # --- Gradient Penalty (Pass RAW images) ---\n",
    "            gp = gradient_penalty(\n",
    "                critic, \n",
    "                real_image, \n",
    "                fake_image, \n",
    "                text_embed, \n",
    "                batch_size,\n",
    "                diffaug_policy=hparas['DIFFAUG_POLICY'] if hparas['USE_DIFFAUG'] else None,\n",
    "                aug_params=aug_params\n",
    "            )\n",
    "            \n",
    "            c_loss = c_loss_w + hparas['LAMBDA_GP'] * gp + hparas['LAMBDA_MISMATCH'] * c_loss_mismatch\n",
    "        \n",
    "        # Apply Gradients\n",
    "        grad_c = critic_tape.gradient(c_loss, critic.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(grad_c, critic.trainable_variables))\n",
    "    \n",
    "    # ============================================================\n",
    "    # 2. Train Generator\n",
    "    # ============================================================\n",
    "    noise = tf.random.normal([batch_size, hparas['Z_DIM']])\n",
    "    gen_aug_params = get_aug_params(batch_size)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "        text_embed = tf.math.l2_normalize(text_embed, axis=1)\n",
    "        text_embed = tf.stop_gradient(text_embed)\n",
    "        \n",
    "        _, fake_image = generator(text_embed, noise, training=True)\n",
    "        \n",
    "        if hparas['USE_DIFFAUG']:\n",
    "            fake_image_aug = DiffAugment(fake_image, policy=hparas['DIFFAUG_POLICY'], params=gen_aug_params)\n",
    "        else:\n",
    "            fake_image_aug = fake_image\n",
    "            \n",
    "        fake_scores = critic(fake_image_aug, text_embed, training=True)\n",
    "        g_loss = wasserstein_loss_generator(fake_scores)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    \n",
    "    # ============================================================\n",
    "    # 3. Calculate Metrics\n",
    "    # ============================================================\n",
    "    wasserstein_dist = calculate_wasserstein_distance(real_scores, fake_scores)\n",
    "    \n",
    "    # Calculate Gradient Norms for monitoring stability\n",
    "    # Filter out None gradients (frozen layers)\n",
    "    valid_grads_c = [g for g in grad_c if g is not None]\n",
    "    valid_grads_g = [g for g in grad_g if g is not None]\n",
    "    \n",
    "    grad_norm_c = tf.linalg.global_norm(valid_grads_c)\n",
    "    grad_norm_g = tf.linalg.global_norm(valid_grads_g)\n",
    "    \n",
    "    # Return EVERYTHING needed for the train loop logging\n",
    "    return {\n",
    "        'g_loss': g_loss,\n",
    "        'c_loss': c_loss,\n",
    "        'c_loss_wasserstein': c_loss_w,\n",
    "        'c_loss_mismatch': c_loss_mismatch,\n",
    "        'gp': gp,\n",
    "        'wasserstein_dist': wasserstein_dist,\n",
    "        'grad_norm_g': grad_norm_g,\n",
    "        'grad_norm_c': grad_norm_c,\n",
    "        'lr_g': generator_optimizer.learning_rate,\n",
    "        'lr_c': critic_optimizer.learning_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(input_ids, attention_mask, noise):\n",
    "\t\t# Encode text with DistilBERT (no hidden state)\n",
    "\t\ttext_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "\t\t_, fake_image = generator(text_embed, noise, training=False)\n",
    "\t\treturn fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "\t\th, w = images.shape[1], images.shape[2]\n",
    "\t\timg = np.zeros((h * size[0], w * size[1], 3))\n",
    "\t\tfor idx, image in enumerate(images):\n",
    "\t\t\t\ti = idx % size[1]\n",
    "\t\t\t\tj = idx // size[1]\n",
    "\t\t\t\timg[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\t\treturn img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "\t\t# getting the pixel values between [0, 1] to save it\n",
    "\t\treturn plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "\t\treturn imsave(images, size, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for visualization during training\n",
    "# IMPORTANT: All three variables must have the same batch size!\n",
    "\n",
    "sample_size = BATCH_SIZE  # Current: 32\n",
    "ni = int(np.ceil(np.sqrt(sample_size)))  # Grid size for visualization\n",
    "\n",
    "# Create random noise seed\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "# Define 8 diverse sample sentences\n",
    "base_sentences = [\n",
    "\t\t\"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "\t\t\"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "\t\t\"the petals on this flower are white with a yellow center\",\n",
    "\t\t\"this flower has a lot of small round pink petals.\",\n",
    "\t\t\"this flower is orange in color, and has petals that are ruffled and rounded.\",\n",
    "\t\t\"the flower has yellow petals and the center of it is brown.\",\n",
    "\t\t\"this flower has petals that are blue and white.\",\n",
    "\t\t\"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "]\n",
    "\n",
    "# Repeat sentences to match sample_size (batch size)\n",
    "sample_sentences = []\n",
    "for i in range(sample_size):\n",
    "\t\tsample_sentences.append(base_sentences[i % len(base_sentences)])\n",
    "\n",
    "# Tokenize with CLIP\n",
    "sample_encoded = preprocess_text_clip(sample_sentences, max_length=77)\n",
    "sample_input_ids = sample_encoded['input_ids']\n",
    "sample_attention_mask = sample_encoded['attention_mask']\n",
    "\n",
    "# Verify all dimensions match!\n",
    "print(f\"✓ Sample data created:\")\n",
    "print(f\"  Batch size: {sample_size}\")\n",
    "print(f\"  Grid size (ni): {ni} × {ni} = {ni*ni}\")\n",
    "print(f\"  Sample sentences: {len(sample_sentences)} sentences\")\n",
    "print(f\"  sample_seed shape: {sample_seed.shape}\")\n",
    "print(f\"  sample_input_ids shape: {sample_input_ids.shape}\")\n",
    "print(f\"  sample_attention_mask shape: {sample_attention_mask.shape}\")\n",
    "\n",
    "# Check for dimension mismatches\n",
    "assert len(sample_sentences) == sample_size, f\"Mismatch: {len(sample_sentences)} != {sample_size}\"\n",
    "assert sample_seed.shape[0] == sample_size, f\"Mismatch: {sample_seed.shape[0]} != {sample_size}\"\n",
    "assert sample_input_ids.shape[0] == sample_size, f\"Mismatch: {sample_input_ids.shape[0]} != {sample_size}\"\n",
    "assert sample_attention_mask.shape[0] == sample_size, f\"Mismatch: {sample_attention_mask.shape[0]} != {sample_size}\"\n",
    "print(\"✓ All dimensions match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for managing training runs\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def list_available_runs():\n",
    "\t\t\"\"\"List all available training runs with their details\"\"\"\n",
    "\t\trun_dirs = sorted(glob.glob('runs/*/'))\n",
    "\t\t\n",
    "\t\tif not run_dirs:\n",
    "\t\t\t\tprint('No training runs found in runs/ directory')\n",
    "\t\t\t\treturn []\n",
    "\t\t\n",
    "\t\tprint('=' * 80)\n",
    "\t\tprint('Available Training Runs:')\n",
    "\t\tprint('=' * 80)\n",
    "\t\t\n",
    "\t\tavailable_runs = []\n",
    "\t\tfor run_dir in run_dirs:\n",
    "\t\t\t\ttimestamp = run_dir.split('/')[-2]\n",
    "\t\t\t\tavailable_runs.append(timestamp)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Check for checkpoints\n",
    "\t\t\t\tcheckpoint_dir = f'{run_dir}checkpoints'\n",
    "\t\t\t\tlatest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\t\t\t\thas_checkpoint = '✓' if latest_ckpt else '✗'\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Check for config\n",
    "\t\t\t\tconfig_path = f'{run_dir}config.json'\n",
    "\t\t\t\thas_config = '✓' if os.path.exists(config_path) else '✗'\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Count sample images\n",
    "\t\t\t\tsample_count = len(glob.glob(f'{run_dir}samples/*.jpg'))\n",
    "\t\t\t\t\n",
    "\t\t\t\tprint(f'{timestamp}  |  Checkpoint: {has_checkpoint}  |  Config: {has_config}  |  Samples: {sample_count}')\n",
    "\t\t\t\t\n",
    "\t\t\t\tif latest_ckpt:\n",
    "\t\t\t\t\t\tprint(f'  └─ Latest checkpoint: {latest_ckpt}')\n",
    "\t\t\n",
    "\t\tprint('=' * 80)\n",
    "\t\treturn available_runs\n",
    "\n",
    "def load_run_config(run_timestamp):\n",
    "\t\t\"\"\"Load configuration from a previous run\"\"\"\n",
    "\t\tconfig_path = f'runs/{run_timestamp}/config.json'\n",
    "\t\t\n",
    "\t\tif not os.path.exists(config_path):\n",
    "\t\t\t\traise FileNotFoundError(f'Config not found: {config_path}')\n",
    "\t\t\n",
    "\t\twith open(config_path, 'r') as f:\n",
    "\t\t\t\tconfig = json.load(f)\n",
    "\t\t\n",
    "\t\tprint(f'✓ Loaded config from: {config_path}')\n",
    "\t\treturn config\n",
    "\n",
    "# List available runs\n",
    "print('Checking for existing training runs...')\n",
    "list_available_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# RESUME TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "# Set to None for new run, or specify run timestamp to resume\n",
    "# Example: RESUME_RUN = '20251116-225453'\n",
    "RESUME_RUN = None # ← Change this to resume from specific run\n",
    "\n",
    "# ============================================================\n",
    "# RUN DIRECTORY SETUP\n",
    "# ============================================================\n",
    "if RESUME_RUN:\n",
    "\t\t# Resume from existing run\n",
    "\t\trun_dir = f'runs/{RESUME_RUN}'\n",
    "\t\t\n",
    "\t\t# Verify directory exists\n",
    "\t\tif not os.path.exists(run_dir):\n",
    "\t\t\t\traise FileNotFoundError(f'Run directory not found: {run_dir}')\n",
    "\t\t\n",
    "\t\t# Load existing config\n",
    "\t\ttry:\n",
    "\t\t\t\tprev_config = load_run_config(RESUME_RUN)\n",
    "\t\t\t\trun_timestamp = prev_config.get('run_timestamp', RESUME_RUN)\n",
    "\t\t\t\tprint(f'\\n⟳ RESUMING training from: {run_dir}')\n",
    "\t\t\t\tprint(f'  Original start: {run_timestamp}')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Warn if hyperparameters might be different\n",
    "\t\t\t\tif 'hyperparameters' in prev_config:\n",
    "\t\t\t\t\t\tprev_hparas = prev_config['hyperparameters']\n",
    "\t\t\t\t\t\tif prev_hparas.get('BATCH_SIZE') != BATCH_SIZE:\n",
    "\t\t\t\t\t\t\t\tprint(f'  ⚠ WARNING: Batch size changed ({prev_hparas.get(\"BATCH_SIZE\")} → {hparas[\"BATCH_SIZE\"]})')\n",
    "\t\t\t\t\t\tif prev_hparas.get('LR') != hparas['LR']:\n",
    "\t\t\t\t\t\t\t\tprint(f'  ⚠ WARNING: Learning rate changed ({prev_hparas.get(\"LR\")} → {hparas[\"LR\"]})')\n",
    "\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f'⚠ Could not load previous config: {e}')\n",
    "\t\t\t\trun_timestamp = RESUME_RUN\n",
    "\t\t\n",
    "\t\t# Use existing subdirectories\n",
    "\t\tcheckpoint_dir = f'{run_dir}/checkpoints'\n",
    "\t\tbest_models_dir = f'{run_dir}/best_models'\n",
    "\t\tsamples_dir = f'{run_dir}/samples'\n",
    "\t\tinference_dir = f'{run_dir}/inference'\n",
    "\t\t\n",
    "\t\t# Create directories if they don't exist (shouldn't happen, but safety check)\n",
    "\t\tos.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\t\tos.makedirs(best_models_dir, exist_ok=True)\n",
    "\t\tos.makedirs(samples_dir, exist_ok=True)\n",
    "\t\tos.makedirs(inference_dir, exist_ok=True)\n",
    "\t\t\n",
    "else:\n",
    "\t\t# Create new run\n",
    "\t\trun_timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\t\trun_dir = f'runs/{run_timestamp}'\n",
    "\t\t\n",
    "\t\t# All outputs for this run go in subdirectories\n",
    "\t\tcheckpoint_dir = f'{run_dir}/checkpoints'\n",
    "\t\tbest_models_dir = f'{run_dir}/best_models'\n",
    "\t\tsamples_dir = f'{run_dir}/samples'\n",
    "\t\tinference_dir = f'{run_dir}/inference'\n",
    "\t\t\n",
    "\t\t# Create all directories\n",
    "\t\tos.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\t\tos.makedirs(best_models_dir, exist_ok=True)\n",
    "\t\tos.makedirs(samples_dir, exist_ok=True)\n",
    "\t\tos.makedirs(inference_dir, exist_ok=True)\n",
    "\t\t\n",
    "\t\tprint(f'✓ Created NEW run directory: {run_dir}')\n",
    "\t\t\n",
    "\t\t# Save hyperparameters\n",
    "\t\tconfig_to_save = {\n",
    "\t\t\t\t'run_timestamp': run_timestamp,\n",
    "\t\t\t\t'hyperparameters': hparas,\n",
    "\t\t}\n",
    "\t\twith open(f'{run_dir}/config.json', 'w') as f:\n",
    "\t\t\t\tjson.dump(config_to_save, f, indent=4)\n",
    "\t\tprint(f'✓ Saved configuration to: {run_dir}/config.json')\n",
    "\n",
    "\n",
    "# Display directory structure\n",
    "print(f'\\nRun directory structure:')\n",
    "print(f'  {run_dir}/')\n",
    "print(f'  ├── checkpoints/  : {checkpoint_dir}')\n",
    "print(f'  ├── best_models/  : {best_models_dir}')\n",
    "print(f'  ├── samples/      : {samples_dir}')\n",
    "print(f'  └── inference/    : {inference_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESTORE CHECKPOINT FOR RESUMING TRAINING\n",
    "# ============================================================\n",
    "if RESUME_RUN:\n",
    "\t\t# When resuming, restore the LATEST regular checkpoint (not best model)\n",
    "\t\t# This ensures training continues from where it left off\n",
    "\t\tprint(f'\\nRestoring checkpoint for resuming training...')\n",
    "\t\tprint(f'Looking for latest checkpoint in: {checkpoint_dir}')\n",
    "\t\t\n",
    "\t\tlatest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\t\tif latest_checkpoint:\n",
    "\t\t\t\tcheckpoint.restore(latest_checkpoint).expect_partial()\n",
    "\t\t\t\tckpt_num = latest_checkpoint.split('-')[-1]\n",
    "\t\t\t\tprint(f'✓ Restored latest checkpoint: {latest_checkpoint}')\n",
    "\t\t\t\tprint(f'  Checkpoint number: {ckpt_num}')\n",
    "\t\t\t\tprint(f'  Training will continue from this point')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Also check if best model exists\n",
    "\t\t\t\tbest_checkpoint = tf.train.latest_checkpoint(best_models_dir)\n",
    "\t\t\t\tif best_checkpoint:\n",
    "\t\t\t\t\t\tprint(f'\\n✓ Best model also available at: {best_checkpoint}')\n",
    "\t\telse:\n",
    "\t\t\t\tprint('⚠ No checkpoint found in the run directory')\n",
    "\t\t\t\tprint('  Training will start from epoch 1 (this is unusual for RESUME mode)')\n",
    "else:\n",
    "\t\tprint('\\n✓ Starting NEW training run - no checkpoint restoration needed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "def train(dataset, epochs):\n",
    "\t\tglobal run_dir, checkpoint_dir, best_models_dir, samples_dir, inference_dir\n",
    "\t\t\n",
    "\t\tcheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\t\tbest_model_prefix = os.path.join(best_models_dir, \"best_ckpt\")\n",
    "\t\t\n",
    "\t\tlog_dir = f'{run_dir}/logs'\n",
    "\t\tos.makedirs(log_dir, exist_ok=True)\n",
    "\t\tsummary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\t\n",
    "\t\tprint(f\"Run tensorboard --logdir {log_dir}\")\n",
    "\t\tprint(f'Model: WGAN-GP with {hparas[\"N_CRITIC\"]} critic iterations')\n",
    "\t\tprint(f'DiffAugment: {hparas[\"USE_DIFFAUG\"]} ({hparas.get(\"DIFFAUG_POLICY\", \"N/A\")})')\n",
    "\t\t\n",
    "\t\tsteps_per_epoch = int(hparas['N_SAMPLE']/BATCH_SIZE)\n",
    "\t\tglobal_step = 0\n",
    "\t\t\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\t\t# Using sys.executable ensures we use tensorboard from the correct python env\n",
    "\t\t\t\ttensorboard_process = subprocess.Popen([\n",
    "\t\t\t\t\t\tsys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir\n",
    "\t\t\t\t])\n",
    "\t\t\t\tprint(f\"✓ TensorBoard launched as a background process (PID: {tensorboard_process.pid}).\")\n",
    "\t\t\t\tprint(\"  It might take a few seconds to become available in your browser.\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"⚠ Could not start TensorBoard automatically: {e}\")\n",
    "\t\t\t\tprint(f\"  You can start it manually by running: tensorboard --logdir {log_dir}\")\n",
    "\n",
    "\t\t# ========== EARLY STOPPING SETUP ==========\n",
    "\t\tbest_wasserstein_dist = float('inf')\n",
    "\t\tpatience = 300\n",
    "\t\tpatience_counter = 0\n",
    "\t\t# ==========================================\n",
    "\t\t\n",
    "\t\tfor epoch in range(hparas['N_EPOCH']):\n",
    "\t\t\t\t# ========== LEARNING RATE DECAY ==========\n",
    "\t\t\t\tif epoch >= hparas['LR_DECAY_START'] and epoch % hparas['LR_DECAY_EVERY'] == 0:\n",
    "\t\t\t\t\t\tcurrent_lr_g = generator_optimizer.learning_rate.numpy()\n",
    "\t\t\t\t\t\tcurrent_lr_c = critic_optimizer.learning_rate.numpy()\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tnew_lr_g = max(current_lr_g * hparas['LR_DECAY_FACTOR'], hparas['LR_MIN'])\n",
    "\t\t\t\t\t\tnew_lr_c = max(current_lr_c * hparas['LR_DECAY_FACTOR'], hparas['LR_MIN'])\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tgenerator_optimizer.learning_rate.assign(new_lr_g)\n",
    "\t\t\t\t\t\tcritic_optimizer.learning_rate.assign(new_lr_c)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tprint(f'  📉 LR Decay: G={new_lr_g:.2e}, C={new_lr_c:.2e}')\n",
    "\t\t\t\t# ==========================================\n",
    "\t\t\t\t\n",
    "\t\t\t\tg_total_loss = 0\n",
    "\t\t\t\tc_total_loss = 0\n",
    "\t\t\t\tc_total_loss_wasserstein = 0\n",
    "\t\t\t\tgp_total = 0\n",
    "\t\t\t\twd_total = 0\n",
    "\t\t\t\tstart = time.time()\n",
    "\t\t\t\t\n",
    "\t\t\t\tpbar = tqdm(dataset, desc=f'Epoch {epoch+1}/{hparas[\"N_EPOCH\"]}', \n",
    "\t\t\t\t\t\t\t\t\t total=steps_per_epoch, unit='batch')\n",
    "\t\t\t\t\n",
    "\t\t\t\tfor batch_idx, (image, input_ids, attention_mask) in enumerate(pbar):\n",
    "\t\t\t\t\t\tmetrics = train_step(image, input_ids, attention_mask)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Accumulate losses\n",
    "\t\t\t\t\t\tg_total_loss += metrics['g_loss']\n",
    "\t\t\t\t\t\tc_total_loss += metrics['c_loss']\n",
    "\t\t\t\t\t\tc_total_loss_wasserstein += metrics['c_loss_wasserstein']\n",
    "\t\t\t\t\t\tgp_total += metrics['gp']\n",
    "\t\t\t\t\t\twd_total += metrics['wasserstein_dist']\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Log to TensorBoard\n",
    "\t\t\t\t\t\twith summary_writer.as_default():\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/generator_loss', metrics['g_loss'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/critic_loss_total', metrics['c_loss'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/critic_loss_wasserstein', metrics['c_loss_wasserstein'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/critic_loss_mismatch', metrics['c_loss_mismatch'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Losses/gradient_penalty', metrics['gp'], step=global_step)\n",
    "\t\t\t\t\t\t\t\ttf.summary.scalar('Metrics/wasserstein_distance', metrics['wasserstein_dist'], step=global_step)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tif global_step % 50 == 0:\n",
    "\t\t\t\t\t\t\t\t\t\ttf.summary.scalar('Gradients/generator_gradient_norm', metrics['grad_norm_g'], step=global_step)\n",
    "\t\t\t\t\t\t\t\t\t\ttf.summary.scalar('Gradients/critic_gradient_norm', metrics['grad_norm_c'], step=global_step)\n",
    "\t\t\t\t\t\t\t\t\t\t# ========== LOG LEARNING RATES ==========\n",
    "\t\t\t\t\t\t\t\t\t\ttf.summary.scalar('Training/learning_rate_generator', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tgenerator_optimizer.learning_rate.numpy(), step=global_step)\n",
    "\t\t\t\t\t\t\t\t\t\ttf.summary.scalar('Training/learning_rate_critic', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcritic_optimizer.learning_rate.numpy(), step=global_step)\n",
    "\t\t\t\t\t\t\t\t\t\t# ========================================\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Update progress bar\n",
    "\t\t\t\t\t\tpbar.set_postfix({\n",
    "\t\t\t\t\t\t\t\t'G_loss': f'{metrics[\"g_loss\"]:.4f}',\n",
    "\t\t\t\t\t\t\t\t'C_loss': f'{metrics[\"c_loss\"]:.4f}',\n",
    "\t\t\t\t\t\t\t\t'W_dist': f'{metrics[\"wasserstein_dist\"]:.4f}'\n",
    "\t\t\t\t\t\t})\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tglobal_step += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\tpbar.close()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Print epoch summary\n",
    "\t\t\t\tavg_g_loss = g_total_loss / steps_per_epoch\n",
    "\t\t\t\tavg_c_loss = c_total_loss / steps_per_epoch\n",
    "\t\t\t\tavg_c_loss_w = c_total_loss_wasserstein / steps_per_epoch\n",
    "\t\t\t\tavg_gp = gp_total / steps_per_epoch\n",
    "\t\t\t\tavg_wd = wd_total / steps_per_epoch\n",
    "\t\t\t\tepoch_time = time.time() - start\n",
    "\t\t\t\t\n",
    "\t\t\t\tprint(f'Epoch {epoch+1}: G_loss={avg_g_loss:.4f}, C_loss={avg_c_loss:.4f} ' +\n",
    "\t\t\t\t\t\t\tf'(W={avg_c_loss_w:.4f}, GP={avg_gp:.4f}), W_dist={avg_wd:.4f}, Time={epoch_time:.2f}s')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Log epoch averages\n",
    "\t\t\t\twith summary_writer.as_default():\n",
    "\t\t\t\t\t\ttf.summary.scalar('Epoch/generator_loss_avg', avg_g_loss, step=epoch)\n",
    "\t\t\t\t\t\ttf.summary.scalar('Epoch/critic_loss_avg', avg_c_loss, step=epoch)\n",
    "\t\t\t\t\t\ttf.summary.scalar('Epoch/wasserstein_distance_avg', avg_wd, step=epoch)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# ========== EARLY STOPPING CHECK ==========\n",
    "\t\t\t\tif avg_wd < best_wasserstein_dist:  # Avoid near-zero\n",
    "\t\t\t\t\t\tbest_wasserstein_dist = avg_wd\n",
    "\t\t\t\t\t\tpatience_counter = 0\n",
    "\t\t\t\t\t\t# Save best model in separate directory\n",
    "\t\t\t\t\t\tbest_path = checkpoint.save(file_prefix=best_model_prefix)\n",
    "\t\t\t\t\t\tprint(f'  ⭐ Best model saved! W_dist={avg_wd:.4f} → {best_path}')\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\tpatience_counter += 1\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tif patience_counter >= patience:\n",
    "\t\t\t\t\t\tprint(f'\\n⚠️ Early stopping triggered at epoch {epoch+1}')\n",
    "\t\t\t\t\t\tprint(f'   Best Wasserstein distance: {best_wasserstein_dist:.4f}')\n",
    "\t\t\t\t\t\tprint(f'   No improvement for {patience} epochs')\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t# ==========================================\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Save checkpoint (more frequently now)\n",
    "\t\t\t\tif (epoch + 1) % 5 == 0:  # Changed from 50\n",
    "\t\t\t\t\t\tsaved_path = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\t\t\t\t\t\tprint(f'  ✓ Checkpoint saved: {saved_path}')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Visualization\n",
    "\t\t\t\tif (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "\t\t\t\t\t\tfake_image = test_step(sample_input_ids, sample_attention_mask, sample_seed)\n",
    "\t\t\t\t\t\tsave_images(fake_image, [ni, ni], f'{samples_dir}/train_{epoch+1:03d}.jpg')\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\twith summary_writer.as_default():\n",
    "\t\t\t\t\t\t\t\tdisplay_images = (fake_image + 1.0) / 2.0\n",
    "\t\t\t\t\t\t\t\ttf.summary.image('Generated_Samples', display_images, step=epoch, max_outputs=16)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tprint(f'  ✓ Sample image saved and logged to TensorBoard')\n",
    "\t\t\n",
    "\t\tprint('\\n✓ Training completed!')\n",
    "\t\tprint(f'All outputs saved to: {run_dir}')\n",
    "\t\tprint(f'Best Wasserstein distance achieved: {best_wasserstein_dist:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataset, hparas['N_EPOCH'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Evaluation</center></h1>\n",
    "\n",
    "<p><code>dataset/testData.pkl</code> is a pandas dataframe containing testing text with attributes 'ID' and 'Captions'.</p>\n",
    "\n",
    "<ul>\n",
    "<li>'ID': text ID used to name generated image.</li>\n",
    "<li>'Captions': text used as condition to generate image.</li>\n",
    "</ul>\n",
    "\n",
    "<p>For each captions, you need to generate <strong>inference_ID.png</strong> to evaluate quality of generated image. You must name the generated image in this format, otherwise we cannot evaluate your images.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing data generator using CLIP tokenization\n",
    "\t\t\n",
    "\t\tArgs:\n",
    "\t\t\t\tcaption_text: Raw text string\n",
    "\t\t\t\tindex: Test sample ID\n",
    "\t\t\n",
    "\t\tReturns:\n",
    "\t\t\t\tinput_ids, attention_mask, index\n",
    "\t\t\"\"\"\n",
    "\t\tdef tokenize_caption_clip(text):\n",
    "\t\t\t\t\"\"\"Python function to tokenize text using CLIP tokenizer\"\"\"\n",
    "\t\t\t\t# Convert EagerTensor to bytes, then decode to string\n",
    "\t\t\t\ttext = text.numpy().decode('utf-8')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Tokenize using CLIP\n",
    "\t\t\t\tencoded = tokenizer(\n",
    "\t\t\t\t\t\ttext,\n",
    "\t\t\t\t\t\tpadding='max_length',\n",
    "\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\tmax_length=77,\n",
    "\t\t\t\t\t\treturn_tensors='np'\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\t\t\n",
    "\t\t# Use tf.py_function to call Python tokenizer\n",
    "\t\tinput_ids, attention_mask = tf.py_function(\n",
    "\t\t\t\tfunc=tokenize_caption_clip,\n",
    "\t\t\t\tinp=[caption_text],\n",
    "\t\t\t\tTout=[tf.int32, tf.int32]\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Set shapes explicitly\n",
    "\t\tinput_ids.set_shape([77])\n",
    "\t\tattention_mask.set_shape([77])\n",
    "\t\t\n",
    "\t\treturn input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated testing dataset generator - decodes IDs to raw text\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_pickle('./dataset/testData.pkl')\n",
    "\t\tcaptions_ids = data['Captions'].values\n",
    "\t\tcaption_texts = []\n",
    "\t\t\n",
    "\t\t# Decode pre-tokenized IDs back to text\n",
    "\t\tfor i in range(len(captions_ids)):\n",
    "\t\t\t\tchosen_caption_ids = captions_ids[i]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode IDs back to text using id2word_dict\n",
    "\t\t\t\twords = []\n",
    "\t\t\t\tfor word_id in chosen_caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':  # Skip padding tokens\n",
    "\t\t\t\t\t\t\t\twords.append(word)\n",
    "\t\t\t\t\n",
    "\t\t\t\tcaption_text = ' '.join(words)\n",
    "\t\t\t\tcaption_texts.append(caption_text)\n",
    "\t\t\n",
    "\t\tindex = data['ID'].values\n",
    "\t\tindex = np.asarray(index)\n",
    "\t\t\n",
    "\t\t# Create dataset from raw text\n",
    "\t\tdataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "\t\tdataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\t\tdataset = dataset.repeat().batch(batch_size)\n",
    "\t\t\n",
    "\t\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(BATCH_SIZE, testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference directory is already created by the train() function\n",
    "# No need to create it again here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore BEST MODEL for inference\n",
    "print(f'Looking for BEST model in: {best_models_dir}')\n",
    "\n",
    "best_checkpoint = tf.train.latest_checkpoint(best_models_dir)\n",
    "if best_checkpoint:\n",
    "\t\tcheckpoint.restore(best_checkpoint)\n",
    "\t\tprint(f'✓ Restored BEST model: {best_checkpoint}')\n",
    "\t\tprint(f'  This is the model with the lowest Wasserstein distance during training')\n",
    "else:\n",
    "\t\tprint('⚠ No best model found, trying regular checkpoints...')\n",
    "\t\tlatest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\t\tif latest_checkpoint:\n",
    "\t\t\t\tcheckpoint.restore(latest_checkpoint)\n",
    "\t\t\t\tprint(f'✓ Restored latest checkpoint: {latest_checkpoint}')\n",
    "\t\t\t\tprint('  ⚠ WARNING: Using latest checkpoint, not best model')\n",
    "\t\telse:\n",
    "\t\t\t\tprint('⚠ No checkpoint found at all, using fresh/untrained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdated inference function for CLIP\n",
    "\t\tFIXED: Generate fresh random noise for each batch!\n",
    "\t\t\"\"\"\n",
    "\t\tsample_size = BATCH_SIZE\n",
    "\t\t\n",
    "\t\tstep = 0\n",
    "\t\tstart = time.time()\n",
    "\t\ttotal_images = 0\n",
    "\t\t\n",
    "\t\t# Progress bar for inference\n",
    "\t\tpbar = tqdm(total=NUM_TEST, desc='Generating images', unit='img')\n",
    "\t\t\n",
    "\t\t# Unpack 3 values: input_ids, attention_mask, idx\n",
    "\t\tfor input_ids, attention_mask, idx in dataset:\n",
    "\t\t\t\tif step > EPOCH_TEST:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\t# CRITICAL FIX: Generate FRESH random noise for each batch\n",
    "\t\t\t\t# This ensures diversity across all 819 test images\n",
    "\t\t\t\tsample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "\t\t\t\t\n",
    "\t\t\t\tfake_image = test_step(input_ids, attention_mask, sample_seed)\n",
    "\t\t\t\tstep += 1\n",
    "\t\t\t\t\n",
    "\t\t\t\tfor i in range(BATCH_SIZE):\n",
    "\t\t\t\t\t\tplt.imsave(f'{inference_dir}/inference_{idx[i]:04d}.jpg', fake_image[i].numpy()*0.5 + 0.5)\n",
    "\t\t\t\t\t\ttotal_images += 1\n",
    "\t\t\t\t\t\tpbar.update(1)\n",
    "\t\t\n",
    "\t\tpbar.close()\n",
    "\t\tprint(f'\\n✓ Generated {total_images} images in {time.time()-start:.4f} sec')\n",
    "\t\tprint(f'✓ Images saved to: {inference_dir}')\n",
    "\t\tprint(f'✓ Each image generated with unique random noise for better diversity!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "\t\tprint(f'⚠ No images found in {inference_dir}')\n",
    "\t\tprint('Please run the inference cell first!')\n",
    "else:\n",
    "\t\t# Randomly sample 20 images\n",
    "\t\tnp.random.seed(42)  # For reproducibility\n",
    "\t\tnum_samples = min(20, len(image_files))\n",
    "\t\tsample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "\t\tsample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "\t\t# Create 4x5 grid\n",
    "\t\tfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\t\taxes = axes.flatten()\n",
    "\n",
    "\t\tfor idx, img_path in enumerate(sample_files):\n",
    "\t\t\t\t# Extract image ID from filename\n",
    "\t\t\t\timg_id = int(Path(img_path).stem.split('_')[1])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Find caption\n",
    "\t\t\t\tcaption_idx = np.where(test_ids == img_id)[0][0]\n",
    "\t\t\t\tcaption_ids = test_captions[caption_idx]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Decode caption\n",
    "\t\t\t\tcaption_text = ''\n",
    "\t\t\t\tfor word_id in caption_ids:\n",
    "\t\t\t\t\t\tword = id2word_dict[str(word_id)]\n",
    "\t\t\t\t\t\tif word != '<PAD>':\n",
    "\t\t\t\t\t\t\t\tcaption_text += word + ' '\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Load and display image\n",
    "\t\t\t\timg = plt.imread(img_path)\n",
    "\t\t\t\taxes[idx].imshow(img)\n",
    "\t\t\t\taxes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\t# Hide unused subplots if less than 20 images\n",
    "\t\tfor idx in range(num_samples, 20):\n",
    "\t\t\t\taxes[idx].axis('off')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\tprint(f'\\nTotal generated images: {len(image_files)}')\n",
    "\t\tprint(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
