{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Text to Image</center></h1>\n",
    "\n",
    "<h2 id=\"Platform:-Kaggle\">Platform: <a href=\"https://www.kaggle.com/competitions/2025-datalab-cup-3-reverse-image-caption/overview\">Kaggle</a><a class=\"anchor-link\" href=\"#Platform:-Kaggle\">¶</a></h2>\n",
    "<h2 id=\"Overview\">Overview<a class=\"anchor-link\" href=\"#Overview\">¶</a></h2>\n",
    "<p>In this work, we are interested in translating text in the form of single-sentence human-written descriptions directly into image pixels. For example, \"<strong>this flower has petals that are yellow and has a ruffled stamen</strong>\" and \"<strong>this pink and yellow flower has a beautiful yellow center with many stamens</strong>\". You have to develop a novel deep architecture and GAN formulation to effectively translate visual concepts from characters to pixels.</p>\n",
    "\n",
    "<p>More specifically, given a set of texts, your task is to generate reasonable images with size 64x64x3 to illustrate the corresponding texts. Here we use <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford-102 flower dataset</a> and its <a href=\"https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view\">paired texts</a> as our training dataset.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/example.png\"/>\n",
    "\n",
    "<ul>\n",
    "<li>7370 images as training set, where each images is annotated with at most 10 texts.</li>\n",
    "<li>819 texts for testing. You must generate 1 64x64x3 image for each text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN\">Conditional GAN<a class=\"anchor-link\" href=\"#Conditional-GAN\">¶</a></h2>\n",
    "<p>Given a text, in order to generate the image which can illustrate it, our model must meet several requirements:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Our model should have ability to understand and extract the meaning of given texts.<ul>\n",
    "<li>Use RNN or other language model, such as BERT, ELMo or XLNet, to capture the meaning of text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Our model should be able to generate image.<ul>\n",
    "<li>Use GAN to generate high quality image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>GAN-generated image should illustrate the text.<ul>\n",
    "<li>Use conditional-GAN to generate image conditioned on given text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information $y$. We can perform the conditioning by feeding $y$ into both the discriminator and generator as additional input layer.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/cGAN.png\" width=\"500\"/>\n",
    "\n",
    "<p>There are two motivations for using some extra information in a GAN model:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Improve GAN.</li>\n",
    "<li>Generate targeted image.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Additional information that is correlated with the input images, such as class labels, can be used to improve the GAN. This improvement may come in the form of more stable training, faster training, and/or generated images that have better quality.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/GANCLS.jpg\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell previously contained sent2IdList() function\n",
    "# It has been removed as we now use DistilBERT tokenizer instead\n",
    "# The id2word_dict is still available from cell 6 for visualization purposes\n",
    "\n",
    "print(\"✓ Using DistilBERT tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Configuration\n",
    "# Define this BEFORE training_data_generator to avoid reference issues\n",
    "aug_config = {\n",
    "    'enabled': False,                      # Master switch for augmentation\n",
    "    'random_flip_horizontal': True,       # Flowers can be mirrored\n",
    "    'random_flip_vertical': False,        # Flowers typically grow upward\n",
    "    'random_rotation': True,              # Any rotation is valid for flowers\n",
    "    'random_brightness': 0.15,            # Lighting variations (max delta)\n",
    "    'random_contrast': (0.9, 1.1),        # Subtle contrast changes (lower, upper)\n",
    "    'random_saturation': (0.9, 1.1),      # Color intensity (lower, upper)\n",
    "    'random_hue': 0.05,                   # Small color shifts (max delta)\n",
    "}\n",
    "\n",
    "print('Data Augmentation:', 'ENABLED' if aug_config['enabled'] else 'DISABLED')\n",
    "if aug_config['enabled']:\n",
    "    enabled_augs = [k for k, v in aug_config.items() if k != 'enabled' and v]\n",
    "    print(f'Active augmentations: {\", \".join(enabled_augs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def preprocess_text_distilbert(text, max_length=64):\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiffAugment(x, policy='color,translation,cutout', channels_first=False, params=None):\n",
    "    \"\"\"\n",
    "    Differentiable augmentation for GANs\n",
    "    \n",
    "    Args:\n",
    "        x: Input images [batch, H, W, C] \n",
    "        policy: Comma-separated augmentation policies\n",
    "        channels_first: If True, expects [batch, C, H, W]\n",
    "        params: Optional dict of pre-generated augmentation parameters for consistency\n",
    "    \n",
    "    Returns:\n",
    "        Augmented images\n",
    "    \"\"\"\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            # TensorFlow format: [batch, H, W, C]\n",
    "            for p in policy.split(','):\n",
    "                for f in AUGMENT_FNS[p]:\n",
    "                    x = f(x, params)  # ← Pass params!\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x, params=None):\n",
    "    \"\"\"Random brightness adjustment\"\"\"\n",
    "    if params is not None and 'brightness' in params:\n",
    "        magnitude = params['brightness']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], -0.5, 0.5)\n",
    "    x = x + magnitude\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x, params=None):\n",
    "    \"\"\"Random saturation adjustment\"\"\"\n",
    "    if params is not None and 'saturation' in params:\n",
    "        magnitude = params['saturation']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], 0.0, 2.0)\n",
    "    x_mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x, params=None):\n",
    "    \"\"\"Random contrast adjustment\"\"\"\n",
    "    if params is not None and 'contrast' in params:\n",
    "        magnitude = params['contrast']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], 0.5, 1.5)\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return x\n",
    "\n",
    "def rand_translation(x, params=None, ratio=0.125):\n",
    "    \"\"\"Random translation (shift) - Fully vectorized for @tf.function\"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    image_size = tf.shape(x)[1]\n",
    "    shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "    \n",
    "    # Random translation amounts for entire batch\n",
    "    if params is not None and 'translation_x' in params:\n",
    "        translation_x = params['translation_x']\n",
    "        translation_y = params['translation_y']\n",
    "    else:\n",
    "        translation_x = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "        translation_y = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "    \n",
    "    def translate_single_image(args):\n",
    "        \"\"\"Translate a single image\"\"\"\n",
    "        img, tx, ty = args\n",
    "        img = tf.pad(img, [[shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "        img = tf.image.crop_to_bounding_box(img, shift + ty, shift + tx, image_size, image_size)\n",
    "        return img\n",
    "    \n",
    "    # Use tf.map_fn (graph-mode compatible)\n",
    "    x_translated = tf.map_fn(\n",
    "        translate_single_image,\n",
    "        (x, translation_x, translation_y),\n",
    "        fn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "        parallel_iterations=10\n",
    "    )\n",
    "    \n",
    "    return x_translated\n",
    "\n",
    "\n",
    "def rand_cutout(x, params=None, ratio=0.5):\n",
    "    \"\"\"\n",
    "    Random cutout - SIMPLIFIED vectorized version\n",
    "    \n",
    "    Instead of complex per-pixel masking, we create rectangular masks\n",
    "    using broadcasting and boolean operations\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    image_size = tf.shape(x)[1]\n",
    "    channels = tf.shape(x)[3]\n",
    "    \n",
    "    # Cutout size\n",
    "    cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "    \n",
    "    # Random offset for cutout location\n",
    "    if params is not None and 'cutout_x' in params:\n",
    "        offset_x = params['cutout_x']\n",
    "        offset_y = params['cutout_y']\n",
    "    else:\n",
    "        offset_x = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "        offset_y = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "    \n",
    "    def cutout_single_image(args):\n",
    "        \"\"\"Apply cutout to single image using simple slicing\"\"\"\n",
    "        img, ox, oy = args\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        height_range = tf.range(image_size)\n",
    "        width_range = tf.range(image_size)\n",
    "        \n",
    "        # Create 2D grids\n",
    "        yy, xx = tf.meshgrid(height_range, width_range, indexing='ij')\n",
    "        \n",
    "        # Create mask: True where we want to KEEP pixels\n",
    "        mask_y = tf.logical_or(yy < oy, yy >= oy + cutout_size)\n",
    "        mask_x = tf.logical_or(xx < ox, xx >= ox + cutout_size)\n",
    "        mask = tf.logical_or(mask_y, mask_x)\n",
    "        \n",
    "        # Expand mask to all channels\n",
    "        mask = tf.expand_dims(mask, axis=-1)  # [H, W, 1]\n",
    "        mask = tf.tile(mask, [1, 1, channels])  # [H, W, C]\n",
    "        \n",
    "        # Apply mask (convert bool to float)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return img * mask\n",
    "    \n",
    "    # Use tf.map_fn\n",
    "    x_cutout = tf.map_fn(\n",
    "        cutout_single_image,\n",
    "        (x, offset_x, offset_y),\n",
    "        fn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "        parallel_iterations=10\n",
    "    )\n",
    "    \n",
    "    return x_cutout\n",
    "\n",
    "\n",
    "# Augmentation function registry\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}\n",
    "\n",
    "\n",
    "print(\"✓ DiffAugment functions loaded\")\n",
    "print(\"  Policies available: color, translation, cutout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "    \"\"\"\n",
    "    Updated data generator using DistilBERT tokenization\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string (not IDs!)\n",
    "        image_path: Path to image file\n",
    "    \n",
    "    Returns:\n",
    "        img, input_ids, attention_mask\n",
    "    \"\"\"\n",
    "    # ============= IMAGE PROCESSING (same as before) =============\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    # Data augmentation (only applied during training)\n",
    "    if aug_config['enabled']:\n",
    "        if aug_config['random_flip_horizontal']:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "        \n",
    "        if aug_config['random_flip_vertical']:\n",
    "            img = tf.image.random_flip_up_down(img)\n",
    "        \n",
    "        if aug_config['random_rotation']:\n",
    "            img = tf.image.rot90(img, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "        \n",
    "        if aug_config['random_brightness']:\n",
    "            img = tf.image.random_brightness(img, aug_config['random_brightness'])\n",
    "        \n",
    "        if aug_config['random_contrast']:\n",
    "            img = tf.image.random_contrast(img, \n",
    "                                          aug_config['random_contrast'][0], \n",
    "                                          aug_config['random_contrast'][1])\n",
    "        \n",
    "        if aug_config['random_saturation']:\n",
    "            img = tf.image.random_saturation(img, \n",
    "                                            aug_config['random_saturation'][0], \n",
    "                                            aug_config['random_saturation'][1])\n",
    "        \n",
    "        if aug_config['random_hue']:\n",
    "            img = tf.image.random_hue(img, aug_config['random_hue'])\n",
    "        \n",
    "        img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "    \n",
    "    # Normalize to [-1, 1] to match generator's tanh output\n",
    "    img = (img * 2.0) - 1.0\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    # ============= TEXT PROCESSING (NEW: Use DistilBERT tokenizer) =============\n",
    "    def tokenize_caption(text):\n",
    "        \"\"\"Python function to tokenize text using DistilBERT tokenizer\"\"\"\n",
    "        # Convert EagerTensor to bytes, then decode to string\n",
    "        text = text.numpy().decode('utf-8')\n",
    "        \n",
    "        # Tokenize using DistilBERT\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors='np'  # Use numpy arrays for TF compatibility\n",
    "        )\n",
    "        \n",
    "        return encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "    \n",
    "    # Use tf.py_function to call Python tokenizer\n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize_caption,\n",
    "        inp=[caption_text],\n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    # Set shapes explicitly\n",
    "    input_ids.set_shape([64])\n",
    "    attention_mask.set_shape([64])\n",
    "    \n",
    "    return img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    \"\"\"\n",
    "    Updated dataset generator to work with raw text (decoded from IDs)\n",
    "    \"\"\"\n",
    "    # Load the training data\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions_ids = df['Captions'].values\n",
    "    caption_texts = []\n",
    "    \n",
    "    # Decode pre-tokenized IDs back to raw text\n",
    "    for i in range(len(captions_ids)):\n",
    "        # Randomly choose one caption (list of ID lists)\n",
    "        chosen_caption_ids = random.choice(captions_ids[i])\n",
    "        \n",
    "        # Decode IDs back to text using id2word_dict\n",
    "        words = []\n",
    "        for word_id in chosen_caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':  # Skip padding tokens\n",
    "                words.append(word)\n",
    "        \n",
    "        caption_text = ' '.join(words)\n",
    "        caption_texts.append(caption_text)\n",
    "    \n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    # Verify same length\n",
    "    assert len(caption_texts) == len(image_paths)\n",
    "    \n",
    "    # Create dataset from raw text and image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption_texts, image_paths))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption_texts)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¶</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel\n",
    "\n",
    "class DistillBertEncoder(tf.keras.Model):\n",
    "    def __init__(self, output_dim=128, freeze_bert=True):\n",
    "        super(DistillBertEncoder, self).__init__()\n",
    "        \n",
    "        self.distilbert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        if(freeze_bert):\n",
    "            self.distilbert.trainable = False\n",
    "\n",
    "        self.projection = tf.keras.layers.Dense(output_dim, activation='relu')\n",
    "    \n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, input_ids, attention_mask, training=False):\n",
    "        outputs = self.distilbert(input_ids, attention_mask=attention_mask, training=training)\n",
    "\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        cls_embedding = self.dropout(cls_embedding, training=training)\n",
    "\n",
    "        text_features = self.projection(cls_embedding)\n",
    "\n",
    "        return text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Generator\">Generator<a class=\"anchor-link\" href=\"#Generator\">¶</a></h2>\n",
    "<p>A image generator which generates the target image illustrating the input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: hidden representation of input text and random noise z with random seed.</li>\n",
    "<li>Output: target image, which is conditioned on the given text, in size 64x64x3.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization as per DCGAN paper\n",
    "def dcgan_weight_init():\n",
    "    \"\"\"Returns weight initializer for DCGAN: Normal(mean=0, stddev=0.02)\"\"\"\n",
    "    return tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    DCGAN Generator for 64x64 images\n",
    "    Uses transposed convolutions to progressively upsample from noise+text\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # Weight initializer\n",
    "        init = dcgan_weight_init()\n",
    "        \n",
    "        # Project and reshape\n",
    "        # Input: [batch, z_dim + text_embed_dim] (e.g., 512 + 128 = 640)\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            4 * 4 * 1024,  # Will reshape to [batch, 4, 4, 1024]\n",
    "            use_bias=False,\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        self.bn0 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Transposed convolutions for upsampling\n",
    "        # 4x4 -> 8x8\n",
    "        self.conv1 = tf.keras.layers.Conv2DTranspose(\n",
    "            512, kernel_size=4, strides=2, padding='same',\n",
    "            use_bias=False, kernel_initializer=init\n",
    "        )\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 8x8 -> 16x16\n",
    "        self.conv2 = tf.keras.layers.Conv2DTranspose(\n",
    "            256, kernel_size=4, strides=2, padding='same',\n",
    "            use_bias=False, kernel_initializer=init\n",
    "        )\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 16x16 -> 32x32\n",
    "        self.conv3 = tf.keras.layers.Conv2DTranspose(\n",
    "            128, kernel_size=4, strides=2, padding='same',\n",
    "            use_bias=False, kernel_initializer=init\n",
    "        )\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 32x32 -> 64x64 (final output)\n",
    "        self.conv4 = tf.keras.layers.Conv2DTranspose(\n",
    "            3, kernel_size=4, strides=2, padding='same',\n",
    "            use_bias=False, kernel_initializer=init\n",
    "        )\n",
    "        # No batch norm on output layer\n",
    "        \n",
    "    def call(self, text, noise_z, training=True):\n",
    "        # Concatenate noise and text embeddings\n",
    "        # text shape: [batch, text_embed_dim] (e.g., [16, 128])\n",
    "        # noise_z shape: [batch, z_dim] (e.g., [16, 512])\n",
    "        x = tf.concat([noise_z, text], axis=1)  # [batch, 640]\n",
    "        \n",
    "        # Project and reshape\n",
    "        x = self.dense(x)  # [batch, 4*4*1024]\n",
    "        x = self.bn0(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.reshape(x, [-1, 4, 4, 1024])  # [batch, 4, 4, 1024]\n",
    "        \n",
    "        # Upsample: 4x4 -> 8x8\n",
    "        x = self.conv1(x)  # [batch, 8, 8, 512]\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Upsample: 8x8 -> 16x16\n",
    "        x = self.conv2(x)  # [batch, 16, 16, 256]\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Upsample: 16x16 -> 32x32\n",
    "        x = self.conv3(x)  # [batch, 32, 32, 128]\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Upsample: 32x32 -> 64x64 (final)\n",
    "        x = self.conv4(x)  # [batch, 64, 64, 3]\n",
    "        output = tf.nn.tanh(x)  # Output in range [-1, 1]\n",
    "        \n",
    "        # Return both for compatibility with existing training code\n",
    "        return x, output  # logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Discriminator\">Discriminator<a class=\"anchor-link\" href=\"#Discriminator\">¶</a></h2>\n",
    "<p>A binary classifier which can discriminate the real and fake image:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Real image<ul>\n",
    "<li>Input: real image and the paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 1.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Fake Image<ul>\n",
    "<li>Input: generated image and paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 0.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    WGAN-GP Critic for 64x64 images\n",
    "    Key differences from DCGAN Discriminator:\n",
    "    1. NO batch normalization (causes issues with gradient penalty)\n",
    "    2. NO sigmoid activation (outputs raw scores)\n",
    "    3. Uses LeakyReLU throughout\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        \n",
    "        # Weight initializer\n",
    "        init = dcgan_weight_init()\n",
    "        \n",
    "        # Strided convolutions for downsampling\n",
    "        # 64x64 -> 32x32 (NO batch norm on first layer)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            64, kernel_size=4, strides=2, padding='same',\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        self.conv1_ln = tf.keras.layers.LayerNormalization()\n",
    "        # 32x32 -> 16x16 (NO batch norm!)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            128, kernel_size=4, strides=2, padding='same',\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        self.conv2_ln = tf.keras.layers.LayerNormalization()\n",
    "        # 16x16 -> 8x8 (NO batch norm!)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            256, kernel_size=4, strides=2, padding='same',\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        self.conv3_ln = tf.keras.layers.LayerNormalization()\n",
    "        # 8x8 -> 4x4 (NO batch norm!)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(\n",
    "            512, kernel_size=4, strides=2, padding='same',\n",
    "            kernel_initializer=init\n",
    "        )\n",
    "        self.conv4_ln = tf.keras.layers.LayerNormalization()\n",
    "        # Text conditioning layers\n",
    "        self.text_dense = tf.keras.layers.Dense(\n",
    "            512, kernel_initializer=init\n",
    "        )\n",
    "        self.text_dense_ln = tf.keras.layers.LayerNormalization()\n",
    "        # Final output layer\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.final = tf.keras.layers.Dense(1, kernel_initializer=init)\n",
    "        \n",
    "    def call(self, img, text, training=True):\n",
    "        # Image path: 64x64x3 -> 4x4x512\n",
    "        x = self.conv1(img)  # [batch, 32, 32, 64]\n",
    "        x = self.conv1_ln(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv2(x)  # [batch, 16, 16, 128]\n",
    "        x = self.conv2_ln(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv3(x)  # [batch, 8, 8, 256]\n",
    "        x = self.conv3_ln(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv4(x)  # [batch, 4, 4, 512]\n",
    "        x = self.conv4_ln(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        # Flatten image features\n",
    "        x = self.flatten(x)  # [batch, 8192]\n",
    "        \n",
    "        # Process text\n",
    "        text_features = self.text_dense(text)  # [batch, 512]\n",
    "        text_features = self.text_dense_ln(text_features, training=training)\n",
    "        text_features = tf.nn.leaky_relu(text_features, alpha=0.2)\n",
    "        \n",
    "        # Concatenate image and text features\n",
    "        combined = tf.concat([x, text_features], axis=1)  # [batch, 8704]\n",
    "        \n",
    "        # Final output - RAW SCORES (no sigmoid!)\n",
    "        output = self.final(combined)  # [batch, 1]\n",
    "        \n",
    "        return output  # Return only scores, not probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,\n",
    "    'EMBED_DIM': 256,\n",
    "    'VOCAB_SIZE': len(word2Id_dict),\n",
    "    'RNN_HIDDEN_SIZE': 256,\n",
    "    'Z_DIM': 512,\n",
    "    'DENSE_DIM': 128,\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    \n",
    "    # ========== UPDATED FOR PHASE 1 ==========\n",
    "    'BATCH_SIZE': 64,              # ← Changed from 16\n",
    "    'LR': 2e-4,                    # ← Changed from 2e-4 (scaled with batch size)\n",
    "    'BETA_1': 0.0,\n",
    "    'BETA_2': 0.9,\n",
    "    'N_CRITIC': 5,                 # ← Changed from 5 (better for small dataset)\n",
    "    'LAMBDA_GP': 10.0,              # ← Changed from 10.0 (gentler regularization)\n",
    "    \n",
    "    # ========== NEW: LEARNING RATE SCHEDULING ==========\n",
    "    'LR_DECAY_START': 50,          # Start decay at epoch 50\n",
    "    'LR_DECAY_EVERY': 10,          # Decay every N epochs\n",
    "    'LR_DECAY_FACTOR': 0.95,       # Multiply LR by this factor\n",
    "    'LR_MIN': 1e-5,                # Minimum learning rate\n",
    "    \n",
    "    # ========== NEW: DIFFAUGMENT ==========\n",
    "    'USE_DIFFAUG': True,           # Enable DiffAugment\n",
    "    'DIFFAUG_POLICY': 'translation,cutout',  # Augmentation policies\n",
    "    \n",
    "    # ========== OTHER ==========\n",
    "    'N_EPOCH': 1000,                # ← Extended from 100 (with early stopping)\n",
    "    'N_SAMPLE': num_training_sample,\n",
    "    'PRINT_FREQ': 2\n",
    "}\n",
    "\n",
    "print(f\"✓ Hyperparameters updated:\")\n",
    "print(f\"  Batch size: {hparas['BATCH_SIZE']}\")\n",
    "print(f\"  Learning rate: {hparas['LR']}\")\n",
    "print(f\"  N_Critic: {hparas['N_CRITIC']}\")\n",
    "print(f\"  Lambda_GP: {hparas['LAMBDA_GP']}\")\n",
    "print(f\"  DiffAugment: {hparas['USE_DIFFAUG']} ({hparas['DIFFAUG_POLICY']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = DistillBertEncoder(output_dim=hparas['RNN_HIDDEN_SIZE'], freeze_bert=True)\n",
    "generator = Generator(hparas)\n",
    "critic = Critic(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Loss-Function-and-Optimization\">Loss Function and Optimization<a class=\"anchor-link\" href=\"#Loss-Function-and-Optimization\">¶</a></h2>\n",
    "<p>Although the conditional GAN model is quite complex, the loss function used to optimize the network is relatively simple. Actually, it is simply a binary classification task, thus we use cross entropy as our loss.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss_critic(real_scores, fake_scores):\n",
    "    \"\"\"\n",
    "    Wasserstein loss for critic\n",
    "    Critic wants to maximize: E[critic(real)] - E[critic(fake)]\n",
    "    So we minimize: E[critic(fake)] - E[critic(real)]\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(fake_scores) - tf.reduce_mean(real_scores)\n",
    "\n",
    "def wasserstein_loss_generator(fake_scores):\n",
    "    \"\"\"\n",
    "    Wasserstein loss for generator\n",
    "    Generator wants to maximize: E[critic(fake)]\n",
    "    So we minimize: -E[critic(fake)]\n",
    "    \"\"\"\n",
    "    return -tf.reduce_mean(fake_scores)\n",
    "\n",
    "def gradient_penalty(critic, real_images, fake_images, text_embed, batch_size):\n",
    "    \"\"\"\n",
    "    Gradient penalty for WGAN-GP with optional DiffAugment\n",
    "    \n",
    "    Computes ||∇_x critic(x)||₂ for interpolated images x\n",
    "    Penalty = λ * mean((||gradient|| - 1)²)\n",
    "    \"\"\"\n",
    "    # Random weight for interpolation\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    \n",
    "    # Interpolated images: x_hat = alpha * real + (1 - alpha) * fake\n",
    "    interpolated = alpha * real_images + (1.0 - alpha) * fake_images\n",
    "    \n",
    "    \n",
    "    # Compute critic scores on (augmented) interpolated images\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        interpolated_scores = critic(interpolated, text_embed, training=True)\n",
    "    \n",
    "    # Compute gradients of scores w.r.t. interpolated images\n",
    "    gradients = gp_tape.gradient(interpolated_scores, [interpolated])[0]\n",
    "    \n",
    "    # Compute L2 norm of gradients for each sample\n",
    "    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "    \n",
    "    # Gradient penalty: mean((||gradient|| - 1)²)\n",
    "    gradient_penalty = tf.reduce_mean(tf.square(gradients_norm - 1.0))\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN-GP: Use Adam with beta_1=0.0, beta_2=0.9\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=hparas['LR'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=hparas['LR'] ,\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    critic_optimizer=critic_optimizer,\n",
    "    text_encoder=text_encoder,\n",
    "    generator=generator,\n",
    "    critic=critic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wasserstein_distance(real_scores, fake_scores):\n",
    "    \"\"\"\n",
    "    Approximation of Wasserstein distance\n",
    "    Higher is better (critic getting better at distinguishing)\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(real_scores) - tf.reduce_mean(fake_scores)\n",
    "\n",
    "def calculate_gradient_norm(gradients):\n",
    "    \"\"\"Calculate L2 norm of gradients\"\"\"\n",
    "    squared_norms = [tf.reduce_sum(tf.square(g)) for g in gradients if g is not None]\n",
    "    total_norm = tf.sqrt(tf.reduce_sum(squared_norms))\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    CORRECTED WGAN-GP training step with:\n",
    "    - TextEncoder call moved INSIDE both tapes\n",
    "    - DiffAugment\n",
    "    - N_critic iterations\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(real_image)[0]\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Critic (multiple iterations)\n",
    "    # ============================================================\n",
    "    for _ in range(hparas['N_CRITIC']):\n",
    "        noise = tf.random.normal([batch_size, hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "        \n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            # --- FIX: Encode text INSIDE critic tape ---\n",
    "            text_embed = text_encoder(input_ids, attention_mask, training=True)\n",
    "            \n",
    "            # Generate fake images\n",
    "            _, fake_image = generator(text_embed, noise, training=True)\n",
    "            \n",
    "            # --- Generate aug params (simplified) ---\n",
    "            aug_params = None\n",
    "            if hparas['USE_DIFFAUG']:\n",
    "                batch_size_static = tf.shape(real_image)[0]\n",
    "                image_size = 64\n",
    "                shift = tf.cast(image_size * 0.125 + 0.5, tf.int32)\n",
    "                cutout_size = tf.cast(image_size * 0.5 + 0.5, tf.int32)\n",
    "                aug_params = {\n",
    "                    'brightness': tf.random.uniform([], -0.5, 0.5),\n",
    "                    'saturation': tf.random.uniform([], 0.0, 2.0),\n",
    "                    'contrast': tf.random.uniform([], 0.5, 1.5),\n",
    "                    'translation_x': tf.random.uniform([batch_size_static], -shift, shift + 1, dtype=tf.int32),\n",
    "                    'translation_y': tf.random.uniform([batch_size_static], -shift, shift + 1, dtype=tf.int32),\n",
    "                    'cutout_x': tf.random.uniform([batch_size_static], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "                    'cutout_y': tf.random.uniform([batch_size_static], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "                }\n",
    "\n",
    "            # Apply aug\n",
    "            if hparas['USE_DIFFAUG']:\n",
    "                real_image_aug = DiffAugment(real_image, policy=hparas['DIFFAUG_POLICY'], params=aug_params)\n",
    "                fake_image_aug = DiffAugment(fake_image, policy=hparas['DIFFAUG_POLICY'], params=aug_params)\n",
    "            else:\n",
    "                real_image_aug = real_image\n",
    "                fake_image_aug = fake_image\n",
    "            \n",
    "            # Get critic scores on augmented images\n",
    "            real_scores = critic(real_image_aug, text_embed, training=True)\n",
    "            fake_scores = critic(fake_image_aug, text_embed, training=True)\n",
    "            \n",
    "            # Wasserstein loss\n",
    "            c_loss_wasserstein = wasserstein_loss_critic(real_scores, fake_scores)\n",
    "            \n",
    "            # Gradient penalty (on AUGMENTED images)\n",
    "            gp = gradient_penalty(critic, real_image_aug, fake_image_aug, text_embed, batch_size)\n",
    "            \n",
    "            # Total critic loss\n",
    "            c_loss = c_loss_wasserstein + hparas['LAMBDA_GP'] * gp\n",
    "        \n",
    "        # Train both critic AND text_encoder (projection head)\n",
    "        combined_critic_vars = critic.trainable_variables + text_encoder.trainable_variables\n",
    "        grad_c = critic_tape.gradient(c_loss, combined_critic_vars)\n",
    "        # Filter out None gradients (e.g., from frozen BERT layers)\n",
    "        valid_grads_c = [(g, v) for g, v in zip(grad_c, combined_critic_vars) if g is not None]\n",
    "        critic_optimizer.apply_gradients(valid_grads_c)\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Generator (once per n_critic iterations)\n",
    "    # ============================================================\n",
    "    noise = tf.random.normal([batch_size, hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    \n",
    "    # --- Generate fresh aug params for generator step ---\n",
    "    gen_aug_params = None\n",
    "    if hparas['USE_DIFFAUG']:\n",
    "        batch_size_static = tf.shape(real_image)[0]\n",
    "        image_size = 64\n",
    "        shift = tf.cast(image_size * 0.125 + 0.5, tf.int32)\n",
    "        cutout_size = tf.cast(image_size * 0.5 + 0.5, tf.int32)\n",
    "        gen_aug_params = {\n",
    "            'brightness': tf.random.uniform([], -0.5, 0.5),\n",
    "            'saturation': tf.random.uniform([], 0.0, 2.0),\n",
    "            'contrast': tf.random.uniform([], 0.5, 1.5),\n",
    "            'translation_x': tf.random.uniform([batch_size_static], -shift, shift + 1, dtype=tf.int32),\n",
    "            'translation_y': tf.random.uniform([batch_size_static], -shift, shift + 1, dtype=tf.int32),\n",
    "            'cutout_x': tf.random.uniform([batch_size_static], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "            'cutout_y': tf.random.uniform([batch_size_static], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "        }\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # --- FIX: Encode text INSIDE gen_tape as well ---\n",
    "        text_embed = text_encoder(input_ids, attention_mask, training=True)\n",
    "        \n",
    "        _, fake_image = generator(text_embed, noise, training=True)\n",
    "        \n",
    "        if hparas['USE_DIFFAUG']:\n",
    "            fake_image_aug = DiffAugment(fake_image, policy=hparas['DIFFAUG_POLICY'], params=gen_aug_params)\n",
    "        else:\n",
    "            fake_image_aug = fake_image\n",
    "        \n",
    "        fake_scores = critic(fake_image_aug, text_embed, training=True)\n",
    "        g_loss = wasserstein_loss_generator(fake_scores)\n",
    "\n",
    "    # Train both generator AND text_encoder (projection head)\n",
    "    combined_gen_vars = generator.trainable_variables + text_encoder.trainable_variables\n",
    "    grad_combined = gen_tape.gradient(g_loss, combined_gen_vars)\n",
    "    # Filter out None gradients\n",
    "    valid_grads_g = [(g, v) for g, v in zip(grad_combined, combined_gen_vars) if g is not None]\n",
    "    generator_optimizer.apply_gradients(valid_grads_g)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    wasserstein_dist = calculate_wasserstein_distance(real_scores, fake_scores)\n",
    "    # Use the VALID gradients for norm calculation\n",
    "    grad_norm_g = calculate_gradient_norm([g for g, v in valid_grads_g])\n",
    "    grad_norm_c = calculate_gradient_norm([g for g, v in valid_grads_c])\n",
    "    \n",
    "    return {\n",
    "        'g_loss': g_loss,\n",
    "        'c_loss': c_loss,\n",
    "        'c_loss_wasserstein': c_loss_wasserstein,\n",
    "        'gp': gp,\n",
    "        'wasserstein_dist': wasserstein_dist,\n",
    "        'grad_norm_g': grad_norm_g,\n",
    "        'grad_norm_c': grad_norm_c\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(input_ids, attention_mask, noise):\n",
    "    # Encode text with DistilBERT (no hidden state)\n",
    "    text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "    _, fake_image = generator(text_embed, noise, training=False)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for visualization during training\n",
    "# IMPORTANT: All three variables must have the same batch size!\n",
    "\n",
    "sample_size = hparas['BATCH_SIZE']  # Current: 32\n",
    "ni = int(np.ceil(np.sqrt(sample_size)))  # Grid size for visualization\n",
    "\n",
    "# Create random noise seed\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "# Define 8 diverse sample sentences\n",
    "base_sentences = [\n",
    "    \"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "    \"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "    \"the petals on this flower are white with a yellow center\",\n",
    "    \"this flower has a lot of small round pink petals.\",\n",
    "    \"this flower is orange in color, and has petals that are ruffled and rounded.\",\n",
    "    \"the flower has yellow petals and the center of it is brown.\",\n",
    "    \"this flower has petals that are blue and white.\",\n",
    "    \"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "]\n",
    "\n",
    "# Repeat sentences to match sample_size (batch size)\n",
    "sample_sentences = []\n",
    "for i in range(sample_size):\n",
    "    sample_sentences.append(base_sentences[i % len(base_sentences)])\n",
    "\n",
    "# Tokenize with DistilBERT\n",
    "sample_encoded = preprocess_text_distilbert(sample_sentences, max_length=64)\n",
    "sample_input_ids = sample_encoded['input_ids']\n",
    "sample_attention_mask = sample_encoded['attention_mask']\n",
    "\n",
    "# Verify all dimensions match!\n",
    "print(f\"✓ Sample data created:\")\n",
    "print(f\"  Batch size: {sample_size}\")\n",
    "print(f\"  Grid size (ni): {ni} × {ni} = {ni*ni}\")\n",
    "print(f\"  Sample sentences: {len(sample_sentences)} sentences\")\n",
    "print(f\"  sample_seed shape: {sample_seed.shape}\")\n",
    "print(f\"  sample_input_ids shape: {sample_input_ids.shape}\")\n",
    "print(f\"  sample_attention_mask shape: {sample_attention_mask.shape}\")\n",
    "\n",
    "# Check for dimension mismatches\n",
    "assert len(sample_sentences) == sample_size, f\"Mismatch: {len(sample_sentences)} != {sample_size}\"\n",
    "assert sample_seed.shape[0] == sample_size, f\"Mismatch: {sample_seed.shape[0]} != {sample_size}\"\n",
    "assert sample_input_ids.shape[0] == sample_size, f\"Mismatch: {sample_input_ids.shape[0]} != {sample_size}\"\n",
    "assert sample_attention_mask.shape[0] == sample_size, f\"Mismatch: {sample_attention_mask.shape[0]} != {sample_size}\"\n",
    "print(\"✓ All dimensions match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for managing training runs\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def list_available_runs():\n",
    "    \"\"\"List all available training runs with their details\"\"\"\n",
    "    run_dirs = sorted(glob.glob('runs/*/'))\n",
    "    \n",
    "    if not run_dirs:\n",
    "        print('No training runs found in runs/ directory')\n",
    "        return []\n",
    "    \n",
    "    print('=' * 80)\n",
    "    print('Available Training Runs:')\n",
    "    print('=' * 80)\n",
    "    \n",
    "    available_runs = []\n",
    "    for run_dir in run_dirs:\n",
    "        timestamp = run_dir.split('/')[-2]\n",
    "        available_runs.append(timestamp)\n",
    "        \n",
    "        # Check for checkpoints\n",
    "        checkpoint_dir = f'{run_dir}checkpoints'\n",
    "        latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        has_checkpoint = '✓' if latest_ckpt else '✗'\n",
    "        \n",
    "        # Check for config\n",
    "        config_path = f'{run_dir}config.json'\n",
    "        has_config = '✓' if os.path.exists(config_path) else '✗'\n",
    "        \n",
    "        # Count sample images\n",
    "        sample_count = len(glob.glob(f'{run_dir}samples/*.jpg'))\n",
    "        \n",
    "        print(f'{timestamp}  |  Checkpoint: {has_checkpoint}  |  Config: {has_config}  |  Samples: {sample_count}')\n",
    "        \n",
    "        if latest_ckpt:\n",
    "            print(f'  └─ Latest checkpoint: {latest_ckpt}')\n",
    "    \n",
    "    print('=' * 80)\n",
    "    return available_runs\n",
    "\n",
    "def load_run_config(run_timestamp):\n",
    "    \"\"\"Load configuration from a previous run\"\"\"\n",
    "    config_path = f'runs/{run_timestamp}/config.json'\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f'Config not found: {config_path}')\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f'✓ Loaded config from: {config_path}')\n",
    "    return config\n",
    "\n",
    "# List available runs\n",
    "print('Checking for existing training runs...')\n",
    "list_available_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# RESUME TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "# Set to None for new run, or specify run timestamp to resume\n",
    "# Example: RESUME_RUN = '20251116-225453'\n",
    "RESUME_RUN = None  # ← Change this to resume from specific run\n",
    "\n",
    "# ============================================================\n",
    "# RUN DIRECTORY SETUP\n",
    "# ============================================================\n",
    "if RESUME_RUN:\n",
    "    # Resume from existing run\n",
    "    run_dir = f'runs/{RESUME_RUN}'\n",
    "    \n",
    "    # Verify directory exists\n",
    "    if not os.path.exists(run_dir):\n",
    "        raise FileNotFoundError(f'Run directory not found: {run_dir}')\n",
    "    \n",
    "    # Load existing config\n",
    "    try:\n",
    "        prev_config = load_run_config(RESUME_RUN)\n",
    "        run_timestamp = prev_config.get('run_timestamp', RESUME_RUN)\n",
    "        print(f'\\n⟳ RESUMING training from: {run_dir}')\n",
    "        print(f'  Original start: {run_timestamp}')\n",
    "        \n",
    "        # Warn if hyperparameters might be different\n",
    "        if 'hyperparameters' in prev_config:\n",
    "            prev_hparas = prev_config['hyperparameters']\n",
    "            if prev_hparas.get('BATCH_SIZE') != hparas['BATCH_SIZE']:\n",
    "                print(f'  ⚠ WARNING: Batch size changed ({prev_hparas.get(\"BATCH_SIZE\")} → {hparas[\"BATCH_SIZE\"]})')\n",
    "            if prev_hparas.get('LR') != hparas['LR']:\n",
    "                print(f'  ⚠ WARNING: Learning rate changed ({prev_hparas.get(\"LR\")} → {hparas[\"LR\"]})')\n",
    "    except Exception as e:\n",
    "        print(f'⚠ Could not load previous config: {e}')\n",
    "        run_timestamp = RESUME_RUN\n",
    "    \n",
    "    # Use existing subdirectories\n",
    "    checkpoint_dir = f'{run_dir}/checkpoints'\n",
    "    best_models_dir = f'{run_dir}/best_models'\n",
    "    samples_dir = f'{run_dir}/samples'\n",
    "    inference_dir = f'{run_dir}/inference'\n",
    "    \n",
    "    # Create directories if they don't exist (shouldn't happen, but safety check)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(best_models_dir, exist_ok=True)\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    os.makedirs(inference_dir, exist_ok=True)\n",
    "    \n",
    "else:\n",
    "    # Create new run\n",
    "    run_timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    run_dir = f'runs/{run_timestamp}'\n",
    "    \n",
    "    # All outputs for this run go in subdirectories\n",
    "    checkpoint_dir = f'{run_dir}/checkpoints'\n",
    "    best_models_dir = f'{run_dir}/best_models'\n",
    "    samples_dir = f'{run_dir}/samples'\n",
    "    inference_dir = f'{run_dir}/inference'\n",
    "    \n",
    "    # Create all directories\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(best_models_dir, exist_ok=True)\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    os.makedirs(inference_dir, exist_ok=True)\n",
    "    \n",
    "    print(f'✓ Created NEW run directory: {run_dir}')\n",
    "    \n",
    "    # Save hyperparameters\n",
    "    config_to_save = {\n",
    "        'run_timestamp': run_timestamp,\n",
    "        'hyperparameters': hparas,\n",
    "    }\n",
    "    with open(f'{run_dir}/config.json', 'w') as f:\n",
    "        json.dump(config_to_save, f, indent=4)\n",
    "    print(f'✓ Saved configuration to: {run_dir}/config.json')\n",
    "\n",
    "\n",
    "# Display directory structure\n",
    "print(f'\\nRun directory structure:')\n",
    "print(f'  {run_dir}/')\n",
    "print(f'  ├── checkpoints/  : {checkpoint_dir}')\n",
    "print(f'  ├── best_models/  : {best_models_dir}')\n",
    "print(f'  ├── samples/      : {samples_dir}')\n",
    "print(f'  └── inference/    : {inference_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESTORE CHECKPOINT FOR RESUMING TRAINING\n",
    "# ============================================================\n",
    "if RESUME_RUN:\n",
    "    # When resuming, restore the LATEST regular checkpoint (not best model)\n",
    "    # This ensures training continues from where it left off\n",
    "    print(f'\\nRestoring checkpoint for resuming training...')\n",
    "    print(f'Looking for latest checkpoint in: {checkpoint_dir}')\n",
    "    \n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "        ckpt_num = latest_checkpoint.split('-')[-1]\n",
    "        print(f'✓ Restored latest checkpoint: {latest_checkpoint}')\n",
    "        print(f'  Checkpoint number: {ckpt_num}')\n",
    "        print(f'  Training will continue from this point')\n",
    "        \n",
    "        # Also check if best model exists\n",
    "        best_checkpoint = tf.train.latest_checkpoint(best_models_dir)\n",
    "        if best_checkpoint:\n",
    "            print(f'\\n✓ Best model also available at: {best_checkpoint}')\n",
    "    else:\n",
    "        print('⚠ No checkpoint found in the run directory')\n",
    "        print('  Training will start from epoch 1 (this is unusual for RESUME mode)')\n",
    "else:\n",
    "    print('\\n✓ Starting NEW training run - no checkpoint restoration needed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    global run_dir, checkpoint_dir, best_models_dir, samples_dir, inference_dir\n",
    "    \n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    best_model_prefix = os.path.join(best_models_dir, \"best_ckpt\")\n",
    "    \n",
    "    log_dir = f'{run_dir}/logs'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "  \n",
    "    print(f\"Run tensorboard --logdir {log_dir}\")\n",
    "    print(f'Model: WGAN-GP with {hparas[\"N_CRITIC\"]} critic iterations')\n",
    "    print(f'DiffAugment: {hparas[\"USE_DIFFAUG\"]} ({hparas.get(\"DIFFAUG_POLICY\", \"N/A\")})')\n",
    "    \n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    global_step = 0\n",
    "    \n",
    "    # ========== EARLY STOPPING SETUP ==========\n",
    "    best_wasserstein_dist = float('inf')\n",
    "    patience = 300\n",
    "    patience_counter = 0\n",
    "    # ==========================================\n",
    "    \n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        # ========== LEARNING RATE DECAY ==========\n",
    "        if epoch >= hparas['LR_DECAY_START'] and epoch % hparas['LR_DECAY_EVERY'] == 0:\n",
    "            current_lr_g = generator_optimizer.learning_rate.numpy()\n",
    "            current_lr_c = critic_optimizer.learning_rate.numpy()\n",
    "            \n",
    "            new_lr_g = max(current_lr_g * hparas['LR_DECAY_FACTOR'], hparas['LR_MIN'])\n",
    "            new_lr_c = max(current_lr_c * hparas['LR_DECAY_FACTOR'], hparas['LR_MIN'])\n",
    "            \n",
    "            generator_optimizer.learning_rate.assign(new_lr_g)\n",
    "            critic_optimizer.learning_rate.assign(new_lr_c)\n",
    "            \n",
    "            print(f'  📉 LR Decay: G={new_lr_g:.2e}, C={new_lr_c:.2e}')\n",
    "        # ==========================================\n",
    "        \n",
    "        g_total_loss = 0\n",
    "        c_total_loss = 0\n",
    "        c_total_loss_wasserstein = 0\n",
    "        gp_total = 0\n",
    "        wd_total = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        pbar = tqdm(dataset, desc=f'Epoch {epoch+1}/{hparas[\"N_EPOCH\"]}', \n",
    "                   total=steps_per_epoch, unit='batch')\n",
    "        \n",
    "        for batch_idx, (image, input_ids, attention_mask) in enumerate(pbar):\n",
    "            metrics = train_step(image, input_ids, attention_mask)\n",
    "            \n",
    "            # Accumulate losses\n",
    "            g_total_loss += metrics['g_loss']\n",
    "            c_total_loss += metrics['c_loss']\n",
    "            c_total_loss_wasserstein += metrics['c_loss_wasserstein']\n",
    "            gp_total += metrics['gp']\n",
    "            wd_total += metrics['wasserstein_dist']\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('Losses/generator_loss', metrics['g_loss'], step=global_step)\n",
    "                tf.summary.scalar('Losses/critic_loss_total', metrics['c_loss'], step=global_step)\n",
    "                tf.summary.scalar('Losses/critic_loss_wasserstein', metrics['c_loss_wasserstein'], step=global_step)\n",
    "                tf.summary.scalar('Losses/gradient_penalty', metrics['gp'], step=global_step)\n",
    "                tf.summary.scalar('Metrics/wasserstein_distance', metrics['wasserstein_dist'], step=global_step)\n",
    "                \n",
    "                if global_step % 50 == 0:\n",
    "                    tf.summary.scalar('Gradients/generator_gradient_norm', metrics['grad_norm_g'], step=global_step)\n",
    "                    tf.summary.scalar('Gradients/critic_gradient_norm', metrics['grad_norm_c'], step=global_step)\n",
    "                    # ========== LOG LEARNING RATES ==========\n",
    "                    tf.summary.scalar('Training/learning_rate_generator', \n",
    "                                    generator_optimizer.learning_rate.numpy(), step=global_step)\n",
    "                    tf.summary.scalar('Training/learning_rate_critic', \n",
    "                                    critic_optimizer.learning_rate.numpy(), step=global_step)\n",
    "                    # ========================================\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'G_loss': f'{metrics[\"g_loss\"]:.4f}',\n",
    "                'C_loss': f'{metrics[\"c_loss\"]:.4f}',\n",
    "                'W_dist': f'{metrics[\"wasserstein_dist\"]:.4f}'\n",
    "            })\n",
    "            \n",
    "            global_step += 1\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_g_loss = g_total_loss / steps_per_epoch\n",
    "        avg_c_loss = c_total_loss / steps_per_epoch\n",
    "        avg_c_loss_w = c_total_loss_wasserstein / steps_per_epoch\n",
    "        avg_gp = gp_total / steps_per_epoch\n",
    "        avg_wd = wd_total / steps_per_epoch\n",
    "        epoch_time = time.time() - start\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: G_loss={avg_g_loss:.4f}, C_loss={avg_c_loss:.4f} ' +\n",
    "              f'(W={avg_c_loss_w:.4f}, GP={avg_gp:.4f}), W_dist={avg_wd:.4f}, Time={epoch_time:.2f}s')\n",
    "        \n",
    "        # Log epoch averages\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('Epoch/generator_loss_avg', avg_g_loss, step=epoch)\n",
    "            tf.summary.scalar('Epoch/critic_loss_avg', avg_c_loss, step=epoch)\n",
    "            tf.summary.scalar('Epoch/wasserstein_distance_avg', avg_wd, step=epoch)\n",
    "        \n",
    "        # ========== EARLY STOPPING CHECK ==========\n",
    "        if avg_wd < best_wasserstein_dist:  # Avoid near-zero\n",
    "            best_wasserstein_dist = avg_wd\n",
    "            patience_counter = 0\n",
    "            # Save best model in separate directory\n",
    "            best_path = checkpoint.save(file_prefix=best_model_prefix)\n",
    "            print(f'  ⭐ Best model saved! W_dist={avg_wd:.4f} → {best_path}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\n⚠️ Early stopping triggered at epoch {epoch+1}')\n",
    "            print(f'   Best Wasserstein distance: {best_wasserstein_dist:.4f}')\n",
    "            print(f'   No improvement for {patience} epochs')\n",
    "            break\n",
    "        # ==========================================\n",
    "        \n",
    "        # Save checkpoint (more frequently now)\n",
    "        if (epoch + 1) % 5 == 0:  # Changed from 50\n",
    "            saved_path = checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "            print(f'  ✓ Checkpoint saved: {saved_path}')\n",
    "        \n",
    "        # Visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            fake_image = test_step(sample_input_ids, sample_attention_mask, sample_seed)\n",
    "            save_images(fake_image, [ni, ni], f'{samples_dir}/train_{epoch+1:03d}.jpg')\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                display_images = (fake_image + 1.0) / 2.0\n",
    "                tf.summary.image('Generated_Samples', display_images, step=epoch, max_outputs=16)\n",
    "            \n",
    "            print(f'  ✓ Sample image saved and logged to TensorBoard')\n",
    "    \n",
    "    print('\\n✓ Training completed!')\n",
    "    print(f'All outputs saved to: {run_dir}')\n",
    "    print(f'Best Wasserstein distance achieved: {best_wasserstein_dist:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataset, hparas['N_EPOCH'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Evaluation</center></h1>\n",
    "\n",
    "<p><code>dataset/testData.pkl</code> is a pandas dataframe containing testing text with attributes 'ID' and 'Captions'.</p>\n",
    "\n",
    "<ul>\n",
    "<li>'ID': text ID used to name generated image.</li>\n",
    "<li>'Captions': text used as condition to generate image.</li>\n",
    "</ul>\n",
    "\n",
    "<p>For each captions, you need to generate <strong>inference_ID.png</strong> to evaluate quality of generated image. You must name the generated image in this format, otherwise we cannot evaluate your images.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "    \"\"\"\n",
    "    Updated testing data generator using DistilBERT tokenization\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string\n",
    "        index: Test sample ID\n",
    "    \n",
    "    Returns:\n",
    "        input_ids, attention_mask, index\n",
    "    \"\"\"\n",
    "    def tokenize_caption(text):\n",
    "        \"\"\"Python function to tokenize text using DistilBERT tokenizer\"\"\"\n",
    "        # Convert EagerTensor to bytes, then decode to string\n",
    "        text = text.numpy().decode('utf-8')\n",
    "        \n",
    "        # Tokenize using DistilBERT\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=64,\n",
    "            return_tensors='np'\n",
    "        )\n",
    "        \n",
    "        return encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "    \n",
    "    # Use tf.py_function to call Python tokenizer\n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize_caption,\n",
    "        inp=[caption_text],\n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    # Set shapes explicitly\n",
    "    input_ids.set_shape([64])\n",
    "    attention_mask.set_shape([64])\n",
    "    \n",
    "    return input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    \"\"\"\n",
    "    Updated testing dataset generator - decodes IDs to raw text\n",
    "    \"\"\"\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    captions_ids = data['Captions'].values\n",
    "    caption_texts = []\n",
    "    \n",
    "    # Decode pre-tokenized IDs back to text\n",
    "    for i in range(len(captions_ids)):\n",
    "        chosen_caption_ids = captions_ids[i]\n",
    "        \n",
    "        # Decode IDs back to text using id2word_dict\n",
    "        words = []\n",
    "        for word_id in chosen_caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':  # Skip padding tokens\n",
    "                words.append(word)\n",
    "        \n",
    "        caption_text = ' '.join(words)\n",
    "        caption_texts.append(caption_text)\n",
    "    \n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    # Create dataset from raw text\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference directory is already created by the train() function\n",
    "# No need to create it again here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore BEST MODEL for inference\n",
    "print(f'Looking for BEST model in: {best_models_dir}')\n",
    "\n",
    "best_checkpoint = tf.train.latest_checkpoint(best_models_dir)\n",
    "if best_checkpoint:\n",
    "    checkpoint.restore(best_checkpoint)\n",
    "    print(f'✓ Restored BEST model: {best_checkpoint}')\n",
    "    print(f'  This is the model with the lowest Wasserstein distance during training')\n",
    "else:\n",
    "    print('⚠ No best model found, trying regular checkpoints...')\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint)\n",
    "        print(f'✓ Restored latest checkpoint: {latest_checkpoint}')\n",
    "        print('  ⚠ WARNING: Using latest checkpoint, not best model')\n",
    "    else:\n",
    "        print('⚠ No checkpoint found at all, using fresh/untrained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    \"\"\"\n",
    "    Updated inference function for DistilBERT\n",
    "    FIXED: Generate fresh random noise for each batch!\n",
    "    \"\"\"\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    total_images = 0\n",
    "    \n",
    "    # Progress bar for inference\n",
    "    pbar = tqdm(total=NUM_TEST, desc='Generating images', unit='img')\n",
    "    \n",
    "    # Unpack 3 values: input_ids, attention_mask, idx\n",
    "    for input_ids, attention_mask, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        # CRITICAL FIX: Generate FRESH random noise for each batch\n",
    "        # This ensures diversity across all 819 test images\n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "        \n",
    "        fake_image = test_step(input_ids, attention_mask, sample_seed)\n",
    "        step += 1\n",
    "        \n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave(f'{inference_dir}/inference_{idx[i]:04d}.jpg', fake_image[i].numpy()*0.5 + 0.5)\n",
    "            total_images += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f'\\n✓ Generated {total_images} images in {time.time()-start:.4f} sec')\n",
    "    print(f'✓ Images saved to: {inference_dir}')\n",
    "    print(f'✓ Each image generated with unique random noise for better diversity!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(f'⚠ No images found in {inference_dir}')\n",
    "    print('Please run the inference cell first!')\n",
    "else:\n",
    "    # Randomly sample 20 images\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    num_samples = min(20, len(image_files))\n",
    "    sample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "    sample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "    # Create 4x5 grid\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, img_path in enumerate(sample_files):\n",
    "        # Extract image ID from filename\n",
    "        img_id = int(Path(img_path).stem.split('_')[1])\n",
    "        \n",
    "        # Find caption\n",
    "        caption_idx = np.where(test_ids == img_id)[0][0]\n",
    "        caption_ids = test_captions[caption_idx]\n",
    "        \n",
    "        # Decode caption\n",
    "        caption_text = ''\n",
    "        for word_id in caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':\n",
    "                caption_text += word + ' '\n",
    "        \n",
    "        # Load and display image\n",
    "        img = plt.imread(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    # Hide unused subplots if less than 20 images\n",
    "    for idx in range(num_samples, 20):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nTotal generated images: {len(image_files)}')\n",
    "    print(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
