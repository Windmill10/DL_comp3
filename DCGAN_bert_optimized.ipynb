{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center id=\"title\">DataLab Cup 3: Reverse Image Caption</center></h1>\n",
    "\n",
    "<center id=\"author\">Shan-Hung Wu &amp; DataLab<br/>Fall 2025</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Text to Image</center></h1>\n",
    "\n",
    "<h2 id=\"Platform:-Kaggle\">Platform: <a href=\"https://www.kaggle.com/competitions/2025-datalab-cup-3-reverse-image-caption/overview\">Kaggle</a><a class=\"anchor-link\" href=\"#Platform:-Kaggle\">¶</a></h2>\n",
    "<h2 id=\"Overview\">Overview<a class=\"anchor-link\" href=\"#Overview\">¶</a></h2>\n",
    "<p>In this work, we are interested in translating text in the form of single-sentence human-written descriptions directly into image pixels. For example, \"<strong>this flower has petals that are yellow and has a ruffled stamen</strong>\" and \"<strong>this pink and yellow flower has a beautiful yellow center with many stamens</strong>\". You have to develop a novel deep architecture and GAN formulation to effectively translate visual concepts from characters to pixels.</p>\n",
    "\n",
    "<p>More specifically, given a set of texts, your task is to generate reasonable images with size 64x64x3 to illustrate the corresponding texts. Here we use <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford-102 flower dataset</a> and its <a href=\"https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view\">paired texts</a> as our training dataset.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/example.png\"/>\n",
    "\n",
    "<ul>\n",
    "<li>7370 images as training set, where each images is annotated with at most 10 texts.</li>\n",
    "<li>819 texts for testing. You must generate 1 64x64x3 image for each text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN\">Conditional GAN<a class=\"anchor-link\" href=\"#Conditional-GAN\">¶</a></h2>\n",
    "<p>Given a text, in order to generate the image which can illustrate it, our model must meet several requirements:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Our model should have ability to understand and extract the meaning of given texts.<ul>\n",
    "<li>Use RNN or other language model, such as BERT, ELMo or XLNet, to capture the meaning of text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Our model should be able to generate image.<ul>\n",
    "<li>Use GAN to generate high quality image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>GAN-generated image should illustrate the text.<ul>\n",
    "<li>Use conditional-GAN to generate image conditioned on given text.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information $y$. We can perform the conditioning by feeding $y$ into both the discriminator and generator as additional input layer.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/cGAN.png\" width=\"500\"/>\n",
    "\n",
    "<p>There are two motivations for using some extra information in a GAN model:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Improve GAN.</li>\n",
    "<li>Generate targeted image.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Additional information that is correlated with the input images, such as class labels, can be used to improve the GAN. This improvement may come in the form of more stable training, faster training, and/or generated images that have better quality.</p>\n",
    "\n",
    "<img alt=\"No description has been provided for this image\" src=\"./data/GANCLS.jpg\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Python random\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TensorFlow random\n",
    "tf.random.set_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Preprocess-Text\">Preprocess Text<a class=\"anchor-link\" href=\"#Preprocess-Text\">¶</a></h2>\n",
    "<p>Since dealing with raw string is inefficient, we have done some data preprocessing for you:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Delete text over <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "<li>Delete all puntuation in the texts.</li>\n",
    "<li>Encode each vocabulary in <code>dictionary/vocab.npy</code>.</li>\n",
    "<li>Represent texts by a sequence of integer IDs.</li>\n",
    "<li>Replace rare words by <code>&lt;RARE&gt;</code> token to reduce vocabulary size for more efficient training.</li>\n",
    "<li>Add padding as <code>&lt;PAD&gt;</code> to each text to make sure all of them have equal length to <code>MAX_SEQ_LENGTH (20)</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<p>It is worth knowing that there is no necessary to append <code>&lt;ST&gt;</code> and <code>&lt;ED&gt;</code> to each text because we don't need to generate any sequence in this task.</p>\n",
    "\n",
    "<p>To make sure correctness of encoding of the original text, we can decode sequence vocabulary IDs by looking up the vocabulary dictionary:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>dictionary/word2Id.npy</code> is a numpy array mapping word to id.</li>\n",
    "<li><code>dictionary/id2Word.npy</code> is a numpy array mapping id back to word.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using DistilBERT tokenizer (sent2IdList removed)\n"
     ]
    }
   ],
   "source": [
    "# This cell previously contained sent2IdList() function\n",
    "# It has been removed as we now use DistilBERT tokenizer instead\n",
    "# The id2word_dict is still available from cell 6 for visualization purposes\n",
    "\n",
    "print(\"✓ Using DistilBERT tokenizer (sent2IdList removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Dataset\">Dataset<a class=\"anchor-link\" href=\"#Dataset\">¶</a></h2>\n",
    "<p>For training, the following files are in dataset folder:</p>\n",
    "\n",
    "<ul>\n",
    "<li><code>./dataset/text2ImgData.pkl</code> is a pandas dataframe with attribute 'Captions' and 'ImagePath'.<ul>\n",
    "<li>'Captions' : A list of text id list contain 1 to 10 captions.</li>\n",
    "<li>'ImagePath': Image path that store paired image.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code>./102flowers/</code> is the directory containing all training images.</li>\n",
    "<li><code>./dataset/testData.pkl</code> is a pandas a dataframe with attribute 'ID' and 'Captions', which contains testing data.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Create-Dataset-by-Dataset-API\">Create Dataset by Dataset API<a class=\"anchor-link\" href=\"#Create-Dataset-by-Dataset-API\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_caption_fn(text):\n",
    "    \"\"\"Global helper for tf.py_function\"\"\"\n",
    "    # Convert EagerTensor to bytes, then decode to string\n",
    "    text = text.numpy().decode('utf-8')\n",
    "    \n",
    "    # Tokenize using DistilBERT\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    return encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\n",
    "def preprocess_text_distilbert(text, max_length=64):\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'],\n",
    "        'attention_mask': encoded['attention_mask']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DiffAugment functions loaded\n",
      "  Policies available: color, translation, cutout\n"
     ]
    }
   ],
   "source": [
    "def DiffAugment(x, policy='color,translation,cutout', channels_first=False, params=None):\n",
    "    \"\"\"\n",
    "    Differentiable augmentation for GANs\n",
    "    \n",
    "    Args:\n",
    "        x: Input images [batch, H, W, C] \n",
    "        policy: Comma-separated augmentation policies\n",
    "        channels_first: If True, expects [batch, C, H, W]\n",
    "        params: Optional dict of pre-generated augmentation parameters for consistency\n",
    "    \n",
    "    Returns:\n",
    "        Augmented images\n",
    "    \"\"\"\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            # TensorFlow format: [batch, H, W, C]\n",
    "            for p in policy.split(','):\n",
    "                for f in AUGMENT_FNS[p]:\n",
    "                    x = f(x, params)  # ← Pass params!\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x, params=None):\n",
    "    \"\"\"Random brightness adjustment\"\"\"\n",
    "    if params is not None and 'brightness' in params:\n",
    "        magnitude = params['brightness']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], -0.5, 0.5)\n",
    "    x = x + magnitude\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x, params=None):\n",
    "    \"\"\"Random saturation adjustment\"\"\"\n",
    "    if params is not None and 'saturation' in params:\n",
    "        magnitude = params['saturation']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], 0.0, 2.0)\n",
    "    x_mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x, params=None):\n",
    "    \"\"\"Random contrast adjustment\"\"\"\n",
    "    if params is not None and 'contrast' in params:\n",
    "        magnitude = params['contrast']\n",
    "    else:\n",
    "        magnitude = tf.random.uniform([], 0.5, 1.5)\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return x\n",
    "\n",
    "def rand_translation(x, params=None, ratio=0.125):\n",
    "    \"\"\"Random translation (shift) - Fully vectorized for @tf.function\"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    image_size = tf.shape(x)[1]\n",
    "    shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "    \n",
    "    # Random translation amounts for entire batch\n",
    "    if params is not None and 'translation_x' in params:\n",
    "        translation_x = params['translation_x']\n",
    "        translation_y = params['translation_y']\n",
    "    else:\n",
    "        translation_x = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "        translation_y = tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32)\n",
    "    \n",
    "    def translate_single_image(args):\n",
    "        \"\"\"Translate a single image\"\"\"\n",
    "        img, tx, ty = args\n",
    "        img = tf.pad(img, [[shift, shift], [shift, shift], [0, 0]], mode='REFLECT')\n",
    "        img = tf.image.crop_to_bounding_box(img, shift + ty, shift + tx, image_size, image_size)\n",
    "        return img\n",
    "    \n",
    "    # Use tf.map_fn (graph-mode compatible)\n",
    "    x_translated = tf.map_fn(\n",
    "        translate_single_image,\n",
    "        (x, translation_x, translation_y),\n",
    "        fn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "        parallel_iterations=10\n",
    "    )\n",
    "    \n",
    "    return x_translated\n",
    "\n",
    "\n",
    "def rand_cutout(x, params=None, ratio=0.5):\n",
    "    \"\"\"\n",
    "    Random cutout - SIMPLIFIED vectorized version\n",
    "    \n",
    "    Instead of complex per-pixel masking, we create rectangular masks\n",
    "    using broadcasting and boolean operations\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    image_size = tf.shape(x)[1]\n",
    "    channels = tf.shape(x)[3]\n",
    "    \n",
    "    # Cutout size\n",
    "    cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "    \n",
    "    # Random offset for cutout location\n",
    "    if params is not None and 'cutout_x' in params:\n",
    "        offset_x = params['cutout_x']\n",
    "        offset_y = params['cutout_y']\n",
    "    else:\n",
    "        offset_x = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "        offset_y = tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32)\n",
    "    \n",
    "    def cutout_single_image(args):\n",
    "        \"\"\"Apply cutout to single image using simple slicing\"\"\"\n",
    "        img, ox, oy = args\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        height_range = tf.range(image_size)\n",
    "        width_range = tf.range(image_size)\n",
    "        \n",
    "        # Create 2D grids\n",
    "        yy, xx = tf.meshgrid(height_range, width_range, indexing='ij')\n",
    "        \n",
    "        # Create mask: True where we want to KEEP pixels\n",
    "        mask_y = tf.logical_or(yy < oy, yy >= oy + cutout_size)\n",
    "        mask_x = tf.logical_or(xx < ox, xx >= ox + cutout_size)\n",
    "        mask = tf.logical_or(mask_y, mask_x)\n",
    "        \n",
    "        # Expand mask to all channels\n",
    "        mask = tf.expand_dims(mask, axis=-1)  # [H, W, 1]\n",
    "        mask = tf.tile(mask, [1, 1, channels])  # [H, W, C]\n",
    "        \n",
    "        # Apply mask (convert bool to float)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return img * mask\n",
    "    \n",
    "    # Use tf.map_fn\n",
    "    x_cutout = tf.map_fn(\n",
    "        cutout_single_image,\n",
    "        (x, offset_x, offset_y),\n",
    "        fn_output_signature=tf.TensorSpec(shape=[64, 64, 3], dtype=tf.float32),\n",
    "        parallel_iterations=10\n",
    "    )\n",
    "    \n",
    "    return x_cutout\n",
    "\n",
    "def get_aug_params(batch_size):\n",
    "    \"\"\"Generate random augmentation parameters for a batch\"\"\"\n",
    "    image_size = 64\n",
    "    shift = tf.cast(image_size * 0.125 + 0.5, tf.int32)\n",
    "    cutout_size = tf.cast(image_size * 0.5 + 0.5, tf.int32)\n",
    "    \n",
    "    return {\n",
    "        'brightness': tf.random.uniform([], -0.5, 0.5),\n",
    "        'saturation': tf.random.uniform([], 0.0, 2.0),\n",
    "        'contrast': tf.random.uniform([], 0.5, 1.5),\n",
    "        'translation_x': tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32),\n",
    "        'translation_y': tf.random.uniform([batch_size], -shift, shift + 1, dtype=tf.int32),\n",
    "        'cutout_x': tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "        'cutout_y': tf.random.uniform([batch_size], 0, image_size - cutout_size + 1, dtype=tf.int32),\n",
    "    }\n",
    "\n",
    "\n",
    "# Augmentation function registry\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}\n",
    "\n",
    "\n",
    "print(\"✓ DiffAugment functions loaded\")\n",
    "print(\"  Policies available: color, translation, cutout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption_text, image_path):\n",
    "    \"\"\"\n",
    "    Updated data generator using DistilBERT tokenization\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string (not IDs!)\n",
    "        image_path: Path to image file\n",
    "    \n",
    "    Returns:\n",
    "        img, input_ids, attention_mask\n",
    "    \"\"\"\n",
    "    # ============= IMAGE PROCESSING (same as before) =============\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0, 1]\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    # Normalize to [-1, 1] to match generator's tanh output\n",
    "    img = (img * 2.0) - 1.0\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "    # ============= TEXT PROCESSING (NEW: Use Global Helper) =============\n",
    "    # Use tf.py_function to call Python tokenizer\n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize_caption_fn,\n",
    "        inp=[caption_text],\n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    # Set shapes explicitly\n",
    "    input_ids.set_shape([64])\n",
    "    attention_mask.set_shape([64])\n",
    "    \n",
    "    return img, input_ids, attention_mask\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    \"\"\"\n",
    "    Updated dataset generator to work with raw text (decoded from IDs)\n",
    "    \"\"\"\n",
    "    # Load the training data\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions_ids = df['Captions'].values\n",
    "    caption_texts = []\n",
    "    \n",
    "    # Decode pre-tokenized IDs back to raw text\n",
    "    for i in range(len(captions_ids)):\n",
    "        # Randomly choose one caption (list of ID lists)\n",
    "        chosen_caption_ids = random.choice(captions_ids[i])\n",
    "        \n",
    "        # Decode IDs back to text using id2word_dict\n",
    "        words = []\n",
    "        for word_id in chosen_caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':  # Skip padding tokens\n",
    "                words.append(word)\n",
    "        \n",
    "        caption_text = ' '.join(words)\n",
    "        caption_texts.append(caption_text)\n",
    "    \n",
    "    image_paths = df['ImagePath'].values\n",
    "    \n",
    "    # Verify same length\n",
    "    assert len(caption_texts) == len(image_paths)\n",
    "    \n",
    "    # Create dataset from raw text and image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption_texts, image_paths))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(len(caption_texts)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Conditional-GAN-Model\">Conditional GAN Model<a class=\"anchor-link\" href=\"#Conditional-GAN-Model\">¶</a></h2>\n",
    "<p>As mentioned above, there are three models in this task, text encoder, generator and discriminator.</p>\n",
    "\n",
    "<h2 id=\"Text-Encoder\">Text Encoder<a class=\"anchor-link\" href=\"#Text-Encoder\">¶</a></h2>\n",
    "<p>A RNN encoder that captures the meaning of input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: text, which is a list of ids.</li>\n",
    "<li>Output: embedding, or hidden representation of input text.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Import TensorFlow FIRST before transformers\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel\n",
    "\n",
    "class DistillBertEncoder(tf.keras.Model):\n",
    "    def __init__(self, output_dim=128, freeze_bert=True):\n",
    "        super(DistillBertEncoder, self).__init__()\n",
    "        \n",
    "        self.distilbert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        if(freeze_bert):\n",
    "            self.distilbert.trainable = False\n",
    "\n",
    "\t\t\t\t#change activation to None\n",
    "        self.projection = tf.keras.layers.Dense(output_dim, activation='relu')\n",
    "\n",
    "        self.ln = tf.keras.layers.LayerNormalization(axis=-1)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, input_ids, attention_mask, training=False):\n",
    "        outputs = self.distilbert(input_ids, attention_mask=attention_mask, training=training)\n",
    "\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        cls_embedding = self.dropout(cls_embedding, training=training)\n",
    "\n",
    "        text_features = self.projection(cls_embedding)\n",
    "\n",
    "        text_features = self.ln(text_features, training=training)\n",
    "\n",
    "        return text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Generator\">Generator<a class=\"anchor-link\" href=\"#Generator\">¶</a></h2>\n",
    "<p>A image generator which generates the target image illustrating the input text.</p>\n",
    "\n",
    "<ul>\n",
    "<li>Input: hidden representation of input text and random noise z with random seed.</li>\n",
    "<li>Output: target image, which is conditioned on the given text, in size 64x64x3.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization as per DCGAN paper\n",
    "def dcgan_weight_init():\n",
    "    \"\"\"Returns weight initializer for DCGAN: Normal(mean=0, stddev=0.02)\"\"\"\n",
    "    return tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        init = dcgan_weight_init()\n",
    "        \n",
    "        # 1. Input Projection\n",
    "        self.dense = tf.keras.layers.Dense(4 * 4 * 512, use_bias=False, kernel_initializer=init)\n",
    "        self.bn0 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 2. Upsample Blocks (Replacing Conv2DTranspose)\n",
    "        \n",
    "        # Block 1: 4x4 -> 8x8\n",
    "        self.up1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv1 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Block 2: 8x8 -> 16x16\n",
    "        self.up2 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Block 3: 16x16 -> 32x32\n",
    "        self.up3 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Block 4: 32x32 -> 64x64\n",
    "        self.up4 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv4 = tf.keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer=init)\n",
    "        \n",
    "    def call(self, text, noise_z, training=True):\n",
    "        # Concatenate inputs\n",
    "        x = tf.concat([noise_z, text], axis=1)\n",
    "        \n",
    "        # Project and Reshape\n",
    "        x = self.dense(x)\n",
    "        x = self.bn0(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.reshape(x, [-1, 4, 4, 512])\n",
    "        \n",
    "        # 4x4 -> 8x8\n",
    "        x = self.up1(x)      # Upsample\n",
    "        x = self.conv1(x)    # Convolve\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # 8x8 -> 16x16\n",
    "        x = self.up2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # 16x16 -> 32x32\n",
    "        x = self.up3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # 32x32 -> 64x64\n",
    "        x = self.up4(x)\n",
    "        x = self.conv4(x)\n",
    "        output = tf.nn.tanh(x)\n",
    "        \n",
    "        return x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Discriminator\">Discriminator<a class=\"anchor-link\" href=\"#Discriminator\">¶</a></h2>\n",
    "<p>A binary classifier which can discriminate the real and fake image:</p>\n",
    "\n",
    "<ol>\n",
    "<li>Real image<ul>\n",
    "<li>Input: real image and the paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 1.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Fake Image<ul>\n",
    "<li>Input: generated image and paired text</li>\n",
    "<li>Output: a floating number representing the result, which is expected to be 0.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    GAN-CLS Critic with Spatial Concatenation.\n",
    "    Matches text to images at the 4x4 feature level.\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        init = dcgan_weight_init()\n",
    "        \n",
    "        # --- IMAGE PATH (Downsampling) ---\n",
    "        # 64x64 -> 32x32\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, padding='same', kernel_initializer=init)\n",
    "        self.conv1_ln = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "        \n",
    "        # 32x32 -> 16x16\n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding='same', kernel_initializer=init)\n",
    "        self.conv2_ln = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "        \n",
    "        # 16x16 -> 8x8\n",
    "        self.conv3 = tf.keras.layers.Conv2D(256, kernel_size=4, strides=2, padding='same', kernel_initializer=init)\n",
    "        self.conv3_ln = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "        \n",
    "        # 8x8 -> 4x4\n",
    "        self.conv4 = tf.keras.layers.Conv2D(512, kernel_size=4, strides=2, padding='same', kernel_initializer=init)\n",
    "        self.conv4_ln = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n",
    "        \n",
    "        # --- TEXT PATH ---\n",
    "        # Project text to match the depth of image features (512 channels)\n",
    "        self.text_dense = tf.keras.layers.Dense(512, kernel_initializer=tf.keras.initializers.Orthogonal())\n",
    "        self.text_dense_ln = tf.keras.layers.LayerNormalization(axis=-1) # Axis -1 for 1D text vector\n",
    "        \n",
    "        # --- COMBINED PATH ---\n",
    "        # We flatten a much larger vector now (4x4x1024)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.final = tf.keras.layers.Dense(1, kernel_initializer=init)\n",
    "        \n",
    "    def call(self, img, text, training=True):\n",
    "        # 1. Process Image Features\n",
    "        x = self.conv1(img)\n",
    "        x = self.conv1_ln(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_ln(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.conv3_ln(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        x = self.conv4(x) # Shape: [Batch, 4, 4, 512]\n",
    "        x = self.conv4_ln(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "        \n",
    "        # 2. Process Text Features\n",
    "        t = self.text_dense(text) # Shape: [Batch, 512]\n",
    "        t = self.text_dense_ln(t, training=training)\n",
    "        t = tf.nn.leaky_relu(t, alpha=0.2)\n",
    "        \n",
    "        # 3. SPATIAL CONCATENATION (The Upgrade)\n",
    "        # Reshape text to [Batch, 1, 1, 512] to be broadcastable\n",
    "        t = tf.reshape(t, [-1, 1, 1, 512])\n",
    "        \n",
    "        # Tile text across the spatial dimensions (4x4)\n",
    "        # We copy the text info to every \"pixel\" location in the feature map\n",
    "        t = tf.tile(t, [1, 4, 4, 1]) # Shape: [Batch, 4, 4, 512]\n",
    "        \n",
    "        # Concatenate along the channel axis\n",
    "        # Input 1: Image [Batch, 4, 4, 512]\n",
    "        # Input 2: Text  [Batch, 4, 4, 512]\n",
    "        # Output: Combined [Batch, 4, 4, 1024]\n",
    "        combined = tf.concat([x, t], axis=-1)\n",
    "        \n",
    "        # 4. Final Scoring\n",
    "        # Flatten the spatially fused features\n",
    "        flat = self.flatten(combined) \n",
    "        output = self.final(flat)\n",
    "        \n",
    "        return output, flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hyperparameters updated:\n",
      "  Batch size: 128\n",
      "  Learning rate: 0.0002\n",
      "  N_Critic: 5\n",
      "  Lambda_GP: 10.0\n",
      "  DiffAugment: True (translation,cutout)\n"
     ]
    }
   ],
   "source": [
    "hparas = {\n",
    "    'RNN_HIDDEN_SIZE': 256,\n",
    "    'Z_DIM': 512,\n",
    "    'DENSE_DIM': 128,\n",
    "    'IMAGE_SIZE': [64, 64, 3],\n",
    "    \n",
    "    # ========== UPDATED FOR PHASE 1 ==========\n",
    "    'BATCH_SIZE': BATCH_SIZE,            \n",
    "    'LR': 2e-4,                    # ← Changed from 2e-4 (scaled with batch size)\n",
    "    'BETA_1': 0.0,\n",
    "    'BETA_2': 0.9,\n",
    "    'N_CRITIC': 5,                 # ← Changed from 5 (better for small dataset)\n",
    "    'LAMBDA_GP': 10.0,              # ← Changed from 10.0 (gentler regularization)\n",
    "    'LAMBDA_MISMATCH': 5.0,\n",
    "    #'LAMBDA_FM': 1.0,\n",
    "    \n",
    "    # ========== NEW: LEARNING RATE SCHEDULING ==========\n",
    "    'LR_DECAY_START': 50,          # Start decay at epoch 50\n",
    "    'LR_DECAY_EVERY': 10,          # Decay every N epochs\n",
    "    'LR_DECAY_FACTOR': 0.95,       # Multiply LR by this factor\n",
    "    'LR_MIN': 1e-5,                # Minimum learning rate\n",
    "    \n",
    "    # ========== NEW: DIFFAUGMENT ==========\n",
    "    'USE_DIFFAUG': True,           # Enable DiffAugment\n",
    "    'DIFFAUG_POLICY': 'translation,cutout',  # Augmentation policies\n",
    "    \n",
    "    # ========== OTHER ==========\n",
    "    'N_EPOCH': 1000,                # ← Extended from 100 (with early stopping)\n",
    "    'N_SAMPLE': num_training_sample,\n",
    "    'PRINT_FREQ': 2\n",
    "}\n",
    "\n",
    "print(f\"✓ Hyperparameters updated:\")\n",
    "print(f\"  Batch size: {hparas['BATCH_SIZE']}\")\n",
    "print(f\"  Learning rate: {hparas['LR']}\")\n",
    "print(f\"  N_Critic: {hparas['N_CRITIC']}\")\n",
    "print(f\"  Lambda_GP: {hparas['LAMBDA_GP']}\")\n",
    "print(f\"  DiffAugment: {hparas['USE_DIFFAUG']} ({hparas['DIFFAUG_POLICY']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "text_encoder = DistillBertEncoder(output_dim=hparas['RNN_HIDDEN_SIZE'], freeze_bert=True)\n",
    "generator = Generator(hparas)\n",
    "critic = Critic(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Loss-Function-and-Optimization\">Loss Function and Optimization<a class=\"anchor-link\" href=\"#Loss-Function-and-Optimization\">¶</a></h2>\n",
    "<p>Although the conditional GAN model is quite complex, the loss function used to optimize the network is relatively simple. Actually, it is simply a binary classification task, thus we use cross entropy as our loss.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss_critic(real_scores, fake_scores):\n",
    "    \"\"\"\n",
    "    Wasserstein loss for critic\n",
    "    Critic wants to maximize: E[critic(real)] - E[critic(fake)]\n",
    "    So we minimize: E[critic(fake)] - E[critic(real)]\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(fake_scores) - tf.reduce_mean(real_scores)\n",
    "\n",
    "def mismatch_loss_critic(real_scores, wrong_scores, margin=1.0):\n",
    "    \"\"\"\n",
    "    Hinge loss for GAN-CLS (mismatched pairs).\n",
    "    Wants: real_scores > wrong_scores + margin\n",
    "    Minimizes: max(0, margin + wrong_scores - real_scores)\n",
    "    \"\"\"\n",
    "    loss = tf.nn.relu(margin + wrong_scores - real_scores)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def wasserstein_loss_generator(fake_scores):\n",
    "    \"\"\"\n",
    "    Wasserstein loss for generator\n",
    "    Generator wants to maximize: E[critic(fake)]\n",
    "    So we minimize: -E[critic(fake)]\n",
    "    \"\"\"\n",
    "    return -tf.reduce_mean(fake_scores)\n",
    "\n",
    "def gradient_penalty(critic, real_images, fake_images, text_embed, batch_size):\n",
    "    \"\"\"\n",
    "    Gradient penalty for WGAN-GP with optional DiffAugment\n",
    "    \n",
    "    Computes ||∇_x critic(x)||₂ for interpolated images x\n",
    "    Penalty = λ * mean((||gradient|| - 1)²)\n",
    "    \"\"\"\n",
    "    # Random weight for interpolation\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "    \n",
    "    # Interpolated images: x_hat = alpha * real + (1 - alpha) * fake\n",
    "    interpolated = alpha * real_images + (1.0 - alpha) * fake_images\n",
    "    \n",
    "    \n",
    "    # Compute critic scores on (augmented) interpolated images\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        interpolated_scores, _ = critic(interpolated, text_embed, training=True)\n",
    "    \n",
    "    # Compute gradients of scores w.r.t. interpolated images\n",
    "    gradients = gp_tape.gradient(interpolated_scores, [interpolated])[0]\n",
    "    \n",
    "    # Compute L2 norm of gradients for each sample\n",
    "    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "    \n",
    "    # Gradient penalty: mean((||gradient|| - 1)²)\n",
    "    gradient_penalty = tf.reduce_mean(tf.square(gradients_norm - 1.0))\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# WGAN-GP: Use Adam with beta_1=0.0, beta_2=0.9\n",
    "generator_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=hparas['LR'],\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=hparas['LR'] * 2 ,\n",
    "    beta_1=hparas['BETA_1'],\n",
    "    beta_2=hparas['BETA_2']\n",
    ")\n",
    "\n",
    "# Optimizaiton code, using dummy generator\n",
    "ema_generator = Generator(hparas)\n",
    "dummy_text = tf.random.normal([1, hparas['RNN_HIDDEN_SIZE']])\n",
    "dummy_noise = tf.random.normal([1, hparas['Z_DIM']])\n",
    "_ = generator(dummy_text, dummy_noise, training=True)\n",
    "_ = ema_generator(dummy_text, dummy_noise, training=True)\n",
    "ema_generator.set_weights(generator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_tracker = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
    "best_wd_tracker = tf.Variable(float('inf'), trainable=False, dtype=tf.float32)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    critic_optimizer=critic_optimizer,\n",
    "    text_encoder=text_encoder,\n",
    "    generator=generator,\n",
    "    ema_generator=ema_generator,\n",
    "    critic=critic,\n",
    "    epoch_tracker=epoch_tracker,\n",
    "    best_wd_tracker=best_wd_tracker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wasserstein_distance(real_scores, fake_scores):\n",
    "    \"\"\"\n",
    "    Approximation of Wasserstein distance\n",
    "    Higher is better (critic getting better at distinguishing)\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(real_scores) - tf.reduce_mean(fake_scores)\n",
    "\n",
    "def calculate_gradient_norm(gradients):\n",
    "    \"\"\"Calculate L2 norm of gradients\"\"\"\n",
    "    squared_norms = [tf.reduce_sum(tf.square(g)) for g in gradients if g is not None]\n",
    "    total_norm = tf.sqrt(tf.reduce_sum(squared_norms))\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    CORRECTED WGAN-GP training step with:\n",
    "    - TextEncoder call moved INSIDE both tapes\n",
    "    - DiffAugment\n",
    "    - N_critic iterations\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(real_image)[0]\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Critic (multiple iterations)\n",
    "    # ============================================================\n",
    "    for _ in range(hparas['N_CRITIC']):\n",
    "        noise = tf.random.normal([batch_size, hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "        \n",
    "        with tf.GradientTape() as critic_tape:\n",
    "            text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "            \n",
    "            # Create mismatched text embedding\n",
    "            wrong_text_embed = tf.roll(text_embed, shift=1, axis=0)\n",
    "            # Generate fake images\n",
    "            _, fake_image = generator(text_embed, noise, training=True)\n",
    "            \n",
    "            aug_params = None\n",
    "            if hparas['USE_DIFFAUG']:\n",
    "                # Use helper function\n",
    "                aug_params = get_aug_params(tf.shape(real_image)[0])\n",
    "\n",
    "            # Apply aug\n",
    "            if hparas['USE_DIFFAUG']:\n",
    "                real_image_aug = DiffAugment(real_image, policy=hparas['DIFFAUG_POLICY'], params=aug_params)\n",
    "                fake_image_aug = DiffAugment(fake_image, policy=hparas['DIFFAUG_POLICY'], params=aug_params)\n",
    "            else:\n",
    "                real_image_aug = real_image\n",
    "                fake_image_aug = fake_image\n",
    "            \n",
    "            # Get critic scores on augmented images\n",
    "            real_scores = critic(real_image_aug, text_embed, training=True)\n",
    "            fake_scores = critic(fake_image_aug, text_embed, training=True)\n",
    "            wrong_scores, _ = critic(real_image_aug, wrong_text_embed, training=True)\n",
    "            \n",
    "            # Wasserstein loss\n",
    "            c_loss_wasserstein = wasserstein_loss_critic(real_scores, fake_scores)\n",
    "\n",
    "            # Gradient penalty (on AUGMENTED images)\n",
    "            gp = gradient_penalty(critic, real_image_aug, fake_image_aug, text_embed, batch_size)\n",
    "            \n",
    "            # Mismtach loss\n",
    "            c_loss_mismatch = mismatch_loss_critic(real_scores, wrong_scores)\n",
    "\n",
    "            # # Drift loss\n",
    "            epsilon_drift = 0.001\n",
    "            drift_loss = epsilon_drift * tf.reduce_mean(tf.square(real_scores))\n",
    "            # Total critic loss\n",
    "            c_loss = c_loss_wasserstein + hparas['LAMBDA_GP'] * gp + hparas['LAMBDA_MISMATCH'] * c_loss_mismatch + drift_loss\n",
    "        combined_critic_vars = critic.trainable_variables + text_encoder.trainable_variables\n",
    "        grad_c = critic_tape.gradient(c_loss, combined_critic_vars)\n",
    "        # Filter out None gradients (e.g., from frozen BERT layers)\n",
    "        valid_grads_c = [(g, v) for g, v in zip(grad_c, combined_critic_vars) if g is not None]\n",
    "        critic_optimizer.apply_gradients(valid_grads_c)\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Generator (once per n_critic iterations)\n",
    "    # ============================================================\n",
    "    noise = tf.random.normal([batch_size, hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    \n",
    "    # --- Generate fresh aug params for generator step ---\n",
    "    gen_aug_params = None\n",
    "    if hparas['USE_DIFFAUG']:\n",
    "        gen_aug_params = get_aug_params(tf.shape(real_image)[0])\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "        \n",
    "        _, fake_image = generator(text_embed, noise, training=True)\n",
    "        \n",
    "        if hparas['USE_DIFFAUG']:\n",
    "            fake_image_aug = DiffAugment(fake_image, policy=hparas['DIFFAUG_POLICY'], params=gen_aug_params)\n",
    "            real_image_aug_g = DiffAugment(real_image, policy=hparas['DIFFAUG_POLICY'], params=gen_aug_params)\n",
    "        else:\n",
    "            fake_image_aug = fake_image\n",
    "            real_image_aug_g = real_image\n",
    "        \n",
    "        fake_scores, fake_feats = critic(fake_image_aug, text_embed, training=True)\n",
    "        \n",
    "\n",
    "        # find feature match loss \n",
    "        _, real_feats_g = critic(real_image_aug_g, text_embed, training=True)\n",
    "\n",
    "        # fm_loss might conflict with gp\n",
    "        #fm_loss = tf.reduce_mean(tf.abs(real_feats_g - fake_feats))\n",
    "\n",
    "        g_loss = wasserstein_loss_generator(fake_scores)\n",
    "\n",
    "\n",
    "    #combined_gen_vars = generator.trainable_variables + text_encoder.trainable_variables\n",
    "    combined_gen_vars = generator.trainable_variables\n",
    "    grad_combined = gen_tape.gradient(g_loss, combined_gen_vars)\n",
    "    # Filter out None gradients\n",
    "    valid_grads_g = [(g, v) for g, v in zip(grad_combined, combined_gen_vars) if g is not None]\n",
    "    generator_optimizer.apply_gradients(valid_grads_g)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    wasserstein_dist = calculate_wasserstein_distance(real_scores, fake_scores)\n",
    "    # Use the VALID gradients for norm calculation\n",
    "    grad_norm_g = calculate_gradient_norm([g for g, v in valid_grads_g])\n",
    "    grad_norm_c = calculate_gradient_norm([g for g, v in valid_grads_c])\n",
    "    \n",
    "    # EMA update\n",
    "    beta = 0.999\n",
    "    for ema_var, live_var in zip(ema_generator.trainable_variables, generator.trainable_variables):\n",
    "        ema_var.assign(beta * ema_var + (1.0 - beta) * live_var)\n",
    "\n",
    "    for ema_var, live_var in zip(ema_generator.non_trainable_variables, generator.non_trainable_variables):\n",
    "        ema_var.assign(live_var)\n",
    "\n",
    "    return {\n",
    "        'g_loss': g_loss,\n",
    "        'c_loss': c_loss,\n",
    "        'c_loss_wasserstein': c_loss_wasserstein,\n",
    "        'c_loss_mismatch': c_loss_mismatch,\n",
    "        'drift_loss': drift_loss,\n",
    "        'gp': gp,\n",
    "        'wasserstein_dist': wasserstein_dist,\n",
    "        'grad_norm_g': grad_norm_g,\n",
    "        'grad_norm_c': grad_norm_c\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(input_ids, attention_mask, noise):\n",
    "    # Encode text with DistilBERT (no hidden state)\n",
    "    text_embed = text_encoder(input_ids, attention_mask, training=False)\n",
    "    _, fake_image = ema_generator(text_embed, noise, training=False)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Visualiztion\">Visualiztion<a class=\"anchor-link\" href=\"#Visualiztion\">¶</a></h2>\n",
    "<p>During training, we can visualize the generated image to evaluate the quality of generator. The followings are some functions helping visualization.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We always use same random seed and same senteces during training, which is more convenient for us to evaluate the quality of generated image.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample data created:\n",
      "  Batch size: 128\n",
      "  Grid size (ni): 12 × 12 = 144\n",
      "  Sample sentences: 128 sentences\n",
      "  sample_seed shape: (128, 512)\n",
      "  sample_input_ids shape: (128, 64)\n",
      "  sample_attention_mask shape: (128, 64)\n",
      "✓ All dimensions match!\n"
     ]
    }
   ],
   "source": [
    "# Create sample data for visualization during training\n",
    "# IMPORTANT: All three variables must have the same batch size!\n",
    "\n",
    "sample_size = hparas['BATCH_SIZE']  # Current: 32\n",
    "ni = int(np.ceil(np.sqrt(sample_size)))  # Grid size for visualization\n",
    "\n",
    "# Create random noise seed\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "# Define 8 diverse sample sentences\n",
    "base_sentences = [\n",
    "    \"the flower shown has yellow anther red pistil and bright red petals.\",\n",
    "    \"this flower has petals that are yellow, white and purple and has dark lines\",\n",
    "    \"the petals on this flower are white with a yellow center\",\n",
    "    \"this flower has a lot of small round pink petals.\",\n",
    "    \"this flower is orange in color, and has petals that are ruffled and rounded.\",\n",
    "    \"the flower has yellow petals and the center of it is brown.\",\n",
    "    \"this flower has petals that are blue and white.\",\n",
    "    \"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "]\n",
    "\n",
    "# Repeat sentences to match sample_size (batch size)\n",
    "sample_sentences = []\n",
    "for i in range(sample_size):\n",
    "    sample_sentences.append(base_sentences[i % len(base_sentences)])\n",
    "\n",
    "# Tokenize with DistilBERT\n",
    "sample_encoded = preprocess_text_distilbert(sample_sentences, max_length=64)\n",
    "sample_input_ids = sample_encoded['input_ids']\n",
    "sample_attention_mask = sample_encoded['attention_mask']\n",
    "\n",
    "# Verify all dimensions match!\n",
    "print(f\"✓ Sample data created:\")\n",
    "print(f\"  Batch size: {sample_size}\")\n",
    "print(f\"  Grid size (ni): {ni} × {ni} = {ni*ni}\")\n",
    "print(f\"  Sample sentences: {len(sample_sentences)} sentences\")\n",
    "print(f\"  sample_seed shape: {sample_seed.shape}\")\n",
    "print(f\"  sample_input_ids shape: {sample_input_ids.shape}\")\n",
    "print(f\"  sample_attention_mask shape: {sample_attention_mask.shape}\")\n",
    "\n",
    "# Check for dimension mismatches\n",
    "assert len(sample_sentences) == sample_size, f\"Mismatch: {len(sample_sentences)} != {sample_size}\"\n",
    "assert sample_seed.shape[0] == sample_size, f\"Mismatch: {sample_seed.shape[0]} != {sample_size}\"\n",
    "assert sample_input_ids.shape[0] == sample_size, f\"Mismatch: {sample_input_ids.shape[0]} != {sample_size}\"\n",
    "assert sample_attention_mask.shape[0] == sample_size, f\"Mismatch: {sample_attention_mask.shape[0]} != {sample_size}\"\n",
    "print(\"✓ All dimensions match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing training runs...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_run_config(run_timestamp):\n",
    "    \"\"\"Load configuration from a previous run\"\"\"\n",
    "    config_path = f'runs/{run_timestamp}/config.json'\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f'Config not found: {config_path}')\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f'✓ Loaded config from: {config_path}')\n",
    "    return config\n",
    "\n",
    "print('Checking for existing training runs...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Training\">Training<a class=\"anchor-link\" href=\"#Training\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created NEW run directory: runs/20251120-155708\n",
      "✓ Saved configuration to: runs/20251120-155708/config.json\n",
      "\n",
      "Run directory structure:\n",
      "  runs/20251120-155708/\n",
      "  ├── checkpoints/  : runs/20251120-155708/checkpoints\n",
      "  ├── best_models/  : runs/20251120-155708/best_models\n",
      "  ├── samples/      : runs/20251120-155708/samples\n",
      "  └── inference/    : runs/20251120-155708/inference\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# RESUME TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "# Set to None for new run, or specify run timestamp to resume\n",
    "# Example: RESUME_RUN = '20251116-225453'\n",
    "RESUME_RUN = None # ← Change this to resume from specific run\n",
    "\n",
    "# ============================================================\n",
    "# RUN DIRECTORY SETUP\n",
    "# ============================================================\n",
    "if RESUME_RUN:\n",
    "    # Resume from existing run\n",
    "    run_dir = f'runs/{RESUME_RUN}'\n",
    "    \n",
    "    # Verify directory exists\n",
    "    if not os.path.exists(run_dir):\n",
    "        raise FileNotFoundError(f'Run directory not found: {run_dir}')\n",
    "    \n",
    "    # Load existing config\n",
    "    try:\n",
    "        prev_config = load_run_config(RESUME_RUN)\n",
    "        run_timestamp = prev_config.get('run_timestamp', RESUME_RUN)\n",
    "        print(f'\\n⟳ RESUMING training from: {run_dir}')\n",
    "        print(f'  Original start: {run_timestamp}')\n",
    "        \n",
    "        # Warn if hyperparameters might be different\n",
    "        if 'hyperparameters' in prev_config:\n",
    "            prev_hparas = prev_config['hyperparameters']\n",
    "            if prev_hparas.get('BATCH_SIZE') != hparas['BATCH_SIZE']:\n",
    "                print(f'  ⚠ WARNING: Batch size changed ({prev_hparas.get(\"BATCH_SIZE\")} → {hparas[\"BATCH_SIZE\"]})')\n",
    "            if prev_hparas.get('LR') != hparas['LR']:\n",
    "                print(f'  ⚠ WARNING: Learning rate changed ({prev_hparas.get(\"LR\")} → {hparas[\"LR\"]})')\n",
    "    except Exception as e:\n",
    "        print(f'⚠ Could not load previous config: {e}')\n",
    "        run_timestamp = RESUME_RUN\n",
    "    \n",
    "    # Use existing subdirectories\n",
    "    checkpoint_dir = f'{run_dir}/checkpoints'\n",
    "    best_models_dir = f'{run_dir}/best_models'\n",
    "    samples_dir = f'{run_dir}/samples'\n",
    "    inference_dir = f'{run_dir}/inference'\n",
    "    \n",
    "    # Create directories if they don't exist (shouldn't happen, but safety check)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(best_models_dir, exist_ok=True)\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    os.makedirs(inference_dir, exist_ok=True)\n",
    "    \n",
    "else:\n",
    "    # Create new run\n",
    "    run_timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    run_dir = f'runs/{run_timestamp}'\n",
    "    \n",
    "    # All outputs for this run go in subdirectories\n",
    "    checkpoint_dir = f'{run_dir}/checkpoints'\n",
    "    best_models_dir = f'{run_dir}/best_models'\n",
    "    samples_dir = f'{run_dir}/samples'\n",
    "    inference_dir = f'{run_dir}/inference'\n",
    "    \n",
    "    # Create all directories\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(best_models_dir, exist_ok=True)\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    os.makedirs(inference_dir, exist_ok=True)\n",
    "    \n",
    "    print(f'✓ Created NEW run directory: {run_dir}')\n",
    "    \n",
    "    # Save hyperparameters\n",
    "    config_to_save = {\n",
    "        'run_timestamp': run_timestamp,\n",
    "        'hyperparameters': hparas,\n",
    "    }\n",
    "    with open(f'{run_dir}/config.json', 'w') as f:\n",
    "        json.dump(config_to_save, f, indent=4)\n",
    "    print(f'✓ Saved configuration to: {run_dir}/config.json')\n",
    "\n",
    "\n",
    "# Display directory structure\n",
    "print(f'\\nRun directory structure:')\n",
    "print(f'  {run_dir}/')\n",
    "print(f'  ├── checkpoints/  : {checkpoint_dir}')\n",
    "print(f'  ├── best_models/  : {best_models_dir}')\n",
    "print(f'  ├── samples/      : {samples_dir}')\n",
    "print(f'  └── inference/    : {inference_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Starting NEW training run - no checkpoint restoration needed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RESTORE CHECKPOINT FOR RESUMING TRAINING\n",
    "# ============================================================\n",
    "# Checkpoint Managers\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
    "best_ckpt_manager = tf.train.CheckpointManager(checkpoint, best_models_dir, max_to_keep=1)\n",
    "\n",
    "if RESUME_RUN:\n",
    "    # When resuming, restore the LATEST regular checkpoint\n",
    "    print(f'\\nRestoring checkpoint for resuming training...')\n",
    "    \n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        checkpoint.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "        print(f'✓ Restored latest checkpoint: {ckpt_manager.latest_checkpoint}')\n",
    "        print(f'  Resuming from epoch: {epoch_tracker.numpy()}')\n",
    "        print(f'  Current best WD: {best_wd_tracker.numpy()}')\n",
    "    else:\n",
    "        print('⚠ RESUME_RUN set but no checkpoint found. Starting from scratch.')\n",
    "else:\n",
    "    print('\\n✓ Starting NEW training run - no checkpoint restoration needed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "def train(dataset, epochs):\n",
    "    global run_dir, checkpoint_dir, best_models_dir, samples_dir, inference_dir\n",
    "    global ckpt_manager, best_ckpt_manager, checkpoint, epoch_tracker, best_wd_tracker\n",
    "    \n",
    "    log_dir = f'{run_dir}/logs'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "  \n",
    "    print(f\"Run tensorboard --logdir {log_dir}\")\n",
    "    print(f'Model: WGAN-GP with {hparas[\"N_CRITIC\"]} critic iterations')\n",
    "    print(f'DiffAugment: {hparas[\"USE_DIFFAUG\"]} ({hparas.get(\"DIFFAUG_POLICY\", \"N/A\")})')\n",
    "    \n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    # Determine start epoch from tracker\n",
    "    start_epoch = int(epoch_tracker.numpy())\n",
    "    global_step = start_epoch * steps_per_epoch\n",
    "    \n",
    "    try:\n",
    "        # Using sys.executable ensures we use tensorboard from the correct python env\n",
    "        tensorboard_process = subprocess.Popen([\n",
    "            sys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir\n",
    "        ])\n",
    "        print(f\"✓ TensorBoard launched as a background process (PID: {tensorboard_process.pid}).\")\n",
    "        print(\"  It might take a few seconds to become available in your browser.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not start TensorBoard automatically: {e}\")\n",
    "        print(f\"  You can start it manually by running: tensorboard --logdir {log_dir}\")\n",
    "\n",
    "    # ========== EARLY STOPPING SETUP ==========\n",
    "    # Initialize best_wd from tracker if available\n",
    "    best_wasserstein_dist = float(best_wd_tracker.numpy())\n",
    "    patience = 300\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"Starting training from epoch {start_epoch} to {epochs}...\")\n",
    "    print(f\"Current Best Wasserstein Distance: {best_wasserstein_dist}\")\n",
    "    # ==========================================\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # ========== LEARNING RATE DECAY ==========\n",
    "        if epoch >= hparas['LR_DECAY_START'] and epoch % hparas['LR_DECAY_EVERY'] == 0:\n",
    "            current_lr_g = generator_optimizer.learning_rate.numpy()\n",
    "            current_lr_c = critic_optimizer.learning_rate.numpy()\n",
    "            \n",
    "            new_lr_g = max(current_lr_g * hparas['LR_DECAY_FACTOR'], hparas['LR_MIN'])\n",
    "            new_lr_c = max(current_lr_c * hparas['LR_DECAY_FACTOR'], hparas['LR_MIN'])\n",
    "            \n",
    "            generator_optimizer.learning_rate.assign(new_lr_g)\n",
    "            critic_optimizer.learning_rate.assign(new_lr_c)\n",
    "            \n",
    "            print(f'  📉 LR Decay: G={new_lr_g:.2e}, C={new_lr_c:.2e}')\n",
    "        # ==========================================\n",
    "        \n",
    "        g_total_loss = 0\n",
    "        c_total_loss = 0\n",
    "        c_total_loss_wasserstein = 0\n",
    "        gp_total = 0\n",
    "        wd_total = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        pbar = tqdm(dataset, desc=f'Epoch {epoch+1}/{epochs}', \n",
    "                   total=steps_per_epoch, unit='batch')\n",
    "        \n",
    "        for batch_idx, (image, input_ids, attention_mask) in enumerate(pbar):\n",
    "            metrics = train_step(image, input_ids, attention_mask)\n",
    "            \n",
    "            # Accumulate losses\n",
    "            g_total_loss += metrics['g_loss']\n",
    "            c_total_loss += metrics['c_loss']\n",
    "            c_total_loss_wasserstein += metrics['c_loss_wasserstein']\n",
    "            gp_total += metrics['gp']\n",
    "            wd_total += metrics['wasserstein_dist']\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('Losses/generator_loss', metrics['g_loss'], step=global_step)\n",
    "                tf.summary.scalar('Losses/critic_loss_total', metrics['c_loss'], step=global_step)\n",
    "                tf.summary.scalar('Losses/critic_loss_wasserstein', metrics['c_loss_wasserstein'], step=global_step)\n",
    "                tf.summary.scalar('Losses/critic_loss_mismatch', metrics['c_loss_mismatch'], step=global_step)\n",
    "                tf.summary.scalar('Losses/gradient_penalty', metrics['gp'], step=global_step)\n",
    "                tf.summary.scalar('Metrics/wasserstein_distance', metrics['wasserstein_dist'], step=global_step)\n",
    "                \n",
    "                if global_step % 50 == 0:\n",
    "                    tf.summary.scalar('Gradients/generator_gradient_norm', metrics['grad_norm_g'], step=global_step)\n",
    "                    tf.summary.scalar('Gradients/critic_gradient_norm', metrics['grad_norm_c'], step=global_step)\n",
    "                    # ========== LOG LEARNING RATES ==========\n",
    "                    tf.summary.scalar('Training/learning_rate_generator', \n",
    "                                    generator_optimizer.learning_rate.numpy(), step=global_step)\n",
    "                    tf.summary.scalar('Training/learning_rate_critic', \n",
    "                                    critic_optimizer.learning_rate.numpy(), step=global_step)\n",
    "                    # ========================================\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'G_loss': f'{metrics[\"g_loss\"]:.4f}',\n",
    "                'C_loss': f'{metrics[\"c_loss\"]:.4f}',\n",
    "                'W_dist': f'{metrics[\"wasserstein_dist\"]:.4f}'\n",
    "            })\n",
    "            \n",
    "            global_step += 1\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        avg_g_loss = g_total_loss / steps_per_epoch\n",
    "        avg_c_loss = c_total_loss / steps_per_epoch\n",
    "        avg_c_loss_w = c_total_loss_wasserstein / steps_per_epoch\n",
    "        avg_gp = gp_total / steps_per_epoch\n",
    "        avg_wd = wd_total / steps_per_epoch\n",
    "        epoch_time = time.time() - start\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: G_loss={avg_g_loss:.4f}, C_loss={avg_c_loss:.4f} ' +\n",
    "              f'(W={avg_c_loss_w:.4f}, GP={avg_gp:.4f}), W_dist={avg_wd:.4f}, Time={epoch_time:.2f}s')\n",
    "        \n",
    "        # Log epoch averages\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('Epoch/generator_loss_avg', avg_g_loss, step=epoch)\n",
    "            tf.summary.scalar('Epoch/critic_loss_avg', avg_c_loss, step=epoch)\n",
    "            tf.summary.scalar('Epoch/wasserstein_distance_avg', avg_wd, step=epoch)\n",
    "        \n",
    "        # Update epoch tracker\n",
    "        epoch_tracker.assign(epoch + 1)\n",
    "        \n",
    "        # ========== EARLY STOPPING & BEST MODEL SAVE ==========\n",
    "        if avg_wd < best_wasserstein_dist:\n",
    "            best_wasserstein_dist = avg_wd\n",
    "            best_wd_tracker.assign(best_wasserstein_dist)\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model using Manager\n",
    "            save_path = best_ckpt_manager.save(checkpoint_number=epoch+1)\n",
    "            print(f'  ⭐ Best model saved! W_dist={avg_wd:.4f} → {save_path}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\n⚠️ Early stopping triggered at epoch {epoch+1}')\n",
    "            print(f'   Best Wasserstein distance: {best_wasserstein_dist:.4f}')\n",
    "            print(f'   No improvement for {patience} epochs')\n",
    "            break\n",
    "        # ======================================================\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_path = ckpt_manager.save(checkpoint_number=epoch+1)\n",
    "            print(f'  ✓ Checkpoint saved: {save_path}')\n",
    "        \n",
    "        # Visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            fake_image = test_step(sample_input_ids, sample_attention_mask, sample_seed)\n",
    "            save_images(fake_image, [ni, ni], f'{samples_dir}/train_{epoch+1:03d}.jpg')\n",
    "            \n",
    "            with summary_writer.as_default():\n",
    "                display_images = (fake_image + 1.0) / 2.0\n",
    "                tf.summary.image('Generated_Samples', display_images, step=epoch, max_outputs=16)\n",
    "            \n",
    "            print(f'  ✓ Sample image saved and logged to TensorBoard')\n",
    "    \n",
    "    print('\\n✓ Training completed!')\n",
    "    print(f'All outputs saved to: {run_dir}')\n",
    "    print(f'Best Wasserstein distance achieved: {best_wasserstein_dist:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run tensorboard --logdir runs/20251120-155708/logs\n",
      "Model: WGAN-GP with 5 critic iterations\n",
      "DiffAugment: True (translation,cutout)\n",
      "✓ TensorBoard launched as a background process (PID: 20616).\n",
      "  It might take a few seconds to become available in your browser.\n",
      "Starting training from epoch 0 to 1000...\n",
      "Current Best Wasserstein Distance: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/57 [00:00<?, ?batch/s]/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorboard/compat/__init__.py\", line 42, in tf\n",
      "    from tensorboard.compat import notf  # noqa: F401\n",
      "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/tensorboard/compat/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/comp3/lib/python3.10/site-packages/google/protobuf/descriptor_pool.py\", line 1292, in Default\n",
      "    def Default():\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Evaluation</center></h1>\n",
    "\n",
    "<p><code>dataset/testData.pkl</code> is a pandas dataframe containing testing text with attributes 'ID' and 'Captions'.</p>\n",
    "\n",
    "<ul>\n",
    "<li>'ID': text ID used to name generated image.</li>\n",
    "<li>'Captions': text used as condition to generate image.</li>\n",
    "</ul>\n",
    "\n",
    "<p>For each captions, you need to generate <strong>inference_ID.png</strong> to evaluate quality of generated image. You must name the generated image in this format, otherwise we cannot evaluate your images.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Testing-Dataset\">Testing Dataset<a class=\"anchor-link\" href=\"#Testing-Dataset\">¶</a></h2>\n",
    "<p>If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption_text, index):\n",
    "    \"\"\"\n",
    "    Updated testing data generator using DistilBERT tokenization\n",
    "    \n",
    "    Args:\n",
    "        caption_text: Raw text string\n",
    "        index: Test sample ID\n",
    "    \n",
    "    Returns:\n",
    "        input_ids, attention_mask, index\n",
    "    \"\"\"\n",
    "    # Use tf.py_function to call Python tokenizer\n",
    "    input_ids, attention_mask = tf.py_function(\n",
    "        func=tokenize_caption_fn,\n",
    "        inp=[caption_text],\n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    \n",
    "    # Set shapes explicitly\n",
    "    input_ids.set_shape([64])\n",
    "    attention_mask.set_shape([64])\n",
    "    \n",
    "    return input_ids, attention_mask, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    \"\"\"\n",
    "    Updated testing dataset generator - decodes IDs to raw text\n",
    "    \"\"\"\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    captions_ids = data['Captions'].values\n",
    "    caption_texts = []\n",
    "    \n",
    "    # Decode pre-tokenized IDs back to text\n",
    "    for i in range(len(captions_ids)):\n",
    "        chosen_caption_ids = captions_ids[i]\n",
    "        \n",
    "        # Decode IDs back to text using id2word_dict\n",
    "        words = []\n",
    "        for word_id in chosen_caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':  # Skip padding tokens\n",
    "                words.append(word)\n",
    "        \n",
    "        caption_text = ' '.join(words)\n",
    "        caption_texts.append(caption_text)\n",
    "    \n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    # Create dataset from raw text\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption_texts, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Inferece\">Inferece<a class=\"anchor-link\" href=\"#Inferece\">¶</a></h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference directory is already created by the train() function\n",
    "# No need to create it again here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore BEST MODEL for inference\n",
    "print(f'Looking for BEST model in: {best_models_dir}')\n",
    "\n",
    "best_checkpoint = tf.train.latest_checkpoint(best_models_dir)\n",
    "if best_checkpoint:\n",
    "    checkpoint.restore(best_checkpoint)\n",
    "    print(f'✓ Restored BEST model: {best_checkpoint}')\n",
    "    print(f'  This is the model with the lowest Wasserstein distance during training')\n",
    "else:\n",
    "    print('⚠ No best model found, trying regular checkpoints...')\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint)\n",
    "        print(f'✓ Restored latest checkpoint: {latest_checkpoint}')\n",
    "        print('  ⚠ WARNING: Using latest checkpoint, not best model')\n",
    "    else:\n",
    "        print('⚠ No checkpoint found at all, using fresh/untrained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    \"\"\"\n",
    "    Updated inference function for DistilBERT\n",
    "    FIXED: Generate fresh random noise for each batch!\n",
    "    \"\"\"\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    total_images = 0\n",
    "    \n",
    "    # Progress bar for inference\n",
    "    pbar = tqdm(total=NUM_TEST, desc='Generating images', unit='img')\n",
    "    \n",
    "    # Unpack 3 values: input_ids, attention_mask, idx\n",
    "    for input_ids, attention_mask, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        # CRITICAL FIX: Generate FRESH random noise for each batch\n",
    "        # This ensures diversity across all 819 test images\n",
    "        sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "        \n",
    "        fake_image = test_step(input_ids, attention_mask, sample_seed)\n",
    "        step += 1\n",
    "        \n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave(f'{inference_dir}/inference_{idx[i]:04d}.jpg', fake_image[i].numpy()*0.5 + 0.5)\n",
    "            total_images += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f'\\n✓ Generated {total_images} images in {time.time()-start:.4f} sec')\n",
    "    print(f'✓ Images saved to: {inference_dir}')\n",
    "    print(f'✓ Each image generated with unique random noise for better diversity!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(testing_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation script to generate score.csv\n",
    "# Note: This must be run from the testing directory because inception_score.py uses relative paths\n",
    "# Arguments: [inference_dir] [output_csv] [batch_size]\n",
    "# Batch size must be 1, 2, 3, 7, 9, 21, or 39 to avoid remainder (819 test images)\n",
    "\n",
    "# Save score.csv inside the run directory\n",
    "print(\"running in \", inference_dir, \"with\", run_dir)\n",
    "!cd testing && python inception_score.py ../{inference_dir}/ ../{run_dir}/score.csv 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generated Images\n",
    "\n",
    "Below we randomly sample 20 images from our generated test results to visually inspect the quality and diversity of the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center class=\"subtitle\">Demo</center></h1>\n",
    "\n",
    "<p>We demonstrate the capability of our model (TA80) to generate plausible images of flowers from detailed text descriptions.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 20 random generated images with their captions\n",
    "import glob\n",
    "\n",
    "# Load test data\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "test_captions = data['Captions'].values\n",
    "test_ids = data['ID'].values\n",
    "\n",
    "# Get all generated images from the current inference directory\n",
    "image_files = sorted(glob.glob(inference_dir + '/inference_*.jpg'))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(f'⚠ No images found in {inference_dir}')\n",
    "    print('Please run the inference cell first!')\n",
    "else:\n",
    "    # Randomly sample 20 images\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    num_samples = min(20, len(image_files))\n",
    "    sample_indices = np.random.choice(len(image_files), size=num_samples, replace=False)\n",
    "    sample_files = [image_files[i] for i in sorted(sample_indices)]\n",
    "\n",
    "    # Create 4x5 grid\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, img_path in enumerate(sample_files):\n",
    "        # Extract image ID from filename\n",
    "        img_id = int(Path(img_path).stem.split('_')[1])\n",
    "        \n",
    "        # Find caption\n",
    "        caption_idx = np.where(test_ids == img_id)[0][0]\n",
    "        caption_ids = test_captions[caption_idx]\n",
    "        \n",
    "        # Decode caption\n",
    "        caption_text = ''\n",
    "        for word_id in caption_ids:\n",
    "            word = id2word_dict[str(word_id)]\n",
    "            if word != '<PAD>':\n",
    "                caption_text += word + ' '\n",
    "        \n",
    "        # Load and display image\n",
    "        img = plt.imread(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'ID: {img_id}\\n{caption_text[:60]}...', fontsize=8)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    # Hide unused subplots if less than 20 images\n",
    "    for idx in range(num_samples, 20):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Random Sample of {num_samples} Generated Images', fontsize=16, y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nTotal generated images: {len(image_files)}')\n",
    "    print(f'Images directory: {inference_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
